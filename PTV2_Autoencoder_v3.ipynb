{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarwilkins1707/DeepLearningThesis/blob/main/PTV2_Autoencoder_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Install Required Libraries"
      ],
      "metadata": {
        "id": "7KHqF7N8KD9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lwv9QOGgqGGX",
        "outputId": "0a0fd029-e06f-460e-d5b9-6cf0b97d69cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt26cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-mi3ws5xz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-mi3ws5xz\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 36aed7c28140a54f27aea6c2429636ff0d1c84b8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Collecting xxhash (from torch-geometric==2.7.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2025.4.26)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1206610 sha256=3aed2e1c6da9fe6b32e8ea09445322ed007e6bea7464608958d51fb1ad535ddf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hbr16126/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: xxhash, torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0 xxhash-3.5.0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html #Use prebuilt wheels to make code faster\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!set CUDA_LAUNCH_BLOCKING = 1\n",
        "!set TORCH_USE_CUDA_DSA = 1"
      ],
      "metadata": {
        "id": "7TrOk0IoqIF-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import natsort\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "from torch_cluster import knn\n",
        "import einops\n",
        "from copy import deepcopy\n",
        "import math\n",
        "from torch_geometric.nn.pool import voxel_grid\n",
        "from torch_scatter import segment_csr, composite\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fybzfupb2AUI",
        "outputId": "1abe1412-427d-4cab-9ae6-c0258f727719"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Define Classes for PTV2 Algorithm"
      ],
      "metadata": {
        "id": "LRipkaeqKNdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PointTransformerV2 Set Up Classes"
      ],
      "metadata": {
        "id": "l_3lydurK1JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def offset2batch(offset):\n",
        "    return torch.cat([torch.tensor([i] * (o - offset[i - 1])) if i > 0 else\n",
        "                      torch.tensor([i] * o) for i, o in enumerate(offset)],\n",
        "                     dim=0).long().to(offset.device)\n",
        "\n",
        "def batch2offset(batch):\n",
        "    return torch.cumsum(batch.bincount(), dim=0).long()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "def inpdict_to_point(inp_dict):\n",
        "  enc_ip = inp_dict['enc_inp']\n",
        "  offset = inp_dict['inp_batch_ids']\n",
        "  coords = enc_ip[:,:3]\n",
        "  feat = enc_ip[:,3]\n",
        "  return coords,feat,offset\n",
        "\n",
        "def grouping(idx,\n",
        "             feat,\n",
        "             xyz,\n",
        "             new_xyz=None,\n",
        "             with_xyz=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Figure out whatever tf this function does.\n",
        "    \"\"\"\n",
        "\n",
        "    #Added this to avoid errors. Is that ok?\n",
        "    xyz = xyz.contiguous()\n",
        "    feat = feat.contiguous()\n",
        "\n",
        "    if new_xyz is None:\n",
        "        new_xyz = xyz\n",
        "\n",
        "    assert xyz.is_contiguous()\n",
        "    assert feat.is_contiguous()\n",
        "\n",
        "    m, nsample, c = idx.shape[0], idx.shape[1], feat.shape[1]\n",
        "    xyz = torch.cat([xyz, torch.zeros([1, 3]).to(xyz.device)], dim=0)\n",
        "    feat = torch.cat([feat, torch.zeros([1, c]).to(feat.device)], dim=0)\n",
        "    grouped_feat = feat[idx.view(-1).long(), :].view(m, nsample, c)  # (m, num_sample, c)\n",
        "\n",
        "    if with_xyz:\n",
        "        assert new_xyz.is_contiguous()\n",
        "        mask = torch.sign(idx + 1)\n",
        "        grouped_xyz = xyz[idx.view(-1).long(), :].view(m, nsample, 3) - new_xyz.unsqueeze(1)  # (m, num_sample, 3)\n",
        "        grouped_xyz = torch.einsum(\"n s c, n s -> n s c\", grouped_xyz, mask)  # (m, num_sample, 3)\n",
        "\n",
        "        return torch.cat((grouped_xyz, grouped_feat), -1)\n",
        "    else:\n",
        "        return grouped_feat\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
        "    \"\"\"\n",
        "    Zeros elements of the input tensor (x) with probability `drop_prob` during training.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
        "    if keep_prob > 0.0 and scale_by_keep:\n",
        "        random_tensor.div_(keep_prob) #Increases magnitude of non-zeros elements to have same magnitude of points going into architecture\n",
        "    return x * random_tensor\n"
      ],
      "metadata": {
        "id": "U3dwkxLVsInB"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.scale_by_keep = scale_by_keep\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Returns Tensor with zeroed out elements based on drop_prob.\n",
        "        \"\"\"\n",
        "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
        "\n",
        "class FeatureAggregation(nn.Module):\n",
        "  def __init__(self, in_channels, dmodel):\n",
        "    super(FeatureAggregation, self).__init__()\n",
        "    self.q = nn.Parameter(torch.randn(1, dmodel).float())\n",
        "    self.v = nn.Conv1d(in_channels = in_channels,\n",
        "                       out_channels = int(in_channels*dmodel),\n",
        "                       kernel_size = 1,\n",
        "                       groups = in_channels)\n",
        "    self.k = nn.Conv1d(in_channels = in_channels,\n",
        "                       out_channels = int(in_channels*dmodel),\n",
        "                       kernel_size = 1,\n",
        "                       groups = in_channels)\n",
        "\n",
        "  def forward(self, points):\n",
        "    \"\"\"f\n",
        "    replaces features in points with aggregated features (based on a feature attention operation).\n",
        "\n",
        "    Inputs:\n",
        "    points (list): [coords, feats, batch]\n",
        "\n",
        "    Outputs:\n",
        "    points (list): [coords, feats, batch]\n",
        "    \"\"\"\n",
        "    coords, feats, batch = points\n",
        "    BK, fc = feats.shape\n",
        "    feat_v = self.v(feats.float().unsqueeze(2)).reshape(BK, fc, -1)\n",
        "    feat_k = self.k(feats.float().unsqueeze(2)).reshape(BK, fc, -1)\n",
        "\n",
        "    qk = torch.einsum('qd, bkd -> bqk', self.q, feat_k)\n",
        "    attn = nn.Softmax(dim=-1)(qk)\n",
        "    out = torch.einsum('bkd, bqk -> bqd', feat_v, attn)\n",
        "\n",
        "    return [coords, out.squeeze(1),  batch]\n",
        "\n",
        "class PointAggregation(nn.Module):\n",
        "    def __init__(self, dmodel):\n",
        "        super(PointAggregation, self).__init__()\n",
        "        #self.configs = configs\n",
        "\n",
        "        self.q = nn.Parameter(torch.randn(1, dmodel).float())\n",
        "        self.v = nn.Linear(dmodel, dmodel)\n",
        "        self.k = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def forward(self, feats, batch):\n",
        "          offset = batch2offset(batch)\n",
        "          offset = torch.cat([offset.new_zeros(1), offset])\n",
        "          #print(feats.shape, batch.shape)\n",
        "          feats_v = self.v(feats.float())\n",
        "          feats_k = self.k(feats.float())\n",
        "\n",
        "          qk = torch.einsum('qd,kd -> kq', self.q, feats_k)\n",
        "          attn = composite.scatter_softmax(qk, batch.long(), dim = 0)\n",
        "          out = segment_csr(torch.einsum('qk,qd -> qd', attn, feats_v), offset)\n",
        "\n",
        "          return out"
      ],
      "metadata": {
        "id": "CzCdTnUYsAQF"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedLinear(nn.Module):\n",
        "    __constants__ = ['in_features', 'out_features', \"groups\"]\n",
        "    in_features: int\n",
        "    out_features: int\n",
        "    groups: int\n",
        "    weight: torch.Tensor\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, groups: int,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(GroupedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.groups = groups\n",
        "        assert in_features & groups == 0\n",
        "        assert out_features % groups == 0\n",
        "        # for convenient, currently only support out_features == groups, one output\n",
        "        assert out_features == groups\n",
        "        self.weight = nn.Parameter(torch.empty((1, in_features), **factory_kwargs))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return (input * self.weight).reshape(\n",
        "            list(input.shape[:-1]) + [self.groups, input.shape[-1] // self.groups]).sum(-1)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class PointBatchNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Batch Normalization for Point Clouds data in shape of [B*N, C], [B*N, L, C]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.BatchNorm1d(embed_channels)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        if input.dim() == 3:\n",
        "            return self.norm(input.transpose(1, 2).contiguous()).transpose(1, 2).contiguous()\n",
        "        elif input.dim() == 2:\n",
        "            return self.norm(input)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class GroupedVectorAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 attn_drop_rate=0.,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=True,\n",
        "                 pe_bias=True\n",
        "                 ):\n",
        "        super(GroupedVectorAttention, self).__init__()\n",
        "        self.embed_channels = embed_channels\n",
        "        self.groups = groups\n",
        "        assert embed_channels % groups == 0\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.pe_multiplier = pe_multiplier\n",
        "        self.pe_bias = pe_bias\n",
        "\n",
        "        self.linear_q = nn.Sequential(\n",
        "            nn.Linear(embed_channels, embed_channels, bias=qkv_bias),\n",
        "            PointBatchNorm(embed_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.linear_k = nn.Sequential(\n",
        "            nn.Linear(embed_channels, embed_channels, bias=qkv_bias),\n",
        "            PointBatchNorm(embed_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.linear_v = nn.Linear(embed_channels, embed_channels, bias=qkv_bias)\n",
        "\n",
        "        if self.pe_multiplier:\n",
        "            self.linear_p_multiplier = nn.Sequential(\n",
        "                nn.Linear(3, embed_channels),\n",
        "                PointBatchNorm(embed_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(embed_channels, embed_channels),\n",
        "            )\n",
        "        if self.pe_bias:\n",
        "            self.linear_p_bias = nn.Sequential(\n",
        "                nn.Linear(3, embed_channels),\n",
        "                PointBatchNorm(embed_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(embed_channels, embed_channels),\n",
        "            )\n",
        "\n",
        "        self.weight_encoding = nn.Sequential(\n",
        "            GroupedLinear(embed_channels, groups, groups),\n",
        "            PointBatchNorm(groups),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(groups, groups)\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.attn_drop = nn.Dropout(attn_drop_rate)\n",
        "\n",
        "    def forward(self, feat, coord, reference_index):\n",
        "        query, key, value = self.linear_q(feat), self.linear_k(feat), self.linear_v(feat)\n",
        "        key = grouping(reference_index, key, coord, with_xyz=True)\n",
        "        value = grouping(reference_index, value, coord, with_xyz=False)\n",
        "        pos, key = key[:, :, 0:3],  key[:, :, 3:]\n",
        "        relation_qk = key - query.unsqueeze(1)\n",
        "\n",
        "        if self.pe_multiplier:\n",
        "            pem = self.linear_p_multiplier(pos)\n",
        "            relation_qk = relation_qk * pem\n",
        "        if self.pe_bias:\n",
        "            peb = self.linear_p_bias(pos)\n",
        "            relation_qk = relation_qk + peb\n",
        "            value = (value + peb)\n",
        "\n",
        "\n",
        "        weight = self.weight_encoding(relation_qk)\n",
        "        weight = self.attn_drop(self.softmax(weight))\n",
        "\n",
        "        mask = torch.sign(reference_index + 1)\n",
        "        weight = torch.einsum(\"n s g, n s -> n s g\", weight, mask)\n",
        "        value = einops.rearrange(value, \"n ns (g i) -> n ns g i\", g=self.groups)\n",
        "        feat = torch.einsum(\"n s g i, n s g -> n g i\", value, weight)\n",
        "        feat = einops.rearrange(feat, \"n g i -> n (g i)\")\n",
        "        return feat\n",
        "\n",
        "class GridPool(nn.Module):\n",
        "    \"\"\"\n",
        "    Partition-based Pooling (Grid Pooling)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 grid_size,\n",
        "                 bias=False):\n",
        "        super(GridPool, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        self.fc = nn.Linear(in_channels, out_channels, bias=bias)\n",
        "        self.norm = PointBatchNorm(out_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, points, start=None):\n",
        "        coord, feat, batch = points\n",
        "        offset = batch2offset(batch)\n",
        "\n",
        "        feat = self.act(self.norm(self.fc(feat)))\n",
        "\n",
        "        start = segment_csr(coord, torch.cat([batch.new_zeros(1), torch.cumsum(batch.bincount(), dim=0)]),\n",
        "                            reduce=\"min\") if start is None else start\n",
        "\n",
        "        cluster = voxel_grid(pos=coord - start[batch], size=self.grid_size, batch=batch, start=0)\n",
        "\n",
        "        unique, cluster, counts = torch.unique(cluster, sorted=True, return_inverse=True, return_counts=True)\n",
        "        _, sorted_cluster_indices = torch.sort(cluster)\n",
        "        idx_ptr = torch.cat([counts.new_zeros(1), torch.cumsum(counts, dim=0)])\n",
        "        coord = segment_csr(coord[sorted_cluster_indices], idx_ptr, reduce=\"mean\")\n",
        "        feat = segment_csr(feat[sorted_cluster_indices], idx_ptr, reduce=\"max\")\n",
        "        batch = batch[idx_ptr[:-1]]\n",
        "        offset = batch2offset(batch)\n",
        "        return [coord, feat, batch], cluster\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 enable_checkpoint=False\n",
        "                 ):\n",
        "\n",
        "        super(Block, self).__init__()\n",
        "        self.attn = GroupedVectorAttention(\n",
        "            embed_channels=embed_channels,\n",
        "            groups=groups,\n",
        "            qkv_bias=qkv_bias,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias\n",
        "        )\n",
        "        self.fc1 = nn.Linear(embed_channels, embed_channels, bias=False)\n",
        "        self.fc3 = nn.Linear(embed_channels, embed_channels, bias=False)\n",
        "        self.norm1 = PointBatchNorm(embed_channels)\n",
        "        self.norm2 = PointBatchNorm(embed_channels)\n",
        "        self.norm3 = PointBatchNorm(embed_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.enable_checkpoint = enable_checkpoint\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, points, reference_index):\n",
        "        coord, feat, batch = points\n",
        "        offset = batch2offset(batch)\n",
        "        identity = feat\n",
        "        feat = self.act(self.norm1(self.fc1(feat)))\n",
        "        feat = self.attn(feat, coord, reference_index) #\\\n",
        "            #if not self.enable_checkpoint else checkpoint(self.attn, feat, coord, time, reference_index)\n",
        "        feat = self.act(self.norm2(feat))\n",
        "        feat = self.norm3(self.fc3(feat))\n",
        "        feat = identity + self.drop_path(feat)\n",
        "        feat = self.act(feat)\n",
        "        return [coord, feat, batch]\n",
        "\n",
        "class BlockSequence(nn.Module):\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 neighbours=16,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 enable_checkpoint=False\n",
        "                 ):\n",
        "        super(BlockSequence, self).__init__()\n",
        "\n",
        "        if isinstance(drop_path_rate, list):\n",
        "            drop_path_rates = drop_path_rate\n",
        "            assert len(drop_path_rates) == depth\n",
        "        elif isinstance(drop_path_rate, float):\n",
        "            drop_path_rates = [deepcopy(drop_path_rate) for _ in range(depth)]\n",
        "        else:\n",
        "            drop_path_rates = [0. for _ in range(depth)]\n",
        "        self.neighbours = neighbours\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            block = Block(\n",
        "                embed_channels=embed_channels,\n",
        "                groups=groups,\n",
        "                qkv_bias=qkv_bias,\n",
        "                pe_multiplier=pe_multiplier,\n",
        "                pe_bias=pe_bias,\n",
        "                attn_drop_rate=attn_drop_rate,\n",
        "                drop_path_rate=drop_path_rates[i],\n",
        "                enable_checkpoint=enable_checkpoint\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    def forward(self, points):\n",
        "        coord, feat, batch = points\n",
        "        # reference index query of neighbourhood attention\n",
        "        # for windows attention, modify reference index query method\n",
        "        #print(coord.shape)\n",
        "        #print(batch.shape)\n",
        "        #print(\"PASSED\")\n",
        "        reference_index = knn(coord.contiguous(), coord.contiguous(), self.neighbours, batch.contiguous(), batch.contiguous())\n",
        "        reference_index = reference_index[1,:].reshape(len(coord), self.neighbours)\n",
        "        for block in self.blocks:\n",
        "            points = block(points, reference_index)\n",
        "        #print(points[0].shape)\n",
        "        #print(points[1].shape)\n",
        "        #print(points[2].shape)\n",
        "        return points\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 in_channels,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 grid_size=None,\n",
        "                 neighbours=16,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=None,\n",
        "                 drop_path_rate=None,\n",
        "                 enable_checkpoint=False,\n",
        "                 ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.down = GridPool(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embed_channels,\n",
        "            grid_size=grid_size,\n",
        "        )\n",
        "\n",
        "        self.blocks = BlockSequence(\n",
        "            depth=depth,\n",
        "            embed_channels=embed_channels,\n",
        "            groups=groups,\n",
        "            neighbours=neighbours,\n",
        "            qkv_bias=qkv_bias,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias,\n",
        "            attn_drop_rate=attn_drop_rate if attn_drop_rate is not None else 0.,\n",
        "            drop_path_rate=drop_path_rate if drop_path_rate is not None else 0.,\n",
        "            enable_checkpoint=enable_checkpoint\n",
        "        )\n",
        "\n",
        "    def forward(self, points):\n",
        "        #print(f\"PRE DOWNSAMPLE SHAPE: {points[0].shape} {points[1].shape} {points[2].shape}\")\n",
        "        points, cluster = self.down(points)\n",
        "        #print(f\"POST DOWNSAMPLE SHAPE: {points[0].shape} {points[1].shape} {points[2].shape}\")\n",
        "        #print(f\"PRE BLOCK SHAPE: {points[0].shape} {points[1].shape} {points[2].shape}\")\n",
        "        check = self.blocks(points)\n",
        "        #print(f\"POST BLOCK SHAPE: {check[0].shape} {check[1].shape} {check[2].shape}\")\n",
        "        return self.blocks(points), cluster\n",
        "\n",
        "\n",
        "\n",
        "class GVAPatchEmbed(nn.Module):\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 in_channels,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 neighbours=8,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 enable_checkpoint=False\n",
        "                 ):\n",
        "        super(GVAPatchEmbed, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_channels = embed_channels\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(in_channels, embed_channels, bias=False),\n",
        "            PointBatchNorm(embed_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.blocks = BlockSequence(\n",
        "            depth=depth,\n",
        "            embed_channels=embed_channels,\n",
        "            groups=groups,\n",
        "            neighbours=neighbours,\n",
        "            qkv_bias=qkv_bias,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            enable_checkpoint=enable_checkpoint\n",
        "        )\n",
        "\n",
        "    def forward(self, points):\n",
        "        coord, feat, batch = points\n",
        "        offset = batch2offset(batch)\n",
        "        feat = self.proj(feat)\n",
        "        return self.blocks([coord, feat, batch])"
      ],
      "metadata": {
        "id": "ZCSGKBwTrXGS"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2) Point Transformer Class"
      ],
      "metadata": {
        "id": "HkM_sVO_KqYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointTransformerV2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 patch_embed_depth=1,\n",
        "                 patch_embed_channels=16,\n",
        "                 patch_embed_groups= 4 ,\n",
        "                 patch_embed_neighbours=16,\n",
        "                 enc_depths=(2, 2, 6),\n",
        "                 enc_channels=(32, 64, 128),\n",
        "                 enc_groups=(8, 16, 32),\n",
        "                 enc_neighbours=(16, 16, 16),\n",
        "                 grid_sizes=(0.06, 0.12, 0.25),\n",
        "                 attn_qkv_bias=True,\n",
        "                 pe_multiplier=True,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0,\n",
        "                 enable_checkpoint=False,\n",
        "                 unpool_backend=\"map\"\n",
        "                 ):\n",
        "\n",
        "        super(PointTransformerV2, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.num_stages = len(enc_depths)\n",
        "        assert self.num_stages == len(enc_channels)\n",
        "        assert self.num_stages == len(enc_groups)\n",
        "        assert self.num_stages == len(enc_neighbours)\n",
        "        assert self.num_stages == len(grid_sizes)\n",
        "\n",
        "        self.feature_aggr = FeatureAggregation(in_channels, patch_embed_channels)\n",
        "\n",
        "        self.patch_embed = GVAPatchEmbed(\n",
        "            in_channels=patch_embed_channels,\n",
        "            embed_channels=patch_embed_channels,\n",
        "            groups=patch_embed_groups,\n",
        "            depth=patch_embed_depth,\n",
        "            neighbours=patch_embed_neighbours,\n",
        "            qkv_bias=attn_qkv_bias,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            enable_checkpoint=enable_checkpoint\n",
        "        )\n",
        "\n",
        "        enc_dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(enc_depths))]\n",
        "\n",
        "        enc_channels = [patch_embed_channels] + list(enc_channels)\n",
        "\n",
        "        self.enc_stages = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            enc = Encoder(\n",
        "                depth=enc_depths[i],\n",
        "                in_channels=enc_channels[i],\n",
        "                embed_channels=enc_channels[i + 1],\n",
        "                groups=enc_groups[i],\n",
        "                grid_size=grid_sizes[i],\n",
        "                neighbours=enc_neighbours[i],\n",
        "                qkv_bias=attn_qkv_bias,\n",
        "                pe_multiplier=pe_multiplier,\n",
        "                pe_bias=pe_bias,\n",
        "                attn_drop_rate=attn_drop_rate,\n",
        "                drop_path_rate=enc_dp_rates[sum(enc_depths[:i]):sum(enc_depths[:i + 1])],\n",
        "                enable_checkpoint=enable_checkpoint\n",
        "            )\n",
        "\n",
        "            self.enc_stages.append(enc)\n",
        "\n",
        "        self.pt_aggr = PointAggregation(enc_channels[-1])\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.BatchNorm1d(enc_channels[-1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(enc_channels[-1], enc_channels[-1])\n",
        "        )\n",
        "\n",
        "    def forward(self, data_dict):\n",
        "        coords, feat, batch = inpdict_to_point(data_dict)\n",
        "        offset = batch2offset(batch)\n",
        "        points = [coords.float(), feat.float(), batch.int()]\n",
        "\n",
        "        points = self.feature_aggr(points)\n",
        "        points = self.patch_embed(points)\n",
        "        skips = [[points]]\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            #print(f\"STAGE {i}\")\n",
        "            #print(f\"PRE ENCODER SHAPE: {points[0].shape} {points[1].shape} {points[2].shape}\")\n",
        "            points, cluster = self.enc_stages[i](points)\n",
        "            #print(f\"POST ENCODER SHAPE: {points[0].shape} {points[1].shape} {points[2].shape}\")\n",
        "            skips[-1].append(cluster)  # record grid cluster of pooling\n",
        "            skips.append([points])  # record points info of current stage\n",
        "\n",
        "        points = skips.pop(-1)[0]\n",
        "        coord, feat, batch = points\n",
        "        seg_logits = self.reg_head(self.pt_aggr(feat, batch))\n",
        "\n",
        "        return seg_logits"
      ],
      "metadata": {
        "id": "TliDZiXaLSgT"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Define AutoEncoder"
      ],
      "metadata": {
        "id": "jkiVtt_ULCLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1) Define Decoder"
      ],
      "metadata": {
        "id": "wwcWPImOLb3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointCloudDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dec_in_dim,\n",
        "                 dec_out_dim,\n",
        "                 num_points,\n",
        "                 num_layers = 1\n",
        "                 ):\n",
        "        super(PointCloudDecoder, self).__init__()\n",
        "\n",
        "        self.num_points = num_points\n",
        "        self.dec_out_dim = dec_out_dim\n",
        "\n",
        "        self.decoder = nn.ModuleList()\n",
        "        layer_dims = list(np.linspace(dec_in_dim,dec_out_dim*num_points,num=num_layers+1, dtype = int))\n",
        "        for i in range(num_layers):\n",
        "          layer = nn.Sequential(\n",
        "              nn.Linear(layer_dims[i], layer_dims[i+1]),\n",
        "              nn.BatchNorm1d(layer_dims[i+1]),\n",
        "              nn.ReLU(inplace=True)\n",
        "          )\n",
        "          self.decoder.append(layer)\n",
        "\n",
        "    def forward(self, points):\n",
        "        x = points\n",
        "        for layer in self.decoder:\n",
        "          x = layer(x)\n",
        "        output = x.view(x.size(0), self.num_points, self.dec_out_dim)\n",
        "        return output\n",
        "\n",
        "\n",
        "'''\n",
        "class PointCloudDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                latent_dim,\n",
        "                out_feat_dim,\n",
        "                hidden_dims=(256, 128, 64),\n",
        "                num_points_template=1024  # Default template size\n",
        "                ):\n",
        "        super(PointCloudDecoder, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.out_feat_dim = out_feat_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_points_template = num_points_template\n",
        "\n",
        "        # Create a learnable point template\n",
        "        self.point_template = nn.Parameter(torch.randn(1, num_points_template, 3) * 0.1)\n",
        "\n",
        "        # MLP to process latent vector\n",
        "        self.latent_mlp = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dims[0]),\n",
        "            nn.LayerNorm(hidden_dims[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[0]),\n",
        "            nn.LayerNorm(hidden_dims[0]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Deformation MLP - combines point template with latent features\n",
        "        deform_layers = []\n",
        "        deform_input_dim = 3 + hidden_dims[0]  # Concat of point coords + latent\n",
        "        last_dim = deform_input_dim\n",
        "\n",
        "        for h in hidden_dims:\n",
        "            deform_layers.extend([\n",
        "                nn.Linear(last_dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ])\n",
        "            last_dim = h\n",
        "\n",
        "        self.deform_mlp = nn.Sequential(*deform_layers)\n",
        "\n",
        "        # Output heads\n",
        "        self.coord_head = nn.Linear(last_dim, 3)\n",
        "        if out_feat_dim > 0:\n",
        "            self.feat_head = nn.Linear(last_dim, out_feat_dim)\n",
        "\n",
        "    def forward(self, x, num_points_list):\n",
        "        \"\"\"\n",
        "        x: [B, latent_dim] - batch of latent vectors\n",
        "        num_points_list: list or tensor of length B - number of points to generate for each batch item\n",
        "        \"\"\"\n",
        "        coords_list = []\n",
        "        feats_list = []\n",
        "        B = x.size(0)\n",
        "\n",
        "        # Process latent vector\n",
        "        latent_features = self.latent_mlp(x)  # [B, hidden_dims[0]]\n",
        "\n",
        "        for i in range(B):\n",
        "            num_points = num_points_list[i]\n",
        "\n",
        "            # Get latent for this sample and expand\n",
        "            latent_feat = latent_features[i].unsqueeze(0).expand(num_points, -1)  # [num_points, hidden_dims[0]]\n",
        "\n",
        "            # Sample points from template\n",
        "            # If num_points > template size, we need to interpolate\n",
        "            if num_points <= self.num_points_template:\n",
        "                # Take first num_points from template\n",
        "                template_points = self.point_template[0, :num_points, :]\n",
        "            else:\n",
        "                # Interpolate (sample with repetition)\n",
        "                indices = torch.linspace(0, self.num_points_template-1, num_points).long()\n",
        "                template_points = self.point_template[0, indices, :]\n",
        "\n",
        "            # Combine point coords with latent features\n",
        "            point_features = torch.cat([template_points, latent_feat], dim=-1)  # [num_points, 3+hidden_dims[0]]\n",
        "\n",
        "            # Apply deformation network\n",
        "            deformed_features = self.deform_mlp(point_features)  # [num_points, last_hidden_dim]\n",
        "\n",
        "            # Generate output coordinates (as offsets to template)\n",
        "            coord_offsets = self.coord_head(deformed_features)  # [num_points, 3]\n",
        "            coords = template_points + coord_offsets  # Apply offset to template\n",
        "            coords_list.append(coords)\n",
        "\n",
        "            # Generate output features if needed\n",
        "            if self.out_feat_dim > 0:\n",
        "                feats = self.feat_head(deformed_features)  # [num_points, feat_dim]\n",
        "                feats_list.append(feats)\n",
        "\n",
        "        # Concatenate results from batch\n",
        "        coords_out = torch.cat(coords_list, dim=0)  # [sum(num_points), 3]\n",
        "\n",
        "        if self.out_feat_dim > 0:\n",
        "            feats_out = torch.cat(feats_list, dim=0)  # [sum(num_points), feat_dim]\n",
        "            return coords_out, feats_out\n",
        "        else:\n",
        "            return coords_out\n",
        "\n",
        "'''\n",
        "print()"
      ],
      "metadata": {
        "id": "R5FYgNLzx_rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56291c08-101c-4365-eba9-3ec0a2fc1d88"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2) Define AutoEncoder Class"
      ],
      "metadata": {
        "id": "ZLdEY4CWLmGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, config_dict):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.config_dict = config_dict\n",
        "    self.point_transformer = PointTransformerV2(in_channels = config_dict['enc_ip_dim'])\n",
        "    self.point_cloud_decoder = PointCloudDecoder(dec_in_dim = config_dict['enc_out_dim'],\n",
        "                                                 dec_out_dim = config_dict['enc_ip_dim'],\n",
        "                                                 num_points = config_dict['num_points'])\n",
        "    self.enc_ip_dim = config_dict['enc_ip_dim']\n",
        "\n",
        "  def forward(self, points):\n",
        "    feat = self.point_transformer(points)\n",
        "    output = self.point_cloud_decoder(feat)\n",
        "    output = output.reshape(-1,self.enc_ip_dim)\n",
        "    return feat, output\n",
        "\n",
        "'''\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, config_dict):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.config_dict = config_dict\n",
        "    self.point_transformer = PointTransformerV2(in_channels = config_dict['enc_ip_dim'])\n",
        "    self.point_cloud_decoder = PointCloudDecoder(latent_dim = config_dict['latent_dim'],\n",
        "                                                 out_feat_dim = config_dict['enc_ip_dim'])\n",
        "\n",
        "  def forward(self, points):\n",
        "    num_points_list = points['num_points']\n",
        "    feat = self.point_transformer(points)\n",
        "    output = self.point_cloud_decoder(feat, num_points_list)\n",
        "\n",
        "\n",
        "    return feat, output\n",
        "'''\n",
        "print()"
      ],
      "metadata": {
        "id": "X3kUg_qSsZYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39217083-7517-49a0-bf60-cbefdfff7a56"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Define Configs"
      ],
      "metadata": {
        "id": "JuiVzYpCLxJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {'enc_ip_dim':1,\n",
        "                'batch_size':8,\n",
        "                'val_batch_size':1,\n",
        "                'dmodel':128,\n",
        "                'n_head':8,\n",
        "                'num_enc':2,\n",
        "                'num_points': 4096,\n",
        "                'dropout':0.1,\n",
        "                'enc_out_dim':128,\n",
        "                'latent_dim': 128,\n",
        "                'total_samples': 2000,\n",
        "                'num_epochs': 5}\n"
      ],
      "metadata": {
        "id": "YOMa-kP0ti8f"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Training"
      ],
      "metadata": {
        "id": "ZS0VVhnYL8fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.1) Example Code"
      ],
      "metadata": {
        "id": "WfAnCtjjMBPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST CODE\n",
        "example_pc = np.load('2024-10-28_0.npy')\n",
        "example_pc = example_pc[:config_dict['num_points'],:]\n",
        "offset = torch.tensor([0, len(example_pc)]).long()\n",
        "batch = (offset2batch(offset)*0).contiguous()\n",
        "enc_inp = torch.from_numpy(example_pc[:, :]).float().contiguous()\n",
        "data_dict = {'enc_inp':enc_inp,'inp_batch_ids': batch, 'num_points': [len(example_pc)]}\n",
        "print(\"enc_inp is contiguous:\", enc_inp.is_contiguous())\n",
        "print(\"inp_batch_ids is contiguous:\", batch.is_contiguous())\n",
        "\n",
        "\n",
        "example_point_transformer = PointTransformerV2(in_channels = config_dict['enc_ip_dim'])\n",
        "example_point_transformer.eval()\n",
        "example_autoencoder = AutoEncoder(config_dict)\n",
        "example_autoencoder.eval()\n",
        "features, output = example_autoencoder(data_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "collapsed": true,
        "id": "D2FIf0QfyhBP",
        "outputId": "4312cf23-0b14-4669-9e5a-9fdf5097c023"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enc_inp is contiguous: True\n",
            "inp_batch_ids is contiguous: True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4096 and 16x16)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-227-eb2961190cd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mexample_autoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mexample_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-225-e12e0bfc45c4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_cloud_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_ip_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-223-38dcbaa29d57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#points = self.feature_aggr(points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-166-4926d5fab746>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch2offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4096 and 16x16)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6IiBeV0G8mri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZERvXOoTyT_"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2) Define Class to Format Data"
      ],
      "metadata": {
        "id": "B3YIuR0ZMZqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PC_dataset_individuals(object):\n",
        "  def __init__(self, root_dir, pc_dir_list, config_dict):\n",
        "    self.pc_dir_list = pc_dir_list\n",
        "    self.root_dir = root_dir\n",
        "    self.num_points = config_dict['num_points']\n",
        "\n",
        "\n",
        "  def get_sample(self, idx, count):\n",
        "\n",
        "    enc_inp = []\n",
        "    inp_batch_ids = []\n",
        "\n",
        "    pc_df = []\n",
        "    pc_df_good_points = 0\n",
        "    while pc_df_good_points < 5:\n",
        "      if os.path.exists(os.path.join(self.root_dir, self.pc_dir_list[idx])):\n",
        "          #print(os.path.join(self.root_dir, self.pc_dir_list[idx]))\n",
        "          pc_df = np.load(os.path.join(self.root_dir, self.pc_dir_list[idx]))\n",
        "          if self.num_points < len(pc_df):\n",
        "            pc_df = pc_df[:self.num_points,:]\n",
        "          pc_df_good_points = len(pc_df[(pc_df[:,3] >= -5) & (pc_df[:,3] <= 5)])\n",
        "\n",
        "          if idx == len(self.pc_dir_list)-1:\n",
        "            idx = 0\n",
        "          else:\n",
        "            idx += 1\n",
        "      if np.isnan(pc_df).any():\n",
        "          print('Nan in file: ')\n",
        "          pc_df = []\n",
        "\n",
        "    enc_inp.append(pc_df)\n",
        "    inp_batch_ids.extend((count + np.zeros_like(pc_df[:,0])).tolist())\n",
        "\n",
        "\n",
        "    enc_inp = torch.from_numpy(np.concatenate(enc_inp, axis = 0))\n",
        "    inp_batch_ids = torch.from_numpy(np.asarray(inp_batch_ids)).int()\n",
        "\n",
        "    #enc_inp[:,3] = (enc_inp[:,3] + 30)/45\n",
        "\n",
        "\n",
        "    data_dict = {'enc_inp': enc_inp,\n",
        "                 'inp_batch_ids': inp_batch_ids,\n",
        "                 'num_points': torch.tensor([len(enc_inp)])}\n",
        "\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "  def data_len(self):\n",
        "    return len(self.pc_dir_list)\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    batch_dict = dict.fromkeys(list(batch[0].keys()))\n",
        "    for key in list(batch_dict.keys()):\n",
        "      bkey = [b[key] for b in batch]\n",
        "      batch_dict[key] = torch.cat(bkey, dim = 0)\n",
        "\n",
        "    return batch_dict\n",
        "\n",
        "  def get_batch_single_pc(self, batch_size): # Two functions internally : get_sample, collate_fn\n",
        "\n",
        "    idxs = np.arange(self.data_len()).astype(int)\n",
        "    np.random.shuffle(idxs)\n",
        "    random_idxs = idxs[:batch_size]\n",
        "\n",
        "    databatch = []\n",
        "    for count,b in enumerate(random_idxs):\n",
        "      databatch.append(self.get_sample(b,count))\n",
        "\n",
        "    return self.collate_fn(databatch)"
      ],
      "metadata": {
        "id": "2sSMzY8iEQkA"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3) Define Loss Criterions"
      ],
      "metadata": {
        "id": "DHIFR2qqpoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DON'T NEED THESE LOSSES. GO BACK TO HUBERLOSSES\n",
        "'''\n",
        "def chamfer_distance(x, y):\n",
        "    \"\"\"\n",
        "    Calculate the Chamfer Distance between two point clouds\n",
        "    x: [N, D] first point cloud (e.g., predicted)\n",
        "    y: [M, D] second point cloud (e.g., target)\n",
        "    Returns the Chamfer Distance between the two point clouds\n",
        "    \"\"\"\n",
        "    # Reshape to [1, N, D] and [1, M, D] if inputs are not batched\n",
        "    if x.dim() == 2:\n",
        "        x = x.unsqueeze(0)\n",
        "    if y.dim() == 2:\n",
        "        y = y.unsqueeze(0)\n",
        "\n",
        "    # Get batch size\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    xx = torch.sum(x**2, dim=2, keepdim=True)       # [B, N, 1]\n",
        "    yy = torch.sum(y**2, dim=2, keepdim=True)       # [B, M, 1]\n",
        "\n",
        "    # Compute all pairwise distances using matrix multiplication\n",
        "    inner = -2 * torch.matmul(x, y.transpose(1, 2))   # [B, N, M]\n",
        "    distances = xx + inner + yy.transpose(1, 2)       # [B, N, M]\n",
        "\n",
        "    # Get min distance for each point in x to any point in y\n",
        "    mins_x, _ = torch.min(distances, dim=2)  # [B, N]\n",
        "\n",
        "    # Get min distance for each point in y to any point in x\n",
        "    mins_y, _ = torch.min(distances, dim=1)  # [B, M]\n",
        "\n",
        "    # Compute the mean over points and add both directions\n",
        "    chamfer_dist = torch.mean(mins_x, dim=1) + torch.mean(mins_y, dim=1)  # [B]\n",
        "\n",
        "    # Return the mean over the batch\n",
        "    return torch.mean(chamfer_dist)\n",
        "\n",
        "class PointCloudLoss(nn.Module):\n",
        "    \"\"\"Combined loss for point cloud reconstruction\"\"\"\n",
        "    def __init__(self, chamfer_weight=1.0, feature_weight=0.1):\n",
        "        super(PointCloudLoss, self).__init__()\n",
        "        self.chamfer_weight = chamfer_weight\n",
        "        self.feature_weight = feature_weight\n",
        "        self.feature_criterion = nn.HuberLoss()\n",
        "\n",
        "    def forward(self, pred_coords, target_coords, pred_feats=None, target_feats=None):\n",
        "        # Chamfer distance for coordinates\n",
        "        chamfer_loss = chamfer_distance(pred_coords, target_coords)\n",
        "\n",
        "        # Feature loss (if features are provided)\n",
        "        feature_loss = 0.0\n",
        "        if pred_feats is not None and target_feats is not None:\n",
        "            feature_loss = self.feature_criterion(pred_feats, target_feats)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.chamfer_weight * chamfer_loss + self.feature_weight * feature_loss\n",
        "\n",
        "        return total_loss, chamfer_loss, feature_loss\n",
        "'''\n",
        "print()"
      ],
      "metadata": {
        "id": "8V0Bc7uvE61E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d888e174-f5a3-47d3-c525-19d76496b1eb"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4) Define Train and Val Functions"
      ],
      "metadata": {
        "id": "GHGOt1f-M-pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_point_cloud(coords, original, reconstructed, save_path):\n",
        "    \"\"\"\n",
        "    Visualize original and reconstructed point clouds side by side\n",
        "\n",
        "    Args:\n",
        "        original: original point cloud coordinates [N, 3]\n",
        "        reconstructed: reconstructed point cloud coordinates [N, 3]\n",
        "        save_path: path to save the visualization\n",
        "    \"\"\"\n",
        "    # Convert to numpy from tensors\n",
        "    if isinstance(original, torch.Tensor):\n",
        "        original = original.detach().cpu().numpy()\n",
        "    if isinstance(reconstructed, torch.Tensor):\n",
        "        reconstructed = reconstructed.detach().cpu().numpy()\n",
        "    if isinstance(coords, torch.Tensor):\n",
        "        coords = coords.detach().cpu().numpy()\n",
        "\n",
        "    #add back coordinates\n",
        "    original = np.hstack((coords,original))\n",
        "    reconstructed = np.hstack((coords,reconstructed))\n",
        "    #print(f'original: {original[:3,:]}')\n",
        "    #print(f'\\nreconstructed {reconstructed[:3,:]}')\n",
        "    #Determine points in correct CNR band\n",
        "    original = original[(original[:,3] >= -5) & (original[:,3] <= 5)]\n",
        "    reconstructed = reconstructed[(reconstructed[:,3] >= -5) & (reconstructed[:,3] <= 5)]\n",
        "    #print(f'new original: {original[:3,:]}')\n",
        "    #print(f'\\nnew reconstructed {reconstructed[:3,:]}')\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Original point cloud\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    ax1.scatter(original[:, 0], original[:, 1], original[:, 2], s=1, c=original[:, 2], cmap='viridis')\n",
        "    ax1.set_title('Original Point Cloud')\n",
        "    ax1.set_xlabel('X')\n",
        "    ax1.set_ylabel('Y')\n",
        "    ax1.set_zlabel('Z')\n",
        "\n",
        "    # Reconstructed point cloud\n",
        "    ax2 = fig.add_subplot(122, projection='3d')\n",
        "    ax2.scatter(reconstructed[:, 0], reconstructed[:, 1], reconstructed[:, 2], s=1, c=reconstructed[:, 2], cmap='viridis')\n",
        "    ax2.set_title('Reconstructed Point Cloud')\n",
        "    ax2.set_xlabel('X')\n",
        "    ax2.set_ylabel('Y')\n",
        "    ax2.set_zlabel('Z')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def inspect_outputs(target, reconstructed, num_samples=5):\n",
        "    \"\"\"Inspect target and reconstructed point clouds\"\"\"\n",
        "    print(f\"\\nOutput Diagnostics (showing {num_samples} samples):\")\n",
        "\n",
        "    # Check if all reconstructed points are the same\n",
        "    recon_all_same = torch.allclose(\n",
        "        reconstructed[0].unsqueeze(0).expand_as(reconstructed),\n",
        "        reconstructed,\n",
        "        rtol=1e-4, atol=1e-4\n",
        "    )\n",
        "\n",
        "    if recon_all_same:\n",
        "        print(f\"CRITICAL ISSUE: All reconstructed points are identical!\")\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Target points - Mean: {target.mean(dim=0)}, Std: {target.std(dim=0)}\")\n",
        "    print(f\"Reconstructed points - Mean: {reconstructed.mean(dim=0)}, Std: {reconstructed.std(dim=0)}\")\n",
        "\n",
        "    # Show some examples\n",
        "    print(\"\\nSample points (target vs reconstructed):\")\n",
        "    for i in range(min(num_samples, len(target))):\n",
        "        print(f\"Point {i}:\")\n",
        "        print(f\"  Target:        {target[i]}\")\n",
        "        print(f\"  Reconstructed: {reconstructed[i]}\")\n",
        "\n",
        "    # Check variance across dimensions\n",
        "    print(f\"\\nVariance per dimension:\")\n",
        "    print(f\"  Target:        {torch.var(target, dim=0)}\")\n",
        "    print(f\"  Reconstructed: {torch.var(reconstructed, dim=0)}\")\n",
        "\n",
        "def train(model, train_dataset, batch_size, optimizer, scheduler, criterion, epoch, device, config_dict, vis_dir=None):\n",
        "    model.train()\n",
        "    epoch_start = time.time()\n",
        "    train_feat_loss = 0\n",
        "    num_steps_per_epoch = (train_dataset.data_len() // batch_size) + 1\n",
        "    count_good_steps = 0\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        # Make batch\n",
        "        data_dict = train_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        embedded_features, recon_feats = model(data_dict)\n",
        "        coords, feats, batch = inpdict_to_point(data_dict)\n",
        "        '''\n",
        "        print(f'embedded_features: {embedded_features.shape}')\n",
        "        print(f'recon_feats: {recon_feats.shape}')\n",
        "        print(f'coords: {coords.shape}')\n",
        "        print(f'feats: {feats.shape}')\n",
        "        print(f'batch: {batch.shape}')\n",
        "        print(data_dict['num_points'])\n",
        "        '''\n",
        "        coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # Visualisation\n",
        "        if vis_dir and step == 0 and epoch % 1 == 0:\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "            # Only visualize first point cloud in batch\n",
        "            first_pc_len = data_dict['num_points'][0].item()\n",
        "            visualise_point_cloud(\n",
        "                coords[:first_pc_len],\n",
        "                target_feats[:first_pc_len],\n",
        "                recon_feats[:first_pc_len],\n",
        "                os.path.join(vis_dir, f'epoch_{epoch}_train.png')\n",
        "            )\n",
        "\n",
        "        # === Sanity checks ===\n",
        "        if torch.isnan(recon_feats).any():\n",
        "            print(\"NaN in model output. Skipping step.\")\n",
        "            continue\n",
        "\n",
        "        # Loss criterion (only care about losses for features as coordinates should be same)\n",
        "        feat_loss = criterion(recon_feats.squeeze(-1).float(), target_feats.squeeze(-1).float())\n",
        "\n",
        "        # Backwards and optimise\n",
        "        feat_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_feat_loss += feat_loss.item()\n",
        "        count_good_steps += 1\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Epoch {epoch}, Step {step}/{num_steps_per_epoch}, \"\n",
        "              f\"Feat Loss: {feat_loss.item():.4f}, \"\n",
        "              f\"Time: {step_end - step_start:.2f}s\")\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f}s\")\n",
        "\n",
        "    # Return average losses\n",
        "    avg_feat_loss = train_feat_loss / max(count_good_steps, 1)\n",
        "\n",
        "    return avg_feat_loss\n",
        "\n",
        "def val(model, val_dataset, batch_size, criterion, epoch, device, config_dict, vis_dir=None):\n",
        "    model.eval()\n",
        "    epoch_start = time.time()\n",
        "    val_feat_loss = 0\n",
        "    num_steps_per_epoch = (val_dataset.data_len() // batch_size) + 1\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        data_dict = val_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        embedded_features, recon_feats = model(data_dict)\n",
        "        coords, feats, batch = inpdict_to_point(data_dict)\n",
        "        coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # Visualisation\n",
        "        if vis_dir and step == 0 and epoch % 1 == 0:\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "            # Only visualize first point cloud in batch\n",
        "            first_pc_len = data_dict['num_points'][0].item()\n",
        "            visualise_point_cloud(\n",
        "                coords[:first_pc_len],\n",
        "                target_feats[:first_pc_len],\n",
        "                recon_feats[:first_pc_len],\n",
        "                os.path.join(vis_dir, f'epoch_{epoch}_train.png')\n",
        "            )\n",
        "\n",
        "            # Debug output inspection\n",
        "            inspect_outputs(target_feats[:first_pc_len], recon_feats[:first_pc_len])\n",
        "\n",
        "        # Loss criterion (only care about losses for features as coordinates should be same)\n",
        "        feat_loss = criterion(recon_feats.squeeze(-1).float(), target_feats.squeeze(-1).float())\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Epoch: {epoch}, Step: {step}/{num_steps_per_epoch}, \"\n",
        "              f\"Feat Loss: {feat_loss.item():.4f}, \"\n",
        "              f\"Time: {step_end-step_start:.2f}s\")\n",
        "\n",
        "        val_feat_loss += feat_loss.item()\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    print(f'Time to complete validation: {time.time()-epoch_start:.2f}s')\n",
        "\n",
        "    # Return average losses\n",
        "    avg_feat_loss = val_feat_loss / num_steps_per_epoch\n",
        "\n",
        "    return avg_feat_loss"
      ],
      "metadata": {
        "id": "QJAwVOdyFfzE"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.5) Training Script"
      ],
      "metadata": {
        "id": "Tx8uxOT7NLdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up device stuff\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(device)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#Finding where the data is and making save and visualisation directories\n",
        "root = os.getcwd()\n",
        "lidar_dir = os.path.join(root, 'LiDAR')\n",
        "pcdata_dir = os.path.join(lidar_dir, 'pcloud_norm')\n",
        "pc_files = natsort.natsorted(os.listdir(pcdata_dir))\n",
        "pc_paths = [os.path.join(pcdata_dir, pcf) for pcf in pc_files]\n",
        "if config_dict['total_samples'] < len(pc_paths):\n",
        "  pc_paths = pc_paths[:config_dict['total_samples']]\n",
        "save_dir = os.path.join(lidar_dir, os.path.join('saved_models'))\n",
        "if not os.path.exists(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "vis_dir = os.path.join(lidar_dir, 'visualisations')\n",
        "os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "#Creating datasets\n",
        "train_val_pcs = pc_paths[:(int(len(pc_paths)*0.7))]\n",
        "test_pcs = pc_paths[(int(len(pc_paths)*0.7)):]\n",
        "train_idx = np.arange(len(train_val_pcs)).astype(int)[:int(0.9*len(train_val_pcs))]\n",
        "val_idx = np.arange(len(train_val_pcs)).astype(int)[int(0.9*len(train_val_pcs)):]\n",
        "train_pcs = np.asarray(train_val_pcs)[train_idx].tolist()\n",
        "val_pcs = np.asarray(train_val_pcs)[val_idx].tolist()\n",
        "train_dataset = PC_dataset_individuals(lidar_dir, train_pcs, config_dict)\n",
        "val_dataset = PC_dataset_individuals(lidar_dir, val_pcs, config_dict)\n",
        "test_dataset = PC_dataset_individuals(lidar_dir, test_pcs, config_dict)\n",
        "num_steps_per_epoch = (train_dataset.data_len() // config_dict['batch_size']) + 1\n",
        "\n",
        "#Initialising model stuff\n",
        "model = AutoEncoder(config_dict).to(device)\n",
        "criterion = nn.HuberLoss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0005,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                max_lr = 0.001,\n",
        "                                                epochs=config_dict['num_epochs'],\n",
        "                                                steps_per_epoch=num_steps_per_epoch)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config_dict['num_epochs']):\n",
        "    # Train\n",
        "    train_feat_loss = train(\n",
        "        model, train_dataset, config_dict['batch_size'],\n",
        "        optimizer, scheduler, criterion, epoch, device, config_dict,\n",
        "        vis_dir=os.path.join(vis_dir, 'train')\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "        val_feat_loss = val(\n",
        "            model, val_dataset, config_dict['val_batch_size'],\n",
        "            criterion, epoch, device, config_dict,\n",
        "            vis_dir=os.path.join(vis_dir, 'val')\n",
        "        )\n",
        "\n",
        "    # Update learning rate based on validation loss\n",
        "    scheduler.step(val_feat_loss)\n",
        "\n",
        "    print(f'Epoch {epoch} summary:')\n",
        "    print(f'  Train - Total: {train_feat_loss:.4f}')\n",
        "    print(f'  Val   - Total: {val_feat_loss:.4f}')\n",
        "\n",
        "    # Save model checkpoint\n",
        "    if epoch % 5 == 0 or epoch == config_dict['num_epochs'] - 1:\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f'model_epoch_{epoch}.pth'))\n",
        "\n",
        "    # Save best model\n",
        "    if epoch == 0 or val_feat_loss < min_val_loss:\n",
        "        min_val_loss = val_feat_loss\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
        "        print(f'Saved new best model with val_loss: {val_feat_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "EdcV9jKWNTKP",
        "outputId": "93b0aef5-a50f-4a6f-d31b-c98f5f9cfe1a"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32768 and 16x16)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-f9d51add4476>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     train_feat_loss = train(\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-230-0a8493f1b46a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, batch_size, optimizer, scheduler, criterion, epoch, device, config_dict, vis_dir)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0membedded_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpdict_to_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         '''\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-225-e12e0bfc45c4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_cloud_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_ip_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-223-38dcbaa29d57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#points = self.feature_aggr(points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-166-4926d5fab746>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch2offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32768 and 16x16)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6) Testing"
      ],
      "metadata": {
        "id": "uqlj3ZJCN3tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.1) Define Testing Function"
      ],
      "metadata": {
        "id": "kzdobW-TN6_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_dataset, batch_size, criterion, device, config_dict, vis_dir=None):\n",
        "    model.eval()\n",
        "    epoch_start = time.time()\n",
        "    test_loss = 0\n",
        "    test_coord_loss = 0\n",
        "    test_feat_loss = 0\n",
        "    num_steps_per_epoch = (test_dataset.data_len() // batch_size) + 1 * (test_dataset.data_len() % batch_size != 0)\n",
        "\n",
        "    recon_coords_list = []\n",
        "    recon_feats_list = []\n",
        "    target_coords_list = []\n",
        "    target_feats_list = []\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        data_dict = test_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        # === Forward pass ===\n",
        "        _, output = model(data_dict)\n",
        "        recon_coords = output[0]\n",
        "        recon_feats = output[1]\n",
        "\n",
        "        coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "        target_coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # === Store results for visualization ===\n",
        "        recon_coords_list.append(recon_coords.detach().cpu())\n",
        "        recon_feats_list.append(recon_feats.detach().cpu())\n",
        "        target_coords_list.append(target_coords.detach().cpu())\n",
        "        target_feats_list.append(target_feats.detach().cpu())\n",
        "\n",
        "        # === Loss computation ===\n",
        "        total_loss, coord_loss, feat_loss = criterion(\n",
        "            recon_coords, target_coords, recon_feats, target_feats\n",
        "        )\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Step: {step}/{num_steps_per_epoch}, \"\n",
        "              f\"Total Loss: {total_loss.item():.4f}, \"\n",
        "              f\"Coord Loss: {coord_loss.item():.4f}, \"\n",
        "              f\"Feat Loss: {feat_loss.item():.4f}, \"\n",
        "              f\"Time: {step_end-step_start:.2f}s\")\n",
        "\n",
        "        test_loss += total_loss.item()\n",
        "        test_coord_loss += coord_loss.item()\n",
        "        test_feat_loss += feat_loss.item()\n",
        "\n",
        "        # === Visualize test results ===\n",
        "        if vis_dir and step < 5:  # Visualize first 5 test samples\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "            # Only visualize first point cloud in batch\n",
        "            first_pc_len = data_dict['num_points'][0].item()\n",
        "            visualize_point_cloud(\n",
        "                target_coords[:first_pc_len],\n",
        "                recon_coords[:first_pc_len],\n",
        "                os.path.join(vis_dir, f'test_sample_{step}.png')\n",
        "            )\n",
        "\n",
        "            # Debug output inspection\n",
        "            inspect_outputs(target_coords[:first_pc_len], recon_coords[:first_pc_len])\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    print(f'Time to complete testing: {time.time()-epoch_start:.2f}s')\n",
        "\n",
        "    # Return average losses and result lists\n",
        "    avg_loss = test_loss / num_steps_per_epoch\n",
        "    avg_coord_loss = test_coord_loss / num_steps_per_epoch\n",
        "    avg_feat_loss = test_feat_loss / num_steps_per_epoch\n",
        "\n",
        "    return (avg_loss, avg_coord_loss, avg_feat_loss,\n",
        "            recon_coords_list, recon_feats_list,\n",
        "            target_coords_list, target_feats_list)"
      ],
      "metadata": {
        "id": "Y9_to0_5Fm2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.2) Testing Script"
      ],
      "metadata": {
        "id": "1vKiGVYmVLli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the best model\n",
        "model.load_state_dict(torch.load(os.path.join(save_dir, 'best_model.pth')))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_results = test(\n",
        "        model, test_dataset, batch_size=1, criterion=criterion,\n",
        "        device=device, config_dict=config_dict,\n",
        "        vis_dir=os.path.join(vis_dir, 'test')\n",
        "    )\n",
        "\n",
        "print(f'Final test loss: {test_results[0]:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "zCAqlg_IVPP4",
        "outputId": "b1430b62-070b-4c22-883c-e1ca86fb7426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'PC_dataset_individuals' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e2058f448ce6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     test_results = test(\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mvis_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'PC_dataset_individuals' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}