{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvOfxnSCZ5Y7oIS7LU1DRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarwilkins1707/DeepLearningThesis/blob/main/PTV2_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Install Required Libraries"
      ],
      "metadata": {
        "id": "7KHqF7N8KD9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lwv9QOGgqGGX",
        "outputId": "877d2918-2b6c-4a93-e09e-f08b173ee659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt26cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-_hihi6n9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-_hihi6n9\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 36aed7c28140a54f27aea6c2429636ff0d1c84b8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Collecting xxhash (from torch-geometric==2.7.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2025.4.26)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1206610 sha256=f8cfa795166d87f2c8f563c0238a71f6ec0cd95bae6b67a1ea7def06ed44891e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a8n6keg_/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: xxhash, torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0 xxhash-3.5.0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html #Use prebuilt wheels to make code faster\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!set CUDA_LAUNCH_BLOCKING = 1\n",
        "!set TORCH_USE_CUDA_DSA = 1"
      ],
      "metadata": {
        "id": "7TrOk0IoqIF-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import natsort\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "from torch_cluster import knn\n",
        "import einops\n",
        "from copy import deepcopy\n",
        "import math\n",
        "from torch_geometric.nn.pool import voxel_grid\n",
        "from torch_scatter import segment_csr, composite\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fybzfupb2AUI",
        "outputId": "a0f64de7-28ef-48f9-dff1-d11e354375d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Define Classes for PTV2 Algorithm"
      ],
      "metadata": {
        "id": "LRipkaeqKNdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PointTransformerV2 Set Up Classes"
      ],
      "metadata": {
        "id": "l_3lydurK1JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def offset2batch(offset):\n",
        "    return torch.cat([torch.tensor([i] * (o - offset[i - 1])) if i > 0 else\n",
        "                      torch.tensor([i] * o) for i, o in enumerate(offset)],\n",
        "                     dim=0).long().to(offset.device)\n",
        "\n",
        "\n",
        "def batch2offset(batch):\n",
        "    return torch.cumsum(batch.bincount(), dim=0).long()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "def inpdict_to_point(inp_dict):\n",
        "  enc_ip = inp_dict['enc_inp']\n",
        "  offset = inp_dict['inp_batch_ids']\n",
        "  coords = enc_ip[:,:3]\n",
        "  feat = enc_ip[:,3:-2]\n",
        "  time = enc_ip[:,-1]\n",
        "  return coords,feat,time,offset\n",
        "\n",
        "def grouping(idx,\n",
        "             feat,\n",
        "             xyz,\n",
        "             time,\n",
        "             new_xyz=None,\n",
        "             with_xyz=False):\n",
        "    xyz = xyz.contiguous()\n",
        "    feat = feat.contiguous()\n",
        "    time = time.contiguous()\n",
        "\n",
        "    if new_xyz is None:\n",
        "        new_xyz = xyz\n",
        "        new_time = time\n",
        "\n",
        "    assert xyz.is_contiguous()\n",
        "    assert feat.is_contiguous()\n",
        "    assert time.is_contiguous()\n",
        "\n",
        "    m, nsample, c = idx.shape[0], idx.shape[1], feat.shape[1]\n",
        "    xyz = torch.cat([xyz, torch.zeros([1, 3]).to(xyz.device)], dim=0)\n",
        "    feat = torch.cat([feat, torch.zeros([1, c]).to(feat.device)], dim=0)\n",
        "    time = torch.cat([time, torch.zeros([1,]).to(time.device)])\n",
        "    grouped_feat = feat[idx.view(-1).long(), :].view(m, nsample, c)  # (m, num_sample, c)\n",
        "\n",
        "    if with_xyz:\n",
        "        assert new_xyz.is_contiguous()\n",
        "        mask = torch.sign(idx + 1)\n",
        "        grouped_xyz = xyz[idx.view(-1).long(), :].view(m, nsample, 3) - new_xyz.unsqueeze(1)  # (m, num_sample, 3)\n",
        "        grouped_xyz = torch.einsum(\"n s c, n s -> n s c\", grouped_xyz, mask)  # (m, num_sample, 3)\n",
        "\n",
        "        grouped_time = time[idx.view(-1).long()].view(m, nsample) - new_time.unsqueeze(1)  # (m, num_sample)\n",
        "        grouped_time = torch.einsum(\"n s, n s -> n s\", grouped_time, mask)  # (m, num_sample)\n",
        "        return torch.cat((grouped_xyz, grouped_time.unsqueeze(-1), grouped_feat), -1)\n",
        "    else:\n",
        "        return grouped_feat\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
        "    if keep_prob > 0.0 and scale_by_keep:\n",
        "        random_tensor.div_(keep_prob)\n",
        "    return x * random_tensor\n"
      ],
      "metadata": {
        "id": "U3dwkxLVsInB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.scale_by_keep = scale_by_keep\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
        "\n",
        "class Temporal_Embedding(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Temporal_Embedding, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.day_emb = nn.Embedding(32, output_dim)\n",
        "    self.hour_emb = nn.Embedding(24, output_dim)\n",
        "    self.month_emb = nn.Embedding(13, output_dim)\n",
        "    self.light_emb = nn.Embedding(2, output_dim)\n",
        "\n",
        "    self.val_emb = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, feats, datetime):\n",
        "\n",
        "    if len(feats.shape)==3:\n",
        "        B, S, _ = feats.shape\n",
        "    elif len(feats.shape)==4:\n",
        "        B, S, V, _ = feats.shape\n",
        "\n",
        "    month = datetime[:,1].int().to(feats.device)\n",
        "    day = datetime[:,2].int().to(feats.device)\n",
        "    hour = datetime[:,3].int().to(feats.device)\n",
        "\n",
        "    light_id = torch.where((day > 6) & (day < 19), torch.tensor(1, device=feats.device), torch.tensor(0, device=feats.device)).int()\n",
        "\n",
        "    day_emb = self.day_emb(day)\n",
        "    hour_emb = self.hour_emb(hour)\n",
        "    month_emb = self.month_emb(month)\n",
        "    light_emb = self.light_emb(light_id)\n",
        "\n",
        "    if feats is not None:\n",
        "        val_emb = self.val_emb(feats)\n",
        "\n",
        "    if len(feats.shape) == 4:\n",
        "        return day_emb.reshape(B,S,-1).unsqueeze(2) + hour_emb.reshape(B,S,-1).unsqueeze(2) + month_emb.reshape(B,S,-1).unsqueeze(2) + val_emb + light_emb.reshape(B,S,-1).unsqueeze(2)\n",
        "    else:\n",
        "        return day_emb.reshape(B,S,-1) + hour_emb.reshape(B,S,-1) + month_emb.reshape(B,S,-1) + val_emb + light_emb.reshape(B,S,-1)\n",
        "\n",
        "class FeatureAggregation(nn.Module):\n",
        "  def __init__(self, in_channels, dmodel):\n",
        "    super(FeatureAggregation, self).__init__()\n",
        "    #self.config_dict = config_dict\n",
        "    self.q = nn.Parameter(torch.randn(1, dmodel).float())\n",
        "    self.v = nn.Conv1d(in_channels = in_channels,\n",
        "                       out_channels = int(in_channels*dmodel),\n",
        "                       kernel_size = 1,\n",
        "                       groups = in_channels)\n",
        "    self.k = nn.Conv1d(in_channels = in_channels,\n",
        "                       out_channels = int(in_channels*dmodel),\n",
        "                       kernel_size = 1,\n",
        "                       groups = in_channels)\n",
        "\n",
        "  def forward(self, points):\n",
        "\n",
        "    coords, feats, time, batch = points\n",
        "    BK, fc = feats.shape\n",
        "    feat_v = self.v(feats.float().unsqueeze(2)).reshape(BK, fc, -1)\n",
        "    feat_k = self.k(feats.float().unsqueeze(2)).reshape(BK, fc, -1)\n",
        "\n",
        "    qk = torch.einsum('qd, bkd -> bqk', self.q, feat_k)\n",
        "    attn = nn.Softmax(dim=-1)(qk)\n",
        "    out = torch.einsum('bkd, bqk -> bqd', feat_v, attn)\n",
        "\n",
        "    return [coords, out.squeeze(1), time, batch]\n",
        "\n",
        "class PointAggregation(nn.Module):\n",
        "    def __init__(self, dmodel):\n",
        "        super(PointAggregation, self).__init__()\n",
        "        #self.configs = configs\n",
        "\n",
        "        self.q = nn.Parameter(torch.randn(1, dmodel).float())\n",
        "        self.v = nn.Linear(dmodel, dmodel)\n",
        "        self.k = nn.Linear(dmodel, dmodel)\n",
        "\n",
        "    def forward(self, feats, batch):\n",
        "\n",
        "          offset = batch2offset(batch)\n",
        "          offset = torch.cat([offset.new_zeros(1), offset])\n",
        "          #print(feats.shape, batch.shape)\n",
        "          feats_v = self.v(feats.float())\n",
        "          feats_k = self.k(feats.float())\n",
        "\n",
        "          qk = torch.einsum('qd,kd -> kq', self.q, feats_k)\n",
        "          attn = composite.scatter_softmax(qk, batch.long(), dim = 0)\n",
        "          out = segment_csr(torch.einsum('qk,qd -> qd', attn, feats_v), offset)\n",
        "\n",
        "          return out"
      ],
      "metadata": {
        "id": "CzCdTnUYsAQF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedLinear(nn.Module):\n",
        "    __constants__ = ['in_features', 'out_features', \"groups\"]\n",
        "    in_features: int\n",
        "    out_features: int\n",
        "    groups: int\n",
        "    weight: torch.Tensor\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, groups: int,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(GroupedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.groups = groups\n",
        "        assert in_features & groups == 0\n",
        "        assert out_features % groups == 0\n",
        "        # for convenient, currently only support out_features == groups, one output\n",
        "        assert out_features == groups\n",
        "        self.weight = nn.Parameter(torch.empty((1, in_features), **factory_kwargs))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return (input * self.weight).reshape(\n",
        "            list(input.shape[:-1]) + [self.groups, input.shape[-1] // self.groups]).sum(-1)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class PointBatchNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Batch Normalization for Point Clouds data in shape of [B*N, C], [B*N, L, C]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.BatchNorm1d(embed_channels)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        if input.dim() == 3:\n",
        "            return self.norm(input.transpose(1, 2).contiguous()).transpose(1, 2).contiguous()\n",
        "        elif input.dim() == 2:\n",
        "            return self.norm(input)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class GroupedVectorAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 attn_drop_rate=0.,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=True,\n",
        "                 pe_bias=True\n",
        "                 ):\n",
        "        super(GroupedVectorAttention, self).__init__()\n",
        "        self.embed_channels = embed_channels\n",
        "        self.groups = groups\n",
        "        assert embed_channels % groups == 0\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.pe_multiplier = pe_multiplier\n",
        "        self.pe_bias = pe_bias\n",
        "\n",
        "        self.linear_q = nn.Sequential(\n",
        "            nn.Linear(embed_channels, embed_channels, bias=qkv_bias),\n",
        "            PointBatchNorm(embed_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.linear_k = nn.Sequential(\n",
        "            nn.Linear(embed_channels, embed_channels, bias=qkv_bias),\n",
        "            PointBatchNorm(embed_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.linear_v = nn.Linear(embed_channels, embed_channels, bias=qkv_bias)\n",
        "\n",
        "        if self.pe_multiplier:\n",
        "            self.linear_p_multiplier = nn.Sequential(\n",
        "                nn.Linear(3, embed_channels),\n",
        "                PointBatchNorm(embed_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(embed_channels, embed_channels),\n",
        "            )\n",
        "            self.linear_t_multiplier = nn.Sequential(\n",
        "                nn.Linear(1, embed_channels),\n",
        "                PointBatchNorm(embed_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(embed_channels, embed_channels),\n",
        "            )\n",
        "        if self.pe_bias:\n",
        "            self.linear_p_bias = nn.Sequential(\n",
        "                nn.Linear(3, embed_channels),\n",
        "                PointBatchNorm(embed_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(embed_channels, embed_channels),\n",
        "            )\n",
        "            self.linear_t_bias = nn.Sequential(\n",
        "                nn.Linear(1, embed_channels),\n",
        "                PointBatchNorm(embed_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(embed_channels, embed_channels),\n",
        "            )\n",
        "\n",
        "        self.weight_encoding = nn.Sequential(\n",
        "            GroupedLinear(embed_channels, groups, groups),\n",
        "            PointBatchNorm(groups),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(groups, groups)\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.attn_drop = nn.Dropout(attn_drop_rate)\n",
        "\n",
        "    def forward(self, feat, coord, time, reference_index):\n",
        "        query, key, value = self.linear_q(feat), self.linear_k(feat), self.linear_v(feat)\n",
        "        key = grouping(reference_index, key, coord, time, with_xyz=True)\n",
        "        value = grouping(reference_index, value, coord, time, with_xyz=False)\n",
        "        pos, del_time, key = key[:, :, 0:3], key[:,:,3:4],  key[:, :, 4:]\n",
        "        relation_qk = key - query.unsqueeze(1)\n",
        "        tem = self.linear_t_multiplier(del_time)\n",
        "        teb = self.linear_t_bias(del_time)\n",
        "\n",
        "        if self.pe_multiplier:\n",
        "            pem = self.linear_p_multiplier(pos)\n",
        "            relation_qk = relation_qk * pem\n",
        "        if self.pe_bias:\n",
        "            peb = self.linear_p_bias(pos)\n",
        "            teb = self.linear_t_bias(del_time)\n",
        "            relation_qk = relation_qk + peb\n",
        "            value = (value + peb)\n",
        "\n",
        "        value = value * tem + teb\n",
        "        relation_qk = relation_qk * tem + teb\n",
        "\n",
        "        weight = self.weight_encoding(relation_qk)\n",
        "        weight = self.attn_drop(self.softmax(weight))\n",
        "\n",
        "        mask = torch.sign(reference_index + 1)\n",
        "        weight = torch.einsum(\"n s g, n s -> n s g\", weight, mask)\n",
        "        value = einops.rearrange(value, \"n ns (g i) -> n ns g i\", g=self.groups)\n",
        "        feat = torch.einsum(\"n s g i, n s g -> n g i\", value, weight)\n",
        "        feat = einops.rearrange(feat, \"n g i -> n (g i)\")\n",
        "        return feat\n",
        "\n",
        "class GridPool(nn.Module):\n",
        "    \"\"\"\n",
        "    Partition-based Pooling (Grid Pooling)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 grid_size,\n",
        "                 bias=False):\n",
        "        super(GridPool, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        self.fc = nn.Linear(in_channels, out_channels, bias=bias)\n",
        "        self.norm = PointBatchNorm(out_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, points, start=None):\n",
        "        coord, feat, time, batch = points\n",
        "        offset = batch2offset(batch)\n",
        "\n",
        "        feat = self.act(self.norm(self.fc(feat)))\n",
        "\n",
        "        start = segment_csr(coord, torch.cat([batch.new_zeros(1), torch.cumsum(batch.bincount(), dim=0)]),\n",
        "                            reduce=\"min\") if start is None else start\n",
        "\n",
        "        cluster = voxel_grid(pos=coord - start[batch], size=self.grid_size, batch=batch, start=0)\n",
        "\n",
        "        unique, cluster, counts = torch.unique(cluster, sorted=True, return_inverse=True, return_counts=True)\n",
        "        _, sorted_cluster_indices = torch.sort(cluster)\n",
        "        idx_ptr = torch.cat([counts.new_zeros(1), torch.cumsum(counts, dim=0)])\n",
        "        coord = segment_csr(coord[sorted_cluster_indices], idx_ptr, reduce=\"mean\")\n",
        "        time = segment_csr(time[sorted_cluster_indices], idx_ptr, reduce=\"mean\")\n",
        "        feat = segment_csr(feat[sorted_cluster_indices], idx_ptr, reduce=\"max\")\n",
        "        batch = batch[idx_ptr[:-1]]\n",
        "        offset = batch2offset(batch)\n",
        "        return [coord, feat, time, batch], cluster\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 enable_checkpoint=False\n",
        "                 ):\n",
        "\n",
        "        super(Block, self).__init__()\n",
        "        self.attn = GroupedVectorAttention(\n",
        "            embed_channels=embed_channels,\n",
        "            groups=groups,\n",
        "            qkv_bias=qkv_bias,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias\n",
        "        )\n",
        "        self.fc1 = nn.Linear(embed_channels, embed_channels, bias=False)\n",
        "        self.fc3 = nn.Linear(embed_channels, embed_channels, bias=False)\n",
        "        self.norm1 = PointBatchNorm(embed_channels)\n",
        "        self.norm2 = PointBatchNorm(embed_channels)\n",
        "        self.norm3 = PointBatchNorm(embed_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.enable_checkpoint = enable_checkpoint\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, points, reference_index):\n",
        "        coord, feat, time, batch = points\n",
        "        offset = batch2offset(batch)\n",
        "        identity = feat\n",
        "        feat = self.act(self.norm1(self.fc1(feat)))\n",
        "        feat = self.attn(feat, coord, time, reference_index) #\\\n",
        "            #if not self.enable_checkpoint else checkpoint(self.attn, feat, coord, time, reference_index)\n",
        "        feat = self.act(self.norm2(feat))\n",
        "        feat = self.norm3(self.fc3(feat))\n",
        "        feat = identity + self.drop_path(feat)\n",
        "        feat = self.act(feat)\n",
        "        return [coord, feat, time, batch]\n",
        "\n",
        "class BlockSequence(nn.Module):\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 neighbours=16,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 enable_checkpoint=False\n",
        "                 ):\n",
        "        super(BlockSequence, self).__init__()\n",
        "\n",
        "        if isinstance(drop_path_rate, list):\n",
        "            drop_path_rates = drop_path_rate\n",
        "            assert len(drop_path_rates) == depth\n",
        "        elif isinstance(drop_path_rate, float):\n",
        "            drop_path_rates = [deepcopy(drop_path_rate) for _ in range(depth)]\n",
        "        else:\n",
        "            drop_path_rates = [0. for _ in range(depth)]\n",
        "        self.neighbours = neighbours\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            block = Block(\n",
        "                embed_channels=embed_channels,\n",
        "                groups=groups,\n",
        "                qkv_bias=qkv_bias,\n",
        "                pe_multiplier=pe_multiplier,\n",
        "                pe_bias=pe_bias,\n",
        "                attn_drop_rate=attn_drop_rate,\n",
        "                drop_path_rate=drop_path_rates[i],\n",
        "                enable_checkpoint=enable_checkpoint\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    def forward(self, points):\n",
        "        coord, feat, time, batch = points\n",
        "\n",
        "        # Process each batch separately\n",
        "        batch_size = batch.max().item() + 1\n",
        "        all_reference_indices = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # Get points for this batch\n",
        "            batch_mask = (batch == b)\n",
        "            if not batch_mask.any():\n",
        "                continue\n",
        "\n",
        "            # Get the coordinates for this batch\n",
        "            batch_coord = coord[batch_mask]\n",
        "\n",
        "            # Get indices in the original tensor\n",
        "            batch_indices = torch.nonzero(batch_mask).squeeze(1)\n",
        "\n",
        "            # Run KNN for this batch only\n",
        "            batch_knn = knn(batch_coord, batch_coord, self.neighbours)\n",
        "\n",
        "            # Map KNN indices back to original indices\n",
        "            mapped_indices = batch_indices[batch_knn[1]]\n",
        "\n",
        "            # Create row indices corresponding to the original points\n",
        "            row_indices = batch_indices[batch_knn[0]]\n",
        "\n",
        "            # Store the point-neighbor pairs\n",
        "            for i, point_idx in enumerate(batch_indices):\n",
        "                # Find all neighbors for this point\n",
        "                neighbors = mapped_indices[batch_knn[0] == i]\n",
        "\n",
        "                # Pad if necessary to ensure k neighbors\n",
        "                if len(neighbors) < self.neighbours:\n",
        "                    padding = neighbors[0].repeat(self.neighbours - len(neighbors))\n",
        "                    neighbors = torch.cat([neighbors, padding])\n",
        "\n",
        "                # If we have too many neighbors, just take the first k\n",
        "                neighbors = neighbors[:self.neighbours]\n",
        "\n",
        "                all_reference_indices.append(neighbors)\n",
        "\n",
        "        # Stack all reference indices\n",
        "        reference_index = torch.stack(all_reference_indices)\n",
        "\n",
        "        # Process through blocks\n",
        "        for block in self.blocks:\n",
        "            points = block(points, reference_index)\n",
        "\n",
        "        return points\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 in_channels,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 grid_size=None,\n",
        "                 neighbours=16,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=None,\n",
        "                 drop_path_rate=None,\n",
        "                 enable_checkpoint=False,\n",
        "                 ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.down = GridPool(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embed_channels,\n",
        "            grid_size=grid_size,\n",
        "        )\n",
        "\n",
        "        self.blocks = BlockSequence(\n",
        "            depth=depth,\n",
        "            embed_channels=embed_channels,\n",
        "            groups=groups,\n",
        "            neighbours=neighbours,\n",
        "            qkv_bias=qkv_bias,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias,\n",
        "            attn_drop_rate=attn_drop_rate if attn_drop_rate is not None else 0.,\n",
        "            drop_path_rate=drop_path_rate if drop_path_rate is not None else 0.,\n",
        "            enable_checkpoint=enable_checkpoint\n",
        "        )\n",
        "\n",
        "    def forward(self, points):\n",
        "        points, cluster = self.down(points)\n",
        "        return self.blocks(points), cluster\n",
        "\n",
        "\n",
        "\n",
        "class GVAPatchEmbed(nn.Module):\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 in_channels,\n",
        "                 embed_channels,\n",
        "                 groups,\n",
        "                 neighbours=8,\n",
        "                 qkv_bias=True,\n",
        "                 pe_multiplier=False,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 enable_checkpoint=False\n",
        "                 ):\n",
        "        super(GVAPatchEmbed, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_channels = embed_channels\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(in_channels, embed_channels, bias=False),\n",
        "            PointBatchNorm(embed_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.blocks = BlockSequence(\n",
        "            depth=depth,\n",
        "            embed_channels=embed_channels,\n",
        "            groups=groups,\n",
        "            neighbours=neighbours,\n",
        "            qkv_bias=qkv_bias,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            enable_checkpoint=enable_checkpoint\n",
        "        )\n",
        "\n",
        "    def forward(self, points):\n",
        "        coord, feat, time, batch = points\n",
        "        offset = batch2offset(batch)\n",
        "        feat = self.proj(feat)\n",
        "        return self.blocks([coord, feat, time, batch])"
      ],
      "metadata": {
        "id": "ZCSGKBwTrXGS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2) Point Transformer Class"
      ],
      "metadata": {
        "id": "HkM_sVO_KqYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointTransformerV2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 patch_embed_depth=1,\n",
        "                 patch_embed_channels=16,\n",
        "                 patch_embed_groups= 4 ,\n",
        "                 patch_embed_neighbours=16,\n",
        "                 enc_depths=(2, 2, 6),\n",
        "                 enc_channels=(32, 64, 128),\n",
        "                 enc_groups=(8, 16, 32),\n",
        "                 enc_neighbours=(16, 16, 16),\n",
        "                 grid_sizes=(0.06, 0.12, 0.25),\n",
        "                 attn_qkv_bias=True,\n",
        "                 pe_multiplier=True,\n",
        "                 pe_bias=True,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0,\n",
        "                 enable_checkpoint=False,\n",
        "                 unpool_backend=\"map\"\n",
        "                 ):\n",
        "\n",
        "        super(PointTransformerV2, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.num_stages = len(enc_depths)\n",
        "        assert self.num_stages == len(enc_channels)\n",
        "        assert self.num_stages == len(enc_groups)\n",
        "        assert self.num_stages == len(enc_neighbours)\n",
        "        assert self.num_stages == len(grid_sizes)\n",
        "\n",
        "        self.feature_aggr = FeatureAggregation(in_channels, patch_embed_channels)\n",
        "\n",
        "        self.patch_embed = GVAPatchEmbed(\n",
        "            in_channels=patch_embed_channels,\n",
        "            embed_channels=patch_embed_channels,\n",
        "            groups=patch_embed_groups,\n",
        "            depth=patch_embed_depth,\n",
        "            neighbours=patch_embed_neighbours,\n",
        "            qkv_bias=attn_qkv_bias,\n",
        "            pe_multiplier=pe_multiplier,\n",
        "            pe_bias=pe_bias,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            enable_checkpoint=enable_checkpoint\n",
        "        )\n",
        "\n",
        "        enc_dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(enc_depths))]\n",
        "\n",
        "        enc_channels = [patch_embed_channels] + list(enc_channels)\n",
        "\n",
        "        self.enc_stages = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            enc = Encoder(\n",
        "                depth=enc_depths[i],\n",
        "                in_channels=enc_channels[i],\n",
        "                embed_channels=enc_channels[i + 1],\n",
        "                groups=enc_groups[i],\n",
        "                grid_size=grid_sizes[i],\n",
        "                neighbours=enc_neighbours[i],\n",
        "                qkv_bias=attn_qkv_bias,\n",
        "                pe_multiplier=pe_multiplier,\n",
        "                pe_bias=pe_bias,\n",
        "                attn_drop_rate=attn_drop_rate,\n",
        "                drop_path_rate=enc_dp_rates[sum(enc_depths[:i]):sum(enc_depths[:i + 1])],\n",
        "                enable_checkpoint=enable_checkpoint\n",
        "            )\n",
        "\n",
        "            self.enc_stages.append(enc)\n",
        "\n",
        "        self.pt_aggr = PointAggregation(enc_channels[-1])\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.BatchNorm1d(enc_channels[-1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(enc_channels[-1], enc_channels[-1])\n",
        "        )\n",
        "\n",
        "    def forward(self, data_dict):\n",
        "        coords, feat, time, batch = inpdict_to_point(data_dict)\n",
        "        offset = batch2offset(batch)\n",
        "        #print(coords.shape, feat.shape, time.shape, batch.shape, offset.shape)\n",
        "        points = [coords.float(), feat.float(), time.float(), batch.int()]\n",
        "\n",
        "        # a batch of point cloud is a list of coord, feat and offset\n",
        "        points = self.feature_aggr(points)\n",
        "        points = self.patch_embed(points)\n",
        "        skips = [[points]]\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            points, cluster = self.enc_stages[i](points)\n",
        "            skips[-1].append(cluster)  # record grid cluster of pooling\n",
        "            skips.append([points])  # record points info of current stage\n",
        "\n",
        "        points = skips.pop(-1)[0]\n",
        "        coord, feat, time, batch = points\n",
        "        seg_logits = self.reg_head(self.pt_aggr(feat, batch))\n",
        "\n",
        "        return seg_logits"
      ],
      "metadata": {
        "id": "TliDZiXaLSgT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Define AutoEncoder"
      ],
      "metadata": {
        "id": "jkiVtt_ULCLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1) Define Decoder"
      ],
      "metadata": {
        "id": "wwcWPImOLb3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointCloudDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                latent_dim,\n",
        "                out_feat_dim,\n",
        "                hidden_dims=(256, 128, 64),\n",
        "                num_points_template=1024  # Default template size\n",
        "                ):\n",
        "        super(PointCloudDecoder, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.out_feat_dim = out_feat_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_points_template = num_points_template\n",
        "\n",
        "        # Create a learnable point template\n",
        "        self.point_template = nn.Parameter(torch.randn(1, num_points_template, 3) * 0.1)\n",
        "\n",
        "        # MLP to process latent vector\n",
        "        self.latent_mlp = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dims[0]),\n",
        "            nn.LayerNorm(hidden_dims[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[0]),\n",
        "            nn.LayerNorm(hidden_dims[0]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Deformation MLP - combines point template with latent features\n",
        "        deform_layers = []\n",
        "        deform_input_dim = 3 + hidden_dims[0]  # Concat of point coords + latent\n",
        "        last_dim = deform_input_dim\n",
        "\n",
        "        for h in hidden_dims:\n",
        "            deform_layers.extend([\n",
        "                nn.Linear(last_dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ])\n",
        "            last_dim = h\n",
        "\n",
        "        self.deform_mlp = nn.Sequential(*deform_layers)\n",
        "\n",
        "        # Output heads\n",
        "        self.coord_head = nn.Linear(last_dim, 3)\n",
        "        if out_feat_dim > 0:\n",
        "            self.feat_head = nn.Linear(last_dim, out_feat_dim)\n",
        "\n",
        "    def forward(self, x, num_points_list):\n",
        "        \"\"\"\n",
        "        x: [B, latent_dim] - batch of latent vectors\n",
        "        num_points_list: list or tensor of length B - number of points to generate for each batch item\n",
        "        \"\"\"\n",
        "        coords_list = []\n",
        "        feats_list = []\n",
        "        B = x.size(0)\n",
        "\n",
        "        # Process latent vector\n",
        "        latent_features = self.latent_mlp(x)  # [B, hidden_dims[0]]\n",
        "\n",
        "        for i in range(B):\n",
        "            num_points = num_points_list[i]\n",
        "\n",
        "            # Get latent for this sample and expand\n",
        "            latent_feat = latent_features[i].unsqueeze(0).expand(num_points, -1)  # [num_points, hidden_dims[0]]\n",
        "\n",
        "            # Sample points from template\n",
        "            # If num_points > template size, we need to interpolate\n",
        "            if num_points <= self.num_points_template:\n",
        "                # Take first num_points from template\n",
        "                template_points = self.point_template[0, :num_points, :]\n",
        "            else:\n",
        "                # Interpolate (sample with repetition)\n",
        "                indices = torch.linspace(0, self.num_points_template-1, num_points).long()\n",
        "                template_points = self.point_template[0, indices, :]\n",
        "\n",
        "            # Combine point coords with latent features\n",
        "            point_features = torch.cat([template_points, latent_feat], dim=-1)  # [num_points, 3+hidden_dims[0]]\n",
        "\n",
        "            # Apply deformation network\n",
        "            deformed_features = self.deform_mlp(point_features)  # [num_points, last_hidden_dim]\n",
        "\n",
        "            # Generate output coordinates (as offsets to template)\n",
        "            coord_offsets = self.coord_head(deformed_features)  # [num_points, 3]\n",
        "            coords = template_points + coord_offsets  # Apply offset to template\n",
        "            coords_list.append(coords)\n",
        "\n",
        "            # Generate output features if needed\n",
        "            if self.out_feat_dim > 0:\n",
        "                feats = self.feat_head(deformed_features)  # [num_points, feat_dim]\n",
        "                feats_list.append(feats)\n",
        "\n",
        "        # Concatenate results from batch\n",
        "        coords_out = torch.cat(coords_list, dim=0)  # [sum(num_points), 3]\n",
        "\n",
        "        if self.out_feat_dim > 0:\n",
        "            feats_out = torch.cat(feats_list, dim=0)  # [sum(num_points), feat_dim]\n",
        "            return coords_out, feats_out\n",
        "        else:\n",
        "            return coords_out"
      ],
      "metadata": {
        "id": "R5FYgNLzx_rr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2) Define AutoEncoder Class"
      ],
      "metadata": {
        "id": "ZLdEY4CWLmGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, config_dict):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.config_dict = config_dict\n",
        "    self.point_transformer = PointTransformerV2(in_channels = config_dict['enc_ip_dim'])\n",
        "    self.point_cloud_decoder = PointCloudDecoder(latent_dim = config_dict['latent_dim'],\n",
        "                                                 out_feat_dim = config_dict['enc_ip_dim'])\n",
        "\n",
        "  def forward(self, points):\n",
        "    num_points_list = points['num_points']\n",
        "    feat = self.point_transformer(points)\n",
        "    output = self.point_cloud_decoder(feat, num_points_list)\n",
        "\n",
        "\n",
        "    return feat, output"
      ],
      "metadata": {
        "id": "X3kUg_qSsZYx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Define Configs"
      ],
      "metadata": {
        "id": "JuiVzYpCLxJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    config_dict = {'enc_ip_dim':3,\n",
        "                   'batch_size':16,\n",
        "                   'val_batch_size':1,\n",
        "                   'dmodel':128,\n",
        "                   'n_head':8,\n",
        "                   'num_enc':2,\n",
        "                   'num_dec':3,\n",
        "                   #'d_ff':512,\n",
        "                   'dropout':0.1,\n",
        "                   #'act':nn.SiLU(),\n",
        "                   'dec_ip_dim':1,\n",
        "                   'enc_out_dim':128,\n",
        "                   'op_dim':1,\n",
        "                   'seq_type':'6h',\n",
        "                   #'num_dec_vars':4,\n",
        "                   'latent_dim': 128,\n",
        "                   #'feature_loss_weight': 1,\n",
        "                   'total_samples': 1000000000,\n",
        "                   'num_epochs': 50}\n",
        "                   #'combine_dec_vars':False}\n"
      ],
      "metadata": {
        "id": "YOMa-kP0ti8f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Training"
      ],
      "metadata": {
        "id": "ZS0VVhnYL8fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.1) Example Code"
      ],
      "metadata": {
        "id": "WfAnCtjjMBPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST CODE:\n",
        "example_pc = np.load('example_pc.npy')\n",
        "offset = torch.tensor([0, len(example_pc)]).long()\n",
        "batch = (offset2batch(offset)*0).contiguous()\n",
        "enc_inp = torch.from_numpy(example_pc[:, 1:]).float().contiguous()\n",
        "data_dict = {'enc_inp':enc_inp,'inp_batch_ids': batch, 'num_points': [len(example_pc)]}\n",
        "print(\"enc_inp is contiguous:\", enc_inp.is_contiguous())\n",
        "print(\"inp_batch_ids is contiguous:\", batch.is_contiguous())\n",
        "\n",
        "\n",
        "example_point_transformer = PointTransformerV2(in_channels = config_dict['enc_ip_dim'])\n",
        "example_point_transformer.eval()\n",
        "example_autoencoder = AutoEncoder(config_dict)\n",
        "example_autoencoder.eval()\n",
        "features, output = example_autoencoder(data_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D2FIf0QfyhBP",
        "outputId": "3be94b26-c725-4b96-da68-7c16bf2c9ec1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enc_inp is contiguous: True\n",
            "inp_batch_ids is contiguous: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2) Define Class to Format Data"
      ],
      "metadata": {
        "id": "B3YIuR0ZMZqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PC_dataset_individuals(object):\n",
        "  def __init__(self, root_dir, pc_dir_list):\n",
        "    self.pc_dir_list = pc_dir_list\n",
        "    self.root_dir = root_dir\n",
        "\n",
        "  def get_sample(self, idx, count):\n",
        "\n",
        "    enc_inp = []\n",
        "    inp_batch_ids = []\n",
        "\n",
        "    pc_df = []\n",
        "\n",
        "    while len(pc_df) < 50:\n",
        "      if os.path.exists(os.path.join(self.root_dir, self.pc_dir_list[idx])):\n",
        "          #print(os.path.join(self.root_dir, self.pc_dir_list[idx]))\n",
        "          pc_df = np.load(os.path.join(self.root_dir, self.pc_dir_list[idx]))\n",
        "          pc_df = pc_df[(pc_df[:,3] >= -5) & (pc_df[:,3] <= 5)]\n",
        "      else:\n",
        "          pc_df = []\n",
        "          print('Filepath does not exist:', self.pc_dir_list[idx])\n",
        "\n",
        "      if np.isnan(pc_df).any():\n",
        "          print('Nan in file: ')\n",
        "          pc_df = []\n",
        "\n",
        "      idx += 1 #picks another pc_df file if this one has no points\n",
        "      if idx == len(self.pc_dir_list):\n",
        "          idx = 0\n",
        "\n",
        "    enc_inp.append(pc_df)\n",
        "    inp_batch_ids.extend((count + np.zeros_like(pc_df[:,0])).tolist())\n",
        "\n",
        "\n",
        "    enc_inp = torch.from_numpy(np.concatenate(enc_inp, axis = 0))\n",
        "    inp_batch_ids = torch.from_numpy(np.asarray(inp_batch_ids)).int()\n",
        "\n",
        "    enc_inp[:,3] = (enc_inp[:,3] + 30)/45\n",
        "\n",
        "\n",
        "    data_dict = {'enc_inp': enc_inp,\n",
        "                 'inp_batch_ids': inp_batch_ids,\n",
        "                 'num_points': torch.tensor([len(enc_inp)])}\n",
        "\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "  def data_len(self):\n",
        "    return len(self.pc_dir_list)\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    batch_dict = dict.fromkeys(list(batch[0].keys()))\n",
        "    for key in list(batch_dict.keys()):\n",
        "      bkey = [b[key] for b in batch]\n",
        "      batch_dict[key] = torch.cat(bkey, dim = 0)\n",
        "\n",
        "    return batch_dict\n",
        "\n",
        "  def get_batch_single_pc(self, batch_size): # Two functions internally : get_sample, collate_fn\n",
        "\n",
        "    idxs = np.arange(self.data_len()).astype(int)\n",
        "    np.random.shuffle(idxs)\n",
        "    random_idxs = idxs[:batch_size]\n",
        "\n",
        "    databatch = []\n",
        "    for count,b in enumerate(random_idxs):\n",
        "      databatch.append(self.get_sample(b,count))\n",
        "\n",
        "    return self.collate_fn(databatch)"
      ],
      "metadata": {
        "id": "2sSMzY8iEQkA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3) Define Loss Criterions"
      ],
      "metadata": {
        "id": "DHIFR2qqpoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chamfer_distance(x, y):\n",
        "    \"\"\"\n",
        "    Calculate the Chamfer Distance between two point clouds\n",
        "    x: [N, D] first point cloud (e.g., predicted)\n",
        "    y: [M, D] second point cloud (e.g., target)\n",
        "    Returns the Chamfer Distance between the two point clouds\n",
        "    \"\"\"\n",
        "    # Reshape to [1, N, D] and [1, M, D] if inputs are not batched\n",
        "    if x.dim() == 2:\n",
        "        x = x.unsqueeze(0)\n",
        "    if y.dim() == 2:\n",
        "        y = y.unsqueeze(0)\n",
        "\n",
        "    # Get batch size\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    xx = torch.sum(x**2, dim=2, keepdim=True)       # [B, N, 1]\n",
        "    yy = torch.sum(y**2, dim=2, keepdim=True)       # [B, M, 1]\n",
        "\n",
        "    # Compute all pairwise distances using matrix multiplication\n",
        "    inner = -2 * torch.matmul(x, y.transpose(1, 2))   # [B, N, M]\n",
        "    distances = xx + inner + yy.transpose(1, 2)       # [B, N, M]\n",
        "\n",
        "    # Get min distance for each point in x to any point in y\n",
        "    mins_x, _ = torch.min(distances, dim=2)  # [B, N]\n",
        "\n",
        "    # Get min distance for each point in y to any point in x\n",
        "    mins_y, _ = torch.min(distances, dim=1)  # [B, M]\n",
        "\n",
        "    # Compute the mean over points and add both directions\n",
        "    chamfer_dist = torch.mean(mins_x, dim=1) + torch.mean(mins_y, dim=1)  # [B]\n",
        "\n",
        "    # Return the mean over the batch\n",
        "    return torch.mean(chamfer_dist)\n",
        "\n",
        "class PointCloudLoss(nn.Module):\n",
        "    \"\"\"Combined loss for point cloud reconstruction\"\"\"\n",
        "    def __init__(self, chamfer_weight=1.0, feature_weight=0.1):\n",
        "        super(PointCloudLoss, self).__init__()\n",
        "        self.chamfer_weight = chamfer_weight\n",
        "        self.feature_weight = feature_weight\n",
        "        self.feature_criterion = nn.HuberLoss()\n",
        "\n",
        "    def forward(self, pred_coords, target_coords, pred_feats=None, target_feats=None):\n",
        "        # Chamfer distance for coordinates\n",
        "        chamfer_loss = chamfer_distance(pred_coords, target_coords)\n",
        "\n",
        "        # Feature loss (if features are provided)\n",
        "        feature_loss = 0.0\n",
        "        if pred_feats is not None and target_feats is not None:\n",
        "            feature_loss = self.feature_criterion(pred_feats, target_feats)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.chamfer_weight * chamfer_loss + self.feature_weight * feature_loss\n",
        "\n",
        "        return total_loss, chamfer_loss, feature_loss"
      ],
      "metadata": {
        "id": "8V0Bc7uvE61E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4) Define Train and Val Functions"
      ],
      "metadata": {
        "id": "GHGOt1f-M-pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_point_cloud(original, reconstructed, save_path):\n",
        "    \"\"\"\n",
        "    Visualize original and reconstructed point clouds side by side\n",
        "\n",
        "    Args:\n",
        "        original: original point cloud coordinates [N, 3]\n",
        "        reconstructed: reconstructed point cloud coordinates [N, 3]\n",
        "        save_path: path to save the visualization\n",
        "    \"\"\"\n",
        "    # Convert to numpy if tensors\n",
        "    if isinstance(original, torch.Tensor):\n",
        "        original = original.detach().cpu().numpy()\n",
        "    if isinstance(reconstructed, torch.Tensor):\n",
        "        reconstructed = reconstructed.detach().cpu().numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Original point cloud\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    ax1.scatter(original[:, 0], original[:, 1], original[:, 2], s=1, c=original[:, 2], cmap='viridis')\n",
        "    ax1.set_title('Original Point Cloud')\n",
        "    ax1.set_xlabel('X')\n",
        "    ax1.set_ylabel('Y')\n",
        "    ax1.set_zlabel('Z')\n",
        "\n",
        "    # Reconstructed point cloud\n",
        "    ax2 = fig.add_subplot(122, projection='3d')\n",
        "    ax2.scatter(reconstructed[:, 0], reconstructed[:, 1], reconstructed[:, 2], s=1, c=reconstructed[:, 2], cmap='viridis')\n",
        "    ax2.set_title('Reconstructed Point Cloud')\n",
        "    ax2.set_xlabel('X')\n",
        "    ax2.set_ylabel('Y')\n",
        "    ax2.set_zlabel('Z')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "def inspect_outputs(target, reconstructed, num_samples=5):\n",
        "    \"\"\"Inspect target and reconstructed point clouds\"\"\"\n",
        "    print(f\"\\nOutput Diagnostics (showing {num_samples} samples):\")\n",
        "\n",
        "    # Check if all reconstructed points are the same\n",
        "    recon_all_same = torch.allclose(\n",
        "        reconstructed[0].unsqueeze(0).expand_as(reconstructed),\n",
        "        reconstructed,\n",
        "        rtol=1e-4, atol=1e-4\n",
        "    )\n",
        "\n",
        "    if recon_all_same:\n",
        "        print(f\"CRITICAL ISSUE: All reconstructed points are identical!\")\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Target points - Mean: {target.mean(dim=0)}, Std: {target.std(dim=0)}\")\n",
        "    print(f\"Reconstructed points - Mean: {reconstructed.mean(dim=0)}, Std: {reconstructed.std(dim=0)}\")\n",
        "\n",
        "    # Show some examples\n",
        "    print(\"\\nSample points (target vs reconstructed):\")\n",
        "    for i in range(min(num_samples, len(target))):\n",
        "        print(f\"Point {i}:\")\n",
        "        print(f\"  Target:        {target[i]}\")\n",
        "        print(f\"  Reconstructed: {reconstructed[i]}\")\n",
        "\n",
        "    # Check variance across dimensions\n",
        "    print(f\"\\nVariance per dimension:\")\n",
        "    print(f\"  Target:        {torch.var(target, dim=0)}\")\n",
        "    print(f\"  Reconstructed: {torch.var(reconstructed, dim=0)}\")\n",
        "\n",
        "def train(model, train_dataset, batch_size, optimizer, scheduler, criterion, epoch, device, config_dict, vis_dir=None):\n",
        "    model.train()\n",
        "    epoch_start = time.time()\n",
        "    train_loss = 0\n",
        "    train_coord_loss = 0\n",
        "    train_feat_loss = 0\n",
        "    num_steps_per_epoch = (train_dataset.data_len() // batch_size) + 1\n",
        "    count_good_steps = 0\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        # === Get and move batch ===\n",
        "        data_dict = train_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # === Forward pass ===\n",
        "        features, output = model(data_dict)\n",
        "        recon_coords = output[0]\n",
        "        recon_feats = output[1]\n",
        "\n",
        "        coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "        target_coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # === Visualize first batch occasionally ===\n",
        "        if vis_dir and step == 0 and epoch % 5 == 0:\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "            # Only visualize first point cloud in batch\n",
        "            first_pc_len = data_dict['num_points'][0].item()\n",
        "            visualize_point_cloud(\n",
        "                target_coords[:first_pc_len],\n",
        "                recon_coords[:first_pc_len],\n",
        "                os.path.join(vis_dir, f'epoch_{epoch}_train.png')\n",
        "            )\n",
        "\n",
        "        # === Sanity checks ===\n",
        "        if torch.isnan(recon_coords).any() or torch.isnan(recon_feats).any():\n",
        "            print(\"NaN in model output. Skipping step.\")\n",
        "            continue\n",
        "\n",
        "        # === Loss computation ===\n",
        "        total_loss, coord_loss, feat_loss = criterion(\n",
        "            recon_coords, target_coords, recon_feats, target_feats\n",
        "        )\n",
        "\n",
        "        # === Backward + optimize ===\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Reduced from 1.0\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += total_loss.item()\n",
        "        train_coord_loss += coord_loss.item()\n",
        "        train_feat_loss += feat_loss.item()\n",
        "        count_good_steps += 1\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Epoch {epoch}, Step {step}/{num_steps_per_epoch}, \"\n",
        "              f\"Total Loss: {total_loss.item():.4f}, \"\n",
        "              f\"Coord Loss: {coord_loss.item():.4f}, \"\n",
        "              f\"Feat Loss: {feat_loss.item():.4f}, \"\n",
        "              f\"Time: {step_end - step_start:.2f}s\")\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f}s\")\n",
        "\n",
        "    # Return average losses\n",
        "    avg_loss = train_loss / max(count_good_steps, 1)\n",
        "    avg_coord_loss = train_coord_loss / max(count_good_steps, 1)\n",
        "    avg_feat_loss = train_feat_loss / max(count_good_steps, 1)\n",
        "\n",
        "    return avg_loss, avg_coord_loss, avg_feat_loss\n",
        "\n",
        "def val(model, val_dataset, batch_size, criterion, epoch, device, config_dict, vis_dir=None):\n",
        "    model.eval()\n",
        "    epoch_start = time.time()\n",
        "    val_loss = 0\n",
        "    val_coord_loss = 0\n",
        "    val_feat_loss = 0\n",
        "    num_steps_per_epoch = (val_dataset.data_len() // batch_size) + 1\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        data_dict = val_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        # === Forward pass ===\n",
        "        features, output = model(data_dict)\n",
        "        recon_coords = output[0]\n",
        "        recon_feats = output[1]\n",
        "\n",
        "        coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "        target_coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # === Visualize first batch occasionally ===\n",
        "        if vis_dir and step == 0 and epoch % 5 == 0:\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "            # Only visualize first point cloud in batch\n",
        "            first_pc_len = data_dict['num_points'][0].item()\n",
        "            visualize_point_cloud(\n",
        "                target_coords[:first_pc_len],\n",
        "                recon_coords[:first_pc_len],\n",
        "                os.path.join(vis_dir, f'epoch_{epoch}_val.png')\n",
        "            )\n",
        "\n",
        "            # Debug output inspection\n",
        "            inspect_outputs(target_coords[:first_pc_len], recon_coords[:first_pc_len])\n",
        "\n",
        "        # === Loss computation ===\n",
        "        total_loss, coord_loss, feat_loss = criterion(\n",
        "            recon_coords, target_coords, recon_feats, target_feats\n",
        "        )\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Epoch: {epoch}, Step: {step}/{num_steps_per_epoch}, \"\n",
        "              f\"Total Loss: {total_loss.item():.4f}, \"\n",
        "              f\"Coord Loss: {coord_loss.item():.4f}, \"\n",
        "              f\"Feat Loss: {feat_loss.item():.4f}, \"\n",
        "              f\"Time: {step_end-step_start:.2f}s\")\n",
        "\n",
        "        val_loss += total_loss.item()\n",
        "        val_coord_loss += coord_loss.item()\n",
        "        val_feat_loss += feat_loss.item()\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    print(f'Time to complete validation: {time.time()-epoch_start:.2f}s')\n",
        "\n",
        "    # Return average losses\n",
        "    avg_loss = val_loss / num_steps_per_epoch\n",
        "    avg_coord_loss = val_coord_loss / num_steps_per_epoch\n",
        "    avg_feat_loss = val_feat_loss / num_steps_per_epoch\n",
        "\n",
        "    return avg_loss, avg_coord_loss, avg_feat_loss"
      ],
      "metadata": {
        "id": "QJAwVOdyFfzE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.5) Training Script"
      ],
      "metadata": {
        "id": "Tx8uxOT7NLdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd()\n",
        "lidar_dir = os.path.join(root, 'LiDAR')\n",
        "\n",
        "pcdata_dir = os.path.join(lidar_dir, 'pcloud_norm')\n",
        "pc_files = natsort.natsorted(os.listdir(pcdata_dir))\n",
        "pc_paths = [os.path.join(pcdata_dir, pcf) for pcf in pc_files]\n",
        "\n",
        "if config_dict['total_samples'] < len(pc_paths):\n",
        "  pc_paths = pc_paths[:config_dict['total_samples']]\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(device)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "save_dir = os.path.join(lidar_dir, os.path.join('saved_models'))\n",
        "if not os.path.exists(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "random.shuffle(pc_paths)\n",
        "pc_paths = pc_paths[:config_dict['total_samples']]\n",
        "\n",
        "train_val_pcs = pc_paths[:(int(len(pc_paths)*0.7))]\n",
        "test_pcs = pc_paths[(int(len(pc_paths)*0.7)):]\n",
        "\n",
        "\n",
        "train_idx = np.arange(len(train_val_pcs)).astype(int)[:int(0.9*len(train_val_pcs))]\n",
        "val_idx = np.arange(len(train_val_pcs)).astype(int)[int(0.9*len(train_val_pcs)):]\n",
        "\n",
        "train_pcs = np.asarray(train_val_pcs)[train_idx].tolist()\n",
        "val_pcs = np.asarray(train_val_pcs)[val_idx].tolist()\n",
        "\n",
        "train_dataset = PC_dataset_individuals(lidar_dir, train_pcs)\n",
        "val_dataset = PC_dataset_individuals(lidar_dir, val_pcs)\n",
        "test_dataset = PC_dataset_individuals(lidar_dir, test_pcs)\n",
        "\n",
        "vis_dir = os.path.join(lidar_dir, 'visualizations')\n",
        "os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "# Initialize model\n",
        "model = AutoEncoder(config_dict).to(device)\n",
        "\n",
        "# Use the new point cloud loss\n",
        "criterion = PointCloudLoss(chamfer_weight=1.0, feature_weight=0.1)\n",
        "\n",
        "# Use a better optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0001,  # Lower learning rate\n",
        "    weight_decay=0.01  # Add regularization\n",
        ")\n",
        "\n",
        "# Use a gentler learning rate schedule\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config_dict['num_epochs']):\n",
        "    # Train\n",
        "    train_loss, train_coord_loss, train_feat_loss = train(\n",
        "        model, train_dataset, config_dict['batch_size'],\n",
        "        optimizer, scheduler, criterion, epoch, device, config_dict,\n",
        "        vis_dir=os.path.join(vis_dir, 'train')\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "        val_loss, val_coord_loss, val_feat_loss = val(\n",
        "            model, val_dataset, config_dict['val_batch_size'],\n",
        "            criterion, epoch, device, config_dict,\n",
        "            vis_dir=os.path.join(vis_dir, 'val')\n",
        "        )\n",
        "\n",
        "    # Update learning rate based on validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch} summary:')\n",
        "    print(f'  Train - Total: {train_loss:.4f}, Coord: {train_coord_loss:.4f}, Feat: {train_feat_loss:.4f}')\n",
        "    print(f'  Val   - Total: {val_loss:.4f}, Coord: {val_coord_loss:.4f}, Feat: {val_feat_loss:.4f}')\n",
        "\n",
        "    # Save model checkpoint\n",
        "    if epoch % 5 == 0 or epoch == config_dict['num_epochs'] - 1:\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f'model_epoch_{epoch}.pth'))\n",
        "\n",
        "    # Save best model\n",
        "    if epoch == 0 or val_loss < min_val_loss:\n",
        "        min_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
        "        print(f'Saved new best model with val_loss: {val_loss:.4f}')\n",
        "\n",
        "# Test the best model\n",
        "model.load_state_dict(torch.load(os.path.join(save_dir, 'best_model.pth')))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_results = test(\n",
        "        model, test_dataset, batch_size=1, criterion=criterion,\n",
        "        device=device, config_dict=config_dict,\n",
        "        vis_dir=os.path.join(vis_dir, 'test')\n",
        "    )\n",
        "\n",
        "print(f'Final test loss: {test_results[0]:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdcV9jKWNTKP",
        "outputId": "3a4737f6-880c-4dfa-8489-cfafd4b1c1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Step 0/40, Total Loss: 0.0251, Coord Loss: 0.0147, Feat Loss: 0.1040, Time: 18.46s\n",
            "Epoch 0, Step 1/40, Total Loss: 0.0166, Coord Loss: 0.0076, Feat Loss: 0.0893, Time: 4.35s\n",
            "Epoch 0, Step 2/40, Total Loss: 0.0145, Coord Loss: 0.0082, Feat Loss: 0.0636, Time: 4.47s\n",
            "Epoch 0, Step 3/40, Total Loss: 0.0094, Coord Loss: 0.0055, Feat Loss: 0.0387, Time: 3.09s\n",
            "Epoch 0, Step 4/40, Total Loss: 0.0154, Coord Loss: 0.0119, Feat Loss: 0.0353, Time: 3.01s\n",
            "Epoch 0, Step 5/40, Total Loss: 0.0142, Coord Loss: 0.0104, Feat Loss: 0.0380, Time: 2.03s\n",
            "Epoch 0, Step 6/40, Total Loss: 0.0060, Coord Loss: 0.0038, Feat Loss: 0.0217, Time: 3.74s\n",
            "Epoch 0, Step 7/40, Total Loss: 0.0106, Coord Loss: 0.0079, Feat Loss: 0.0272, Time: 2.87s\n",
            "Epoch 0, Step 8/40, Total Loss: 0.0089, Coord Loss: 0.0074, Feat Loss: 0.0153, Time: 3.47s\n",
            "Epoch 0, Step 9/40, Total Loss: 0.0090, Coord Loss: 0.0079, Feat Loss: 0.0118, Time: 2.20s\n",
            "Epoch 0, Step 10/40, Total Loss: 0.0080, Coord Loss: 0.0069, Feat Loss: 0.0112, Time: 3.86s\n",
            "Epoch 0, Step 11/40, Total Loss: 0.0075, Coord Loss: 0.0064, Feat Loss: 0.0116, Time: 4.24s\n",
            "Epoch 0, Step 12/40, Total Loss: 0.0073, Coord Loss: 0.0060, Feat Loss: 0.0126, Time: 2.25s\n",
            "Epoch 0, Step 13/40, Total Loss: 0.0093, Coord Loss: 0.0076, Feat Loss: 0.0171, Time: 2.00s\n",
            "Epoch 0, Step 14/40, Total Loss: 0.0063, Coord Loss: 0.0054, Feat Loss: 0.0093, Time: 3.59s\n",
            "Epoch 0, Step 15/40, Total Loss: 0.0059, Coord Loss: 0.0049, Feat Loss: 0.0099, Time: 3.11s\n",
            "Epoch 0, Step 16/40, Total Loss: 0.0051, Coord Loss: 0.0044, Feat Loss: 0.0071, Time: 2.43s\n",
            "Epoch 0, Step 17/40, Total Loss: 0.0060, Coord Loss: 0.0052, Feat Loss: 0.0080, Time: 2.53s\n",
            "Epoch 0, Step 18/40, Total Loss: 0.0138, Coord Loss: 0.0128, Feat Loss: 0.0104, Time: 2.74s\n",
            "Epoch 0, Step 19/40, Total Loss: 0.0098, Coord Loss: 0.0091, Feat Loss: 0.0074, Time: 2.06s\n",
            "Epoch 0, Step 20/40, Total Loss: 0.0065, Coord Loss: 0.0059, Feat Loss: 0.0060, Time: 4.35s\n",
            "Epoch 0, Step 21/40, Total Loss: 0.0062, Coord Loss: 0.0057, Feat Loss: 0.0054, Time: 4.53s\n",
            "Epoch 0, Step 22/40, Total Loss: 0.0054, Coord Loss: 0.0047, Feat Loss: 0.0066, Time: 3.07s\n",
            "Epoch 0, Step 23/40, Total Loss: 0.0058, Coord Loss: 0.0050, Feat Loss: 0.0081, Time: 2.97s\n",
            "Epoch 0, Step 24/40, Total Loss: 0.0059, Coord Loss: 0.0052, Feat Loss: 0.0068, Time: 2.51s\n",
            "Epoch 0, Step 25/40, Total Loss: 0.0073, Coord Loss: 0.0066, Feat Loss: 0.0074, Time: 1.73s\n",
            "Epoch 0, Step 26/40, Total Loss: 0.0069, Coord Loss: 0.0065, Feat Loss: 0.0047, Time: 3.91s\n",
            "Epoch 0, Step 27/40, Total Loss: 0.0098, Coord Loss: 0.0089, Feat Loss: 0.0090, Time: 2.06s\n",
            "Epoch 0, Step 28/40, Total Loss: 0.0095, Coord Loss: 0.0089, Feat Loss: 0.0064, Time: 2.24s\n",
            "Epoch 0, Step 29/40, Total Loss: 0.0062, Coord Loss: 0.0057, Feat Loss: 0.0051, Time: 2.45s\n",
            "Epoch 0, Step 30/40, Total Loss: 0.0107, Coord Loss: 0.0102, Feat Loss: 0.0052, Time: 2.49s\n",
            "Epoch 0, Step 31/40, Total Loss: 0.0069, Coord Loss: 0.0063, Feat Loss: 0.0062, Time: 2.77s\n",
            "Epoch 0, Step 32/40, Total Loss: 0.0077, Coord Loss: 0.0069, Feat Loss: 0.0077, Time: 1.93s\n",
            "Epoch 0, Step 33/40, Total Loss: 0.0045, Coord Loss: 0.0041, Feat Loss: 0.0049, Time: 2.35s\n",
            "Epoch 0, Step 34/40, Total Loss: 0.0068, Coord Loss: 0.0061, Feat Loss: 0.0063, Time: 2.09s\n",
            "Epoch 0, Step 35/40, Total Loss: 0.0051, Coord Loss: 0.0048, Feat Loss: 0.0035, Time: 2.64s\n",
            "Epoch 0, Step 36/40, Total Loss: 0.0051, Coord Loss: 0.0047, Feat Loss: 0.0040, Time: 2.89s\n",
            "Epoch 0, Step 37/40, Total Loss: 0.0051, Coord Loss: 0.0045, Feat Loss: 0.0057, Time: 2.28s\n",
            "Epoch 0, Step 38/40, Total Loss: 0.0052, Coord Loss: 0.0046, Feat Loss: 0.0055, Time: 2.10s\n",
            "Epoch 0, Step 39/40, Total Loss: 0.0036, Coord Loss: 0.0032, Feat Loss: 0.0044, Time: 2.40s\n",
            "Epoch 0 completed in 139.84s\n",
            "\n",
            "Output Diagnostics (showing 5 samples):\n",
            "Target points - Mean: tensor([-0.3014, -0.0043,  0.1882], device='cuda:0'), Std: tensor([0.1209, 0.1858, 0.0084], device='cuda:0')\n",
            "Reconstructed points - Mean: tensor([0.3371, 0.0116, 0.2172], device='cuda:0'), Std: tensor([0.1024, 0.1044, 0.1009], device='cuda:0')\n",
            "\n",
            "Sample points (target vs reconstructed):\n",
            "Point 0:\n",
            "  Target:        tensor([-0.6688, -0.0702,  0.1802], device='cuda:0')\n",
            "  Reconstructed: tensor([ 0.2291, -0.0166,  0.1154], device='cuda:0')\n",
            "Point 1:\n",
            "  Target:        tensor([-0.6036, -0.3484,  0.1867], device='cuda:0')\n",
            "  Reconstructed: tensor([0.3892, 0.0823, 0.1931], device='cuda:0')\n",
            "Point 2:\n",
            "  Target:        tensor([-0.3422,  0.3802,  0.1862], device='cuda:0')\n",
            "  Reconstructed: tensor([0.3304, 0.0234, 0.1939], device='cuda:0')\n",
            "Point 3:\n",
            "  Target:        tensor([-0.3624,  0.3264,  0.1775], device='cuda:0')\n",
            "  Reconstructed: tensor([ 0.4225, -0.0192,  0.2958], device='cuda:0')\n",
            "Point 4:\n",
            "  Target:        tensor([-0.3945,  0.2867,  0.1775], device='cuda:0')\n",
            "  Reconstructed: tensor([ 0.2498, -0.1688,  0.2084], device='cuda:0')\n",
            "\n",
            "Variance per dimension:\n",
            "  Target:        tensor([1.4613e-02, 3.4530e-02, 7.0850e-05], device='cuda:0')\n",
            "  Reconstructed: tensor([0.0105, 0.0109, 0.0102], device='cuda:0')\n",
            "Epoch: 0, Step: 0/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 1.19s\n",
            "Epoch: 0, Step: 1/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.28s\n",
            "Epoch: 0, Step: 2/71, Total Loss: 0.0580, Coord Loss: 0.0568, Feat Loss: 0.0123, Time: 0.21s\n",
            "Epoch: 0, Step: 3/71, Total Loss: 0.0580, Coord Loss: 0.0568, Feat Loss: 0.0123, Time: 0.18s\n",
            "Epoch: 0, Step: 4/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.14s\n",
            "Epoch: 0, Step: 5/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.16s\n",
            "Epoch: 0, Step: 6/71, Total Loss: 0.0797, Coord Loss: 0.0797, Feat Loss: 0.0006, Time: 0.25s\n",
            "Epoch: 0, Step: 7/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.33s\n",
            "Epoch: 0, Step: 8/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.10s\n",
            "Epoch: 0, Step: 9/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.16s\n",
            "Epoch: 0, Step: 10/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.15s\n",
            "Epoch: 0, Step: 11/71, Total Loss: 0.0415, Coord Loss: 0.0413, Feat Loss: 0.0028, Time: 0.10s\n",
            "Epoch: 0, Step: 12/71, Total Loss: 0.0415, Coord Loss: 0.0413, Feat Loss: 0.0028, Time: 0.09s\n",
            "Epoch: 0, Step: 13/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.22s\n",
            "Epoch: 0, Step: 14/71, Total Loss: 0.0418, Coord Loss: 0.0417, Feat Loss: 0.0015, Time: 0.21s\n",
            "Epoch: 0, Step: 15/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.22s\n",
            "Epoch: 0, Step: 16/71, Total Loss: 0.0427, Coord Loss: 0.0427, Feat Loss: 0.0006, Time: 0.26s\n",
            "Epoch: 0, Step: 17/71, Total Loss: 0.0229, Coord Loss: 0.0225, Feat Loss: 0.0040, Time: 0.24s\n",
            "Epoch: 0, Step: 18/71, Total Loss: 0.0797, Coord Loss: 0.0797, Feat Loss: 0.0006, Time: 0.18s\n",
            "Epoch: 0, Step: 19/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.26s\n",
            "Epoch: 0, Step: 20/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.18s\n",
            "Epoch: 0, Step: 21/71, Total Loss: 0.0805, Coord Loss: 0.0801, Feat Loss: 0.0048, Time: 0.11s\n",
            "Epoch: 0, Step: 22/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.22s\n",
            "Epoch: 0, Step: 23/71, Total Loss: 0.0805, Coord Loss: 0.0801, Feat Loss: 0.0048, Time: 0.12s\n",
            "Epoch: 0, Step: 24/71, Total Loss: 0.0415, Coord Loss: 0.0413, Feat Loss: 0.0028, Time: 0.11s\n",
            "Epoch: 0, Step: 25/71, Total Loss: 0.0797, Coord Loss: 0.0797, Feat Loss: 0.0006, Time: 0.21s\n",
            "Epoch: 0, Step: 26/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.11s\n",
            "Epoch: 0, Step: 27/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.13s\n",
            "Epoch: 0, Step: 28/71, Total Loss: 0.0415, Coord Loss: 0.0413, Feat Loss: 0.0028, Time: 0.10s\n",
            "Epoch: 0, Step: 29/71, Total Loss: 0.0427, Coord Loss: 0.0427, Feat Loss: 0.0006, Time: 0.24s\n",
            "Epoch: 0, Step: 30/71, Total Loss: 0.3253, Coord Loss: 0.3247, Feat Loss: 0.0060, Time: 0.79s\n",
            "Epoch: 0, Step: 31/71, Total Loss: 0.3253, Coord Loss: 0.3247, Feat Loss: 0.0060, Time: 0.15s\n",
            "Epoch: 0, Step: 32/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.28s\n",
            "Epoch: 0, Step: 33/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.15s\n",
            "Epoch: 0, Step: 34/71, Total Loss: 0.0805, Coord Loss: 0.0801, Feat Loss: 0.0048, Time: 0.17s\n",
            "Epoch: 0, Step: 35/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.21s\n",
            "Epoch: 0, Step: 36/71, Total Loss: 0.0415, Coord Loss: 0.0413, Feat Loss: 0.0028, Time: 0.13s\n",
            "Epoch: 0, Step: 37/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.88s\n",
            "Epoch: 0, Step: 38/71, Total Loss: 0.0580, Coord Loss: 0.0568, Feat Loss: 0.0123, Time: 0.15s\n",
            "Epoch: 0, Step: 39/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.22s\n",
            "Epoch: 0, Step: 40/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.13s\n",
            "Epoch: 0, Step: 41/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.14s\n",
            "Epoch: 0, Step: 42/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.17s\n",
            "Epoch: 0, Step: 43/71, Total Loss: 0.0477, Coord Loss: 0.0476, Feat Loss: 0.0011, Time: 0.19s\n",
            "Epoch: 0, Step: 44/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.19s\n",
            "Epoch: 0, Step: 45/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.23s\n",
            "Epoch: 0, Step: 46/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.22s\n",
            "Epoch: 0, Step: 47/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.11s\n",
            "Epoch: 0, Step: 48/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.21s\n",
            "Epoch: 0, Step: 49/71, Total Loss: 0.0229, Coord Loss: 0.0225, Feat Loss: 0.0040, Time: 0.21s\n",
            "Epoch: 0, Step: 50/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.18s\n",
            "Epoch: 0, Step: 51/71, Total Loss: 0.0336, Coord Loss: 0.0334, Feat Loss: 0.0022, Time: 0.13s\n",
            "Epoch: 0, Step: 52/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.21s\n",
            "Epoch: 0, Step: 53/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.22s\n",
            "Epoch: 0, Step: 54/71, Total Loss: 0.0797, Coord Loss: 0.0797, Feat Loss: 0.0006, Time: 0.18s\n",
            "Epoch: 0, Step: 55/71, Total Loss: 0.4489, Coord Loss: 0.4477, Feat Loss: 0.0114, Time: 0.14s\n",
            "Epoch: 0, Step: 56/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.15s\n",
            "Epoch: 0, Step: 57/71, Total Loss: 0.0336, Coord Loss: 0.0334, Feat Loss: 0.0022, Time: 0.13s\n",
            "Epoch: 0, Step: 58/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.16s\n",
            "Epoch: 0, Step: 59/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.14s\n",
            "Epoch: 0, Step: 60/71, Total Loss: 0.0797, Coord Loss: 0.0797, Feat Loss: 0.0006, Time: 0.20s\n",
            "Epoch: 0, Step: 61/71, Total Loss: 0.0455, Coord Loss: 0.0454, Feat Loss: 0.0013, Time: 0.15s\n",
            "Epoch: 0, Step: 62/71, Total Loss: 0.3253, Coord Loss: 0.3247, Feat Loss: 0.0060, Time: 0.22s\n",
            "Epoch: 0, Step: 63/71, Total Loss: 0.0764, Coord Loss: 0.0755, Feat Loss: 0.0088, Time: 0.19s\n",
            "Epoch: 0, Step: 64/71, Total Loss: 0.0597, Coord Loss: 0.0591, Feat Loss: 0.0054, Time: 0.27s\n",
            "Epoch: 0, Step: 65/71, Total Loss: 0.0805, Coord Loss: 0.0801, Feat Loss: 0.0048, Time: 0.17s\n",
            "Epoch: 0, Step: 66/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.26s\n",
            "Epoch: 0, Step: 67/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.30s\n",
            "Epoch: 0, Step: 68/71, Total Loss: 0.1655, Coord Loss: 0.1643, Feat Loss: 0.0118, Time: 0.21s\n",
            "Epoch: 0, Step: 69/71, Total Loss: 0.0283, Coord Loss: 0.0277, Feat Loss: 0.0060, Time: 0.25s\n",
            "Epoch: 0, Step: 70/71, Total Loss: 0.0797, Coord Loss: 0.0797, Feat Loss: 0.0006, Time: 0.18s\n",
            "Time to complete validation: 32.25s\n",
            "Epoch 0 summary:\n",
            "  Train - Total: 0.0085, Coord: 0.0068, Feat: 0.0167\n",
            "  Val   - Total: 0.1076, Coord: 0.1071, Feat: 0.0049\n",
            "Saved new best model with val_loss: 0.1076\n",
            "Epoch 1, Step 0/40, Total Loss: 0.0044, Coord Loss: 0.0039, Feat Loss: 0.0057, Time: 2.79s\n",
            "Epoch 1, Step 1/40, Total Loss: 0.0061, Coord Loss: 0.0056, Feat Loss: 0.0049, Time: 2.39s\n",
            "Epoch 1, Step 2/40, Total Loss: 0.0045, Coord Loss: 0.0039, Feat Loss: 0.0057, Time: 2.76s\n",
            "Epoch 1, Step 3/40, Total Loss: 0.0050, Coord Loss: 0.0045, Feat Loss: 0.0052, Time: 3.06s\n",
            "Epoch 1, Step 4/40, Total Loss: 0.0057, Coord Loss: 0.0052, Feat Loss: 0.0044, Time: 2.19s\n",
            "Epoch 1, Step 5/40, Total Loss: 0.0045, Coord Loss: 0.0042, Feat Loss: 0.0031, Time: 2.45s\n",
            "Epoch 1, Step 6/40, Total Loss: 0.0071, Coord Loss: 0.0067, Feat Loss: 0.0034, Time: 2.23s\n",
            "Epoch 1, Step 7/40, Total Loss: 0.0063, Coord Loss: 0.0060, Feat Loss: 0.0030, Time: 2.70s\n",
            "Epoch 1, Step 8/40, Total Loss: 0.0079, Coord Loss: 0.0075, Feat Loss: 0.0041, Time: 2.32s\n",
            "Epoch 1, Step 9/40, Total Loss: 0.0044, Coord Loss: 0.0039, Feat Loss: 0.0042, Time: 2.47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6) Testing"
      ],
      "metadata": {
        "id": "uqlj3ZJCN3tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.1) Define Testing Function"
      ],
      "metadata": {
        "id": "kzdobW-TN6_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_dataset, batch_size, criterion, device, config_dict, vis_dir=None):\n",
        "    model.eval()\n",
        "    epoch_start = time.time()\n",
        "    test_loss = 0\n",
        "    test_coord_loss = 0\n",
        "    test_feat_loss = 0\n",
        "    num_steps_per_epoch = (test_dataset.data_len() // batch_size) + 1 * (test_dataset.data_len() % batch_size != 0)\n",
        "\n",
        "    recon_coords_list = []\n",
        "    recon_feats_list = []\n",
        "    target_coords_list = []\n",
        "    target_feats_list = []\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        data_dict = test_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        # === Forward pass ===\n",
        "        _, output = model(data_dict)\n",
        "        recon_coords = output[0]\n",
        "        recon_feats = output[1]\n",
        "\n",
        "        coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "        target_coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # === Store results for visualization ===\n",
        "        recon_coords_list.append(recon_coords.detach().cpu())\n",
        "        recon_feats_list.append(recon_feats.detach().cpu())\n",
        "        target_coords_list.append(target_coords.detach().cpu())\n",
        "        target_feats_list.append(target_feats.detach().cpu())\n",
        "\n",
        "        # === Loss computation ===\n",
        "        total_loss, coord_loss, feat_loss = criterion(\n",
        "            recon_coords, target_coords, recon_feats, target_feats\n",
        "        )\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Step: {step}/{num_steps_per_epoch}, \"\n",
        "              f\"Total Loss: {total_loss.item():.4f}, \"\n",
        "              f\"Coord Loss: {coord_loss.item():.4f}, \"\n",
        "              f\"Feat Loss: {feat_loss.item():.4f}, \"\n",
        "              f\"Time: {step_end-step_start:.2f}s\")\n",
        "\n",
        "        test_loss += total_loss.item()\n",
        "        test_coord_loss += coord_loss.item()\n",
        "        test_feat_loss += feat_loss.item()\n",
        "\n",
        "        # === Visualize test results ===\n",
        "        if vis_dir and step < 5:  # Visualize first 5 test samples\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "            # Only visualize first point cloud in batch\n",
        "            first_pc_len = data_dict['num_points'][0].item()\n",
        "            visualize_point_cloud(\n",
        "                target_coords[:first_pc_len],\n",
        "                recon_coords[:first_pc_len],\n",
        "                os.path.join(vis_dir, f'test_sample_{step}.png')\n",
        "            )\n",
        "\n",
        "            # Debug output inspection\n",
        "            inspect_outputs(target_coords[:first_pc_len], recon_coords[:first_pc_len])\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    print(f'Time to complete testing: {time.time()-epoch_start:.2f}s')\n",
        "\n",
        "    # Return average losses and result lists\n",
        "    avg_loss = test_loss / num_steps_per_epoch\n",
        "    avg_coord_loss = test_coord_loss / num_steps_per_epoch\n",
        "    avg_feat_loss = test_feat_loss / num_steps_per_epoch\n",
        "\n",
        "    return (avg_loss, avg_coord_loss, avg_feat_loss,\n",
        "            recon_coords_list, recon_feats_list,\n",
        "            target_coords_list, target_feats_list)"
      ],
      "metadata": {
        "id": "Y9_to0_5Fm2z"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7) Garbage"
      ],
      "metadata": {
        "id": "98IhJWaWOKxb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jz2Sfj7AOAt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aU1OCv8wOAq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPAI49QBOAoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12lLCFkiOAma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGkrFuvHOAju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEEGyGkkOAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BfkP1Pe2OAb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6HWGg9-OAZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAEcq0qUOAP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataset, batch_size, optimizer, scheduler, criterion, epoch, device, config_dict):\n",
        "    model.train()\n",
        "    epoch_start = time.time()\n",
        "    train_loss = 0\n",
        "    num_steps_per_epoch = (train_dataset.data_len() // batch_size) + 1\n",
        "    count_good_steps = 0\n",
        "\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        step_start = time.time()\n",
        "\n",
        "        # === Get and move batch ===\n",
        "        data_dict = train_dataset.get_batch_single_pc(batch_size)\n",
        "        data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # === Forward pass ===\n",
        "        features, output = model(data_dict)\n",
        "        recon_coords = output[0]\n",
        "        recon_feats = output[1]\n",
        "        coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "        target_coords = coords.float()\n",
        "        target_feats = feats.float()\n",
        "\n",
        "        # === Sanity checks ===\n",
        "        if torch.isnan(recon_coords).any() or torch.isnan(recon_feats).any():\n",
        "            print(\"NaN in model output. Skipping step.\")\n",
        "            continue\n",
        "\n",
        "        # === Loss computation ===\n",
        "        coord_loss = criterion(recon_coords, target_coords)\n",
        "        feat_loss = criterion(recon_feats, target_feats)\n",
        "        alpha = config_dict.get(\"feature_loss_weight\", 1.0)\n",
        "        loss = coord_loss + alpha * feat_loss\n",
        "\n",
        "        # === Backward + optimize ===\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step(epoch + step / num_steps_per_epoch)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        count_good_steps += 1\n",
        "\n",
        "        step_end = time.time()\n",
        "        print(f\"Epoch {epoch}, Step {step}/{num_steps_per_epoch}, Loss: {loss.item():.4f}, Time: {step_end - step_start:.2f}s\")\n",
        "\n",
        "        gc.collect()\n",
        "        del data_dict\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f}s\")\n",
        "\n",
        "    return train_loss / max(count_good_steps, 1)\n",
        "\n",
        "\n",
        "def val(model, val_dataset, batch_size, criterion, epoch, device, config_dict): # Added device argument\n",
        "\n",
        "  model.eval()\n",
        "  epoch_start = time.time()\n",
        "  val_loss = 0\n",
        "  num_steps_per_epoch = (val_dataset.data_len()//batch_size) + 1\n",
        "\n",
        "  for step in range(num_steps_per_epoch):\n",
        "    step_start = time.time()\n",
        "    data_dict = val_dataset.get_batch_single_pc(batch_size)\n",
        "    data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "    # === Forward pass ===\n",
        "    features, output = model(data_dict)\n",
        "    recon_coords = output[0]\n",
        "    recon_feats = output[1]\n",
        "    coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "    target_coords = coords.float()\n",
        "    target_feats = feats.float()\n",
        "\n",
        "    #--Loss--\n",
        "    coord_loss = criterion(recon_coords, target_coords)\n",
        "    feat_loss = criterion(recon_feats, target_feats)\n",
        "    alpha = config_dict.get(\"feature_loss_weight\", 1.0)\n",
        "    loss = coord_loss + alpha * feat_loss\n",
        "    step_end = time.time()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Step: {step}/{num_steps_per_epoch}, Loss: {loss.item()}, in time: {step_end-step_start}\")\n",
        "\n",
        "\n",
        "    val_loss += loss\n",
        "    #torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    del data_dict\n",
        "    del output, features\n",
        "\n",
        "  print(f'Time to complete one epoch: {time.time()-epoch_start}')\n",
        "  return val_loss/num_steps_per_epoch\n",
        "  #/len(val_loader)\n",
        "# ... (Your existing code for data loading and preprocessing) ..."
      ],
      "metadata": {
        "id": "n1Uz0Cc9x_OD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir(r'C:\\Users\\uqabhat2\\OneDrive - The University of Queensland\\LiDAR')\n",
        "root = os.getcwd()\n",
        "lidar_dir = os.path.join(root, 'LiDAR')\n",
        "\n",
        "pcdata_dir = os.path.join(lidar_dir, 'pcloud_norm')\n",
        "pc_files = natsort.natsorted(os.listdir(pcdata_dir))\n",
        "pc_paths = [os.path.join(pcdata_dir, pcf) for pcf in pc_files]\n",
        "pc_paths = pc_paths[:config_dict['total_samples']]\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(device)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "for seq_ in ['6h']:\n",
        "\n",
        "    save_dir = os.path.join(lidar_dir, os.path.join('saved_models',seq_))\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    seq_len = int(seq_[:-1])\n",
        "    config_dict['seq_type'] = seq_\n",
        "\n",
        "    random.shuffle(pc_paths)\n",
        "    pc_paths = pc_paths[:config_dict['total_samples']]\n",
        "\n",
        "    train_val_pcs = pc_paths[:(int(len(pc_paths)*0.7))]\n",
        "    test_pcs = pc_paths[(int(len(pc_paths)*0.7)):]\n",
        "\n",
        "\n",
        "    train_idx = np.arange(len(train_val_pcs)).astype(int)[:int(0.9*len(train_val_pcs))]\n",
        "    val_idx = np.arange(len(train_val_pcs)).astype(int)[int(0.9*len(train_val_pcs)):]\n",
        "\n",
        "    train_pcs = np.asarray(train_val_pcs)[train_idx].tolist()\n",
        "    val_pcs = np.asarray(train_val_pcs)[val_idx].tolist()\n",
        "\n",
        "    train_dataset = PC_dataset_individuals(lidar_dir, train_pcs)\n",
        "    val_dataset = PC_dataset_individuals(lidar_dir, val_pcs)\n",
        "    test_dataset = PC_dataset_individuals(lidar_dir, test_pcs)\n",
        "\n",
        "    num_epochs = config_dict['num_epochs']\n",
        "    num_steps_per_epoch = train_dataset.data_len()//config_dict['batch_size'] + 1\n",
        "    gc.collect()\n",
        "    model = AutoEncoder(config_dict).to(device)#cuda() # Move model to device\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                    max_lr = 0.001,\n",
        "                                                    epochs=num_epochs,\n",
        "                                                    steps_per_epoch=num_steps_per_epoch)\n",
        "\n",
        "    criterion = nn.HuberLoss()\n",
        "\n",
        "    if os.path.exists(os.path.join(save_dir, 'train_losses.npy')) and os.path.exists(os.path.join(save_dir, 'val_losses.npy')):\n",
        "            train_losses = np.load(os.path.join(save_dir, 'train_losses.npy')).tolist()\n",
        "            val_losses = np.load(os.path.join(save_dir, 'val_losses.npy')).tolist()\n",
        "            min_val = np.min(val_losses)\n",
        "            print('Loaded Losses from saved logs')\n",
        "    else:\n",
        "            train_losses = []\n",
        "            val_losses = []\n",
        "            min_val = np.inf\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "            #scheduler.step()\n",
        "            train_loss = train(model,\n",
        "                                train_dataset,\n",
        "                                config_dict['batch_size'],\n",
        "                                optimizer,\n",
        "                                scheduler,\n",
        "                                criterion,\n",
        "                                epoch,\n",
        "                                device,\n",
        "                                config_dict) # Pass device to train function\n",
        "            with torch.no_grad():\n",
        "                val_loss = val(model,\n",
        "                                val_dataset,\n",
        "                                config_dict['val_batch_size'],\n",
        "                                criterion,\n",
        "                                epoch,\n",
        "                                device,\n",
        "                                config_dict) # Pass device to val function\n",
        "\n",
        "            print(f'train_loss:{train_loss} and val loss:{val_loss}')\n",
        "\n",
        "            if val_loss < min_val:\n",
        "                min_val = val_loss\n",
        "                torch.save(model.state_dict(),\n",
        "                            os.path.join(save_dir, 'best_cosine_model_further.pth'))\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "    np.save(os.path.join(save_dir, 'train_losses.npy'), np.asarray(train_losses))\n",
        "    np.save(os.path.join(save_dir, 'val_losses.npy'), np.asarray(val_losses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5iJH9BOWpqJL",
        "outputId": "b970cc5d-39e3-45d2-bdae-4916096b9b38"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Step 0/40, Loss: 0.0674, Time: 13.46s\n",
            "Epoch 0, Step 1/40, Loss: 0.0722, Time: 2.86s\n",
            "Epoch 0, Step 2/40, Loss: 0.0741, Time: 2.38s\n",
            "Epoch 0, Step 3/40, Loss: 0.0670, Time: 2.83s\n",
            "Epoch 0, Step 4/40, Loss: 0.0785, Time: 2.90s\n",
            "Epoch 0, Step 5/40, Loss: 0.0793, Time: 2.53s\n",
            "Epoch 0, Step 6/40, Loss: 0.0741, Time: 2.82s\n",
            "Epoch 0, Step 7/40, Loss: 0.0832, Time: 2.04s\n",
            "Epoch 0, Step 8/40, Loss: 0.0689, Time: 2.15s\n",
            "Epoch 0, Step 9/40, Loss: 0.0649, Time: 2.24s\n",
            "Epoch 0, Step 10/40, Loss: 0.0620, Time: 2.47s\n",
            "Epoch 0, Step 11/40, Loss: 0.0628, Time: 2.48s\n",
            "Epoch 0, Step 12/40, Loss: 0.0679, Time: 2.38s\n",
            "Epoch 0, Step 13/40, Loss: 0.0643, Time: 2.15s\n",
            "Epoch 0, Step 14/40, Loss: 0.0585, Time: 1.88s\n",
            "Epoch 0, Step 15/40, Loss: 0.0647, Time: 3.60s\n",
            "Epoch 0, Step 16/40, Loss: 0.0701, Time: 3.04s\n",
            "Epoch 0, Step 17/40, Loss: 0.0573, Time: 1.89s\n",
            "Epoch 0, Step 18/40, Loss: 0.0665, Time: 2.49s\n",
            "Epoch 0, Step 19/40, Loss: 0.0590, Time: 2.48s\n",
            "Epoch 0, Step 20/40, Loss: 0.0694, Time: 2.42s\n",
            "Epoch 0, Step 21/40, Loss: 0.0695, Time: 2.37s\n",
            "Epoch 0, Step 22/40, Loss: 0.0601, Time: 2.36s\n",
            "Epoch 0, Step 23/40, Loss: 0.0635, Time: 1.80s\n",
            "Epoch 0, Step 24/40, Loss: 0.0500, Time: 2.06s\n",
            "Epoch 0, Step 25/40, Loss: 0.0558, Time: 3.54s\n",
            "Epoch 0, Step 26/40, Loss: 0.0614, Time: 2.54s\n",
            "Epoch 0, Step 27/40, Loss: 0.0535, Time: 2.36s\n",
            "Epoch 0, Step 28/40, Loss: 0.0669, Time: 2.28s\n",
            "Epoch 0, Step 29/40, Loss: 0.0527, Time: 2.53s\n",
            "Epoch 0, Step 30/40, Loss: 0.0569, Time: 2.98s\n",
            "Epoch 0, Step 31/40, Loss: 0.0547, Time: 2.59s\n",
            "Epoch 0, Step 32/40, Loss: 0.0656, Time: 2.36s\n",
            "Epoch 0, Step 33/40, Loss: 0.0550, Time: 2.38s\n",
            "Epoch 0, Step 34/40, Loss: 0.0587, Time: 2.13s\n",
            "Epoch 0, Step 35/40, Loss: 0.0569, Time: 3.08s\n",
            "Epoch 0, Step 36/40, Loss: 0.0426, Time: 1.95s\n",
            "Epoch 0, Step 37/40, Loss: 0.0505, Time: 2.54s\n",
            "Epoch 0, Step 38/40, Loss: 0.0485, Time: 2.74s\n",
            "Epoch 0, Step 39/40, Loss: 0.0466, Time: 2.77s\n",
            "Epoch 0 completed in 119.50s\n",
            "Epoch: 0, Step: 0/71, Loss: 0.043913062661886215, in time: 0.40560007095336914\n",
            "Epoch: 0, Step: 1/71, Loss: 0.05958475172519684, in time: 0.3822469711303711\n",
            "Epoch: 0, Step: 2/71, Loss: 0.05536668747663498, in time: 0.20023417472839355\n",
            "Epoch: 0, Step: 3/71, Loss: 0.05958475172519684, in time: 0.27737855911254883\n",
            "Epoch: 0, Step: 4/71, Loss: 0.09072165191173553, in time: 0.20816516876220703\n",
            "Epoch: 0, Step: 5/71, Loss: 0.05655853450298309, in time: 0.20984482765197754\n",
            "Epoch: 0, Step: 6/71, Loss: 0.05583716928958893, in time: 0.24210023880004883\n",
            "Epoch: 0, Step: 7/71, Loss: 0.04560251533985138, in time: 0.15477776527404785\n",
            "Epoch: 0, Step: 8/71, Loss: 0.05655853450298309, in time: 0.20035862922668457\n",
            "Epoch: 0, Step: 9/71, Loss: 0.04919137805700302, in time: 0.1851956844329834\n",
            "Epoch: 0, Step: 10/71, Loss: 0.04560251533985138, in time: 0.12961912155151367\n",
            "Epoch: 0, Step: 11/71, Loss: 0.06080390885472298, in time: 0.15170645713806152\n",
            "Epoch: 0, Step: 12/71, Loss: 0.05373266339302063, in time: 0.22102570533752441\n",
            "Epoch: 0, Step: 13/71, Loss: 0.043913062661886215, in time: 0.24162673950195312\n",
            "Epoch: 0, Step: 14/71, Loss: 0.05655853450298309, in time: 0.20849967002868652\n",
            "Epoch: 0, Step: 15/71, Loss: 0.043913062661886215, in time: 0.2460641860961914\n",
            "Epoch: 0, Step: 16/71, Loss: 0.06934548169374466, in time: 0.1498274803161621\n",
            "Epoch: 0, Step: 17/71, Loss: 0.04560251533985138, in time: 0.16114401817321777\n",
            "Epoch: 0, Step: 18/71, Loss: 0.06080390885472298, in time: 0.14355254173278809\n",
            "Epoch: 0, Step: 19/71, Loss: 0.04919137805700302, in time: 0.2235701084136963\n",
            "Epoch: 0, Step: 20/71, Loss: 0.04919137805700302, in time: 0.16986346244812012\n",
            "Epoch: 0, Step: 21/71, Loss: 0.05536668747663498, in time: 0.19927144050598145\n",
            "Epoch: 0, Step: 22/71, Loss: 0.05958475172519684, in time: 0.28011631965637207\n",
            "Epoch: 0, Step: 23/71, Loss: 0.04919137805700302, in time: 0.18695831298828125\n",
            "Epoch: 0, Step: 24/71, Loss: 0.06934548169374466, in time: 0.12572240829467773\n",
            "Epoch: 0, Step: 25/71, Loss: 0.09072165191173553, in time: 0.20180535316467285\n",
            "Epoch: 0, Step: 26/71, Loss: 0.040318556129932404, in time: 0.10915327072143555\n",
            "Epoch: 0, Step: 27/71, Loss: 0.04560251533985138, in time: 0.17823386192321777\n",
            "Epoch: 0, Step: 28/71, Loss: 0.06080390885472298, in time: 0.16926884651184082\n",
            "Epoch: 0, Step: 29/71, Loss: 0.1211790144443512, in time: 0.24900460243225098\n",
            "Epoch: 0, Step: 30/71, Loss: 0.04919137805700302, in time: 0.2434988021850586\n",
            "Epoch: 0, Step: 31/71, Loss: 0.06080390885472298, in time: 0.18491458892822266\n",
            "Epoch: 0, Step: 32/71, Loss: 0.05373266339302063, in time: 0.27407312393188477\n",
            "Epoch: 0, Step: 33/71, Loss: 0.06080390885472298, in time: 0.15489435195922852\n",
            "Epoch: 0, Step: 34/71, Loss: 0.05655853450298309, in time: 0.2087092399597168\n",
            "Epoch: 0, Step: 35/71, Loss: 0.06080390885472298, in time: 0.13743972778320312\n",
            "Epoch: 0, Step: 36/71, Loss: 0.05583716928958893, in time: 0.22501778602600098\n",
            "Epoch: 0, Step: 37/71, Loss: 0.05958475172519684, in time: 0.2790086269378662\n",
            "Epoch: 0, Step: 38/71, Loss: 0.04919137805700302, in time: 0.1581249237060547\n",
            "Epoch: 0, Step: 39/71, Loss: 0.09072165191173553, in time: 0.19732046127319336\n",
            "Epoch: 0, Step: 40/71, Loss: 0.06934548169374466, in time: 0.12672853469848633\n",
            "Epoch: 0, Step: 41/71, Loss: 0.05373266339302063, in time: 0.19294452667236328\n",
            "Epoch: 0, Step: 42/71, Loss: 0.04919137805700302, in time: 0.17380738258361816\n",
            "Epoch: 0, Step: 43/71, Loss: 0.03656971454620361, in time: 0.1824028491973877\n",
            "Epoch: 0, Step: 44/71, Loss: 0.05373266339302063, in time: 0.1926712989807129\n",
            "Epoch: 0, Step: 45/71, Loss: 0.05373266339302063, in time: 0.22302627563476562\n",
            "Epoch: 0, Step: 46/71, Loss: 0.04560251533985138, in time: 0.13115501403808594\n",
            "Epoch: 0, Step: 47/71, Loss: 0.043913062661886215, in time: 0.23717761039733887\n",
            "Epoch: 0, Step: 48/71, Loss: 0.03411442041397095, in time: 0.21120023727416992\n",
            "Epoch: 0, Step: 49/71, Loss: 0.043913062661886215, in time: 0.2376253604888916\n",
            "Epoch: 0, Step: 50/71, Loss: 0.05958475172519684, in time: 0.2852795124053955\n",
            "Epoch: 0, Step: 51/71, Loss: 0.06080390885472298, in time: 0.15334725379943848\n",
            "Epoch: 0, Step: 52/71, Loss: 0.09072165191173553, in time: 0.19605374336242676\n",
            "Epoch: 0, Step: 53/71, Loss: 0.05583716928958893, in time: 0.22979283332824707\n",
            "Epoch: 0, Step: 54/71, Loss: 0.06934548169374466, in time: 0.13609814643859863\n",
            "Epoch: 0, Step: 55/71, Loss: 0.042769208550453186, in time: 0.2174839973449707\n",
            "Epoch: 0, Step: 56/71, Loss: 0.040318556129932404, in time: 0.09114885330200195\n",
            "Epoch: 0, Step: 57/71, Loss: 0.06080390885472298, in time: 0.16137266159057617\n",
            "Epoch: 0, Step: 58/71, Loss: 0.05655853450298309, in time: 0.23019099235534668\n",
            "Epoch: 0, Step: 59/71, Loss: 0.05583716928958893, in time: 0.31338953971862793\n",
            "Epoch: 0, Step: 60/71, Loss: 0.05536668747663498, in time: 0.271543025970459\n",
            "Epoch: 0, Step: 61/71, Loss: 0.05655853450298309, in time: 0.24219655990600586\n",
            "Epoch: 0, Step: 62/71, Loss: 0.04919137805700302, in time: 0.20921945571899414\n",
            "Epoch: 0, Step: 63/71, Loss: 0.06080390885472298, in time: 0.20406198501586914\n",
            "Epoch: 0, Step: 64/71, Loss: 0.09072165191173553, in time: 0.26796650886535645\n",
            "Epoch: 0, Step: 65/71, Loss: 0.04919137805700302, in time: 0.18058228492736816\n",
            "Epoch: 0, Step: 66/71, Loss: 0.06934548169374466, in time: 0.12575626373291016\n",
            "Epoch: 0, Step: 67/71, Loss: 0.06396487355232239, in time: 0.2182908058166504\n",
            "Epoch: 0, Step: 68/71, Loss: 0.05428565666079521, in time: 0.13981056213378906\n",
            "Epoch: 0, Step: 69/71, Loss: 0.05655853450298309, in time: 0.2049098014831543\n",
            "Epoch: 0, Step: 70/71, Loss: 0.05958475172519684, in time: 0.2581596374511719\n",
            "Time to complete one epoch: 29.769150733947754\n",
            "train_loss:0.06253460515290499 and val loss:0.05750593543052673\n",
            "Epoch 1, Step 0/40, Loss: 0.0477, Time: 2.16s\n",
            "Epoch 1, Step 1/40, Loss: 0.0463, Time: 2.40s\n",
            "Epoch 1, Step 2/40, Loss: 0.0459, Time: 2.66s\n",
            "Epoch 1, Step 3/40, Loss: 0.0495, Time: 2.25s\n",
            "Epoch 1, Step 4/40, Loss: 0.0439, Time: 2.42s\n",
            "Epoch 1, Step 5/40, Loss: 0.0419, Time: 2.37s\n",
            "Epoch 1, Step 6/40, Loss: 0.0460, Time: 2.24s\n",
            "Epoch 1, Step 7/40, Loss: 0.0442, Time: 2.09s\n",
            "Epoch 1, Step 8/40, Loss: 0.0397, Time: 2.46s\n",
            "Epoch 1, Step 9/40, Loss: 0.0422, Time: 2.22s\n",
            "Epoch 1, Step 10/40, Loss: 0.0391, Time: 2.37s\n",
            "Epoch 1, Step 11/40, Loss: 0.0343, Time: 2.93s\n",
            "Epoch 1, Step 12/40, Loss: 0.0441, Time: 2.92s\n",
            "Epoch 1, Step 13/40, Loss: 0.0461, Time: 2.24s\n",
            "Epoch 1, Step 14/40, Loss: 0.0449, Time: 2.78s\n",
            "Epoch 1, Step 15/40, Loss: 0.0323, Time: 1.98s\n",
            "Epoch 1, Step 16/40, Loss: 0.0422, Time: 2.17s\n",
            "Epoch 1, Step 17/40, Loss: 0.0424, Time: 2.62s\n",
            "Epoch 1, Step 18/40, Loss: 0.0478, Time: 2.44s\n",
            "Epoch 1, Step 19/40, Loss: 0.0308, Time: 1.88s\n",
            "Epoch 1, Step 20/40, Loss: 0.0288, Time: 2.09s\n",
            "Epoch 1, Step 21/40, Loss: 0.0280, Time: 2.11s\n",
            "Epoch 1, Step 22/40, Loss: 0.0369, Time: 2.65s\n",
            "Epoch 1, Step 23/40, Loss: 0.0344, Time: 2.90s\n",
            "Epoch 1, Step 24/40, Loss: 0.0409, Time: 2.28s\n",
            "Epoch 1, Step 25/40, Loss: 0.0318, Time: 2.25s\n",
            "Epoch 1, Step 26/40, Loss: 0.0293, Time: 2.10s\n",
            "Epoch 1, Step 27/40, Loss: 0.0354, Time: 2.84s\n",
            "Epoch 1, Step 28/40, Loss: 0.0338, Time: 2.49s\n",
            "Epoch 1, Step 29/40, Loss: 0.0280, Time: 3.32s\n",
            "Epoch 1, Step 30/40, Loss: 0.0417, Time: 1.73s\n",
            "Epoch 1, Step 31/40, Loss: 0.0269, Time: 2.44s\n",
            "Epoch 1, Step 32/40, Loss: 0.0330, Time: 2.92s\n",
            "Epoch 1, Step 33/40, Loss: 0.0348, Time: 2.66s\n",
            "Epoch 1, Step 34/40, Loss: 0.0262, Time: 2.28s\n",
            "Epoch 1, Step 35/40, Loss: 0.0243, Time: 2.12s\n",
            "Epoch 1, Step 36/40, Loss: 0.0293, Time: 2.08s\n",
            "Epoch 1, Step 37/40, Loss: 0.0204, Time: 2.66s\n",
            "Epoch 1, Step 38/40, Loss: 0.0213, Time: 2.24s\n",
            "Epoch 1, Step 39/40, Loss: 0.0259, Time: 2.48s\n",
            "Epoch 1 completed in 105.11s\n",
            "Epoch: 1, Step: 0/71, Loss: 0.010375464335083961, in time: 0.2868156433105469\n",
            "Epoch: 1, Step: 1/71, Loss: 0.02848813869059086, in time: 0.13119053840637207\n",
            "Epoch: 1, Step: 2/71, Loss: 0.018315687775611877, in time: 0.11383938789367676\n",
            "Epoch: 1, Step: 3/71, Loss: 0.034798797219991684, in time: 0.19761204719543457\n",
            "Epoch: 1, Step: 4/71, Loss: 0.061628058552742004, in time: 0.19374799728393555\n",
            "Epoch: 1, Step: 5/71, Loss: 0.060813721269369125, in time: 0.2279670238494873\n",
            "Epoch: 1, Step: 6/71, Loss: 0.02848813869059086, in time: 0.14089083671569824\n",
            "Epoch: 1, Step: 7/71, Loss: 0.018828067928552628, in time: 0.17808032035827637\n",
            "Epoch: 1, Step: 8/71, Loss: 0.03198032081127167, in time: 0.28086209297180176\n",
            "Epoch: 1, Step: 9/71, Loss: 0.029891494661569595, in time: 0.17695927619934082\n",
            "Epoch: 1, Step: 10/71, Loss: 0.0414053276181221, in time: 0.14358043670654297\n",
            "Epoch: 1, Step: 11/71, Loss: 0.02283553220331669, in time: 0.2143104076385498\n",
            "Epoch: 1, Step: 12/71, Loss: 0.034798797219991684, in time: 0.21108698844909668\n",
            "Epoch: 1, Step: 13/71, Loss: 0.019975334405899048, in time: 0.14034652709960938\n",
            "Epoch: 1, Step: 14/71, Loss: 0.060813721269369125, in time: 0.30391883850097656\n",
            "Epoch: 1, Step: 15/71, Loss: 0.029891494661569595, in time: 0.21389198303222656\n",
            "Epoch: 1, Step: 16/71, Loss: 0.061628058552742004, in time: 0.2513704299926758\n",
            "Epoch: 1, Step: 17/71, Loss: 0.061628058552742004, in time: 0.28060340881347656\n",
            "Epoch: 1, Step: 18/71, Loss: 0.03510502353310585, in time: 0.27146267890930176\n",
            "Epoch: 1, Step: 19/71, Loss: 0.03510502353310585, in time: 0.27487945556640625\n",
            "Epoch: 1, Step: 20/71, Loss: 0.02848813869059086, in time: 0.13770580291748047\n",
            "Epoch: 1, Step: 21/71, Loss: 0.03076147474348545, in time: 0.24559855461120605\n",
            "Epoch: 1, Step: 22/71, Loss: 0.03076147474348545, in time: 0.2418842315673828\n",
            "Epoch: 1, Step: 23/71, Loss: 0.02283553220331669, in time: 0.19882583618164062\n",
            "Epoch: 1, Step: 24/71, Loss: 0.02848813869059086, in time: 0.1461472511291504\n",
            "Epoch: 1, Step: 25/71, Loss: 0.029891494661569595, in time: 0.19448018074035645\n",
            "Epoch: 1, Step: 26/71, Loss: 0.02848813869059086, in time: 0.1571056842803955\n",
            "Epoch: 1, Step: 27/71, Loss: 0.018315687775611877, in time: 0.09440946578979492\n",
            "Epoch: 1, Step: 28/71, Loss: 0.06770782172679901, in time: 0.22840523719787598\n",
            "Epoch: 1, Step: 29/71, Loss: 0.03198032081127167, in time: 0.27829980850219727\n",
            "Epoch: 1, Step: 30/71, Loss: 0.060813721269369125, in time: 0.21424365043640137\n",
            "Epoch: 1, Step: 31/71, Loss: 0.061628058552742004, in time: 0.21478819847106934\n",
            "Epoch: 1, Step: 32/71, Loss: 0.06770782172679901, in time: 0.20501971244812012\n",
            "Epoch: 1, Step: 33/71, Loss: 0.02283553220331669, in time: 0.2108290195465088\n",
            "Epoch: 1, Step: 34/71, Loss: 0.034798797219991684, in time: 0.21656441688537598\n",
            "Epoch: 1, Step: 35/71, Loss: 0.02283553220331669, in time: 0.20171213150024414\n",
            "Epoch: 1, Step: 36/71, Loss: 0.060813721269369125, in time: 0.23062610626220703\n",
            "Epoch: 1, Step: 37/71, Loss: 0.01953672617673874, in time: 0.15745258331298828\n",
            "Epoch: 1, Step: 38/71, Loss: 0.03198032081127167, in time: 0.27498364448547363\n",
            "Epoch: 1, Step: 39/71, Loss: 0.01953672617673874, in time: 0.153350830078125\n",
            "Epoch: 1, Step: 40/71, Loss: 0.019975334405899048, in time: 0.13578009605407715\n",
            "Epoch: 1, Step: 41/71, Loss: 0.02283553220331669, in time: 0.2181558609008789\n",
            "Epoch: 1, Step: 42/71, Loss: 0.03510502353310585, in time: 0.1954033374786377\n",
            "Epoch: 1, Step: 43/71, Loss: 0.03076147474348545, in time: 0.22825121879577637\n",
            "Epoch: 1, Step: 44/71, Loss: 0.02283553220331669, in time: 0.2832179069519043\n",
            "Epoch: 1, Step: 45/71, Loss: 0.034798797219991684, in time: 0.2557637691497803\n",
            "Epoch: 1, Step: 46/71, Loss: 0.018315687775611877, in time: 0.11388087272644043\n",
            "Epoch: 1, Step: 47/71, Loss: 0.061628058552742004, in time: 0.25118589401245117\n",
            "Epoch: 1, Step: 48/71, Loss: 0.02848813869059086, in time: 0.23165273666381836\n",
            "Epoch: 1, Step: 49/71, Loss: 0.010375464335083961, in time: 0.3636205196380615\n",
            "Epoch: 1, Step: 50/71, Loss: 0.029891494661569595, in time: 0.19873547554016113\n",
            "Epoch: 1, Step: 51/71, Loss: 0.018635474145412445, in time: 0.14387989044189453\n",
            "Epoch: 1, Step: 52/71, Loss: 0.029891494661569595, in time: 0.15245270729064941\n",
            "Epoch: 1, Step: 53/71, Loss: 0.02848813869059086, in time: 0.14949727058410645\n",
            "Epoch: 1, Step: 54/71, Loss: 0.018828067928552628, in time: 0.17360377311706543\n",
            "Epoch: 1, Step: 55/71, Loss: 0.02207096293568611, in time: 0.2176351547241211\n",
            "Epoch: 1, Step: 56/71, Loss: 0.03510502353310585, in time: 0.1840834617614746\n",
            "Epoch: 1, Step: 57/71, Loss: 0.029891494661569595, in time: 0.15529108047485352\n",
            "Epoch: 1, Step: 58/71, Loss: 0.02207096293568611, in time: 0.2171163558959961\n",
            "Epoch: 1, Step: 59/71, Loss: 0.03198032081127167, in time: 0.2649354934692383\n",
            "Epoch: 1, Step: 60/71, Loss: 0.061628058552742004, in time: 0.2323446273803711\n",
            "Epoch: 1, Step: 61/71, Loss: 0.02848813869059086, in time: 0.15072298049926758\n",
            "Epoch: 1, Step: 62/71, Loss: 0.02848813869059086, in time: 0.14101147651672363\n",
            "Epoch: 1, Step: 63/71, Loss: 0.03924164921045303, in time: 0.2210385799407959\n",
            "Epoch: 1, Step: 64/71, Loss: 0.02314874902367592, in time: 0.16718006134033203\n",
            "Epoch: 1, Step: 65/71, Loss: 0.02314874902367592, in time: 0.15818309783935547\n",
            "Epoch: 1, Step: 66/71, Loss: 0.034798797219991684, in time: 0.20381712913513184\n",
            "Epoch: 1, Step: 67/71, Loss: 0.02283553220331669, in time: 0.20896077156066895\n",
            "Epoch: 1, Step: 68/71, Loss: 0.03076147474348545, in time: 0.24547815322875977\n",
            "Epoch: 1, Step: 69/71, Loss: 0.061628058552742004, in time: 0.2043299674987793\n",
            "Epoch: 1, Step: 70/71, Loss: 0.029891494661569595, in time: 0.16521620750427246\n",
            "Time to complete one epoch: 30.09389615058899\n",
            "train_loss:0.03656731336377561 and val loss:0.03356380760669708\n",
            "Epoch 2, Step 0/40, Loss: 0.0236, Time: 2.60s\n",
            "Epoch 2, Step 1/40, Loss: 0.0252, Time: 2.96s\n",
            "Epoch 2, Step 2/40, Loss: 0.0214, Time: 2.43s\n",
            "Epoch 2, Step 3/40, Loss: 0.0222, Time: 2.52s\n",
            "Epoch 2, Step 4/40, Loss: 0.0206, Time: 1.64s\n",
            "Epoch 2, Step 5/40, Loss: 0.0242, Time: 2.21s\n",
            "Epoch 2, Step 6/40, Loss: 0.0219, Time: 2.59s\n",
            "Epoch 2, Step 7/40, Loss: 0.0237, Time: 2.04s\n",
            "Epoch 2, Step 8/40, Loss: 0.0211, Time: 2.76s\n",
            "Epoch 2, Step 9/40, Loss: 0.0200, Time: 2.08s\n",
            "Epoch 2, Step 10/40, Loss: 0.0183, Time: 2.23s\n",
            "Epoch 2, Step 11/40, Loss: 0.0178, Time: 2.87s\n",
            "Epoch 2, Step 12/40, Loss: 0.0187, Time: 2.07s\n",
            "Epoch 2, Step 13/40, Loss: 0.0215, Time: 1.89s\n",
            "Epoch 2, Step 14/40, Loss: 0.0193, Time: 2.36s\n",
            "Epoch 2, Step 15/40, Loss: 0.0237, Time: 2.07s\n",
            "Epoch 2, Step 16/40, Loss: 0.0209, Time: 3.50s\n",
            "Epoch 2, Step 17/40, Loss: 0.0194, Time: 2.05s\n",
            "Epoch 2, Step 18/40, Loss: 0.0231, Time: 2.26s\n",
            "Epoch 2, Step 19/40, Loss: 0.0272, Time: 1.89s\n",
            "Epoch 2, Step 20/40, Loss: 0.0164, Time: 2.52s\n",
            "Epoch 2, Step 21/40, Loss: 0.0202, Time: 3.00s\n",
            "Epoch 2, Step 22/40, Loss: 0.0209, Time: 2.51s\n",
            "Epoch 2, Step 23/40, Loss: 0.0179, Time: 1.96s\n",
            "Epoch 2, Step 24/40, Loss: 0.0171, Time: 2.31s\n",
            "Epoch 2, Step 25/40, Loss: 0.0207, Time: 2.03s\n",
            "Epoch 2, Step 26/40, Loss: 0.0164, Time: 2.92s\n",
            "Epoch 2, Step 27/40, Loss: 0.0147, Time: 2.45s\n",
            "Epoch 2, Step 28/40, Loss: 0.0185, Time: 2.38s\n",
            "Epoch 2, Step 29/40, Loss: 0.0157, Time: 1.82s\n",
            "Epoch 2, Step 30/40, Loss: 0.0199, Time: 2.44s\n",
            "Epoch 2, Step 31/40, Loss: 0.0154, Time: 3.07s\n",
            "Epoch 2, Step 32/40, Loss: 0.0200, Time: 2.11s\n",
            "Epoch 2, Step 33/40, Loss: 0.0156, Time: 2.41s\n",
            "Epoch 2, Step 34/40, Loss: 0.0182, Time: 2.43s\n",
            "Epoch 2, Step 35/40, Loss: 0.0159, Time: 2.51s\n",
            "Epoch 2, Step 36/40, Loss: 0.0170, Time: 2.57s\n",
            "Epoch 2, Step 37/40, Loss: 0.0213, Time: 2.19s\n",
            "Epoch 2, Step 38/40, Loss: 0.0143, Time: 1.78s\n",
            "Epoch 2, Step 39/40, Loss: 0.0201, Time: 2.32s\n",
            "Epoch 2 completed in 103.36s\n",
            "Epoch: 2, Step: 0/71, Loss: 0.02188102900981903, in time: 0.18413066864013672\n",
            "Epoch: 2, Step: 1/71, Loss: 0.02227410487830639, in time: 0.12941646575927734\n",
            "Epoch: 2, Step: 2/71, Loss: 0.02188102900981903, in time: 0.16443252563476562\n",
            "Epoch: 2, Step: 3/71, Loss: 0.01679745502769947, in time: 0.19667768478393555\n",
            "Epoch: 2, Step: 4/71, Loss: 0.01679745502769947, in time: 0.1907668113708496\n",
            "Epoch: 2, Step: 5/71, Loss: 0.036994483321905136, in time: 0.21545076370239258\n",
            "Epoch: 2, Step: 6/71, Loss: 0.008891710080206394, in time: 0.18253040313720703\n",
            "Epoch: 2, Step: 7/71, Loss: 0.01679745502769947, in time: 0.1918184757232666\n",
            "Epoch: 2, Step: 8/71, Loss: 0.018482310697436333, in time: 0.32851648330688477\n",
            "Epoch: 2, Step: 9/71, Loss: 0.008891710080206394, in time: 0.2461376190185547\n",
            "Epoch: 2, Step: 10/71, Loss: 0.030097341164946556, in time: 0.2519876956939697\n",
            "Epoch: 2, Step: 11/71, Loss: 4.054683208465576, in time: 0.19576764106750488\n",
            "Epoch: 2, Step: 12/71, Loss: 0.010906493291258812, in time: 0.24277424812316895\n",
            "Epoch: 2, Step: 13/71, Loss: 4.054683208465576, in time: 0.21765804290771484\n",
            "Epoch: 2, Step: 14/71, Loss: 0.015591004863381386, in time: 0.22442865371704102\n",
            "Epoch: 2, Step: 15/71, Loss: 0.02227410487830639, in time: 0.13897061347961426\n",
            "Epoch: 2, Step: 16/71, Loss: 0.02227410487830639, in time: 0.12408232688903809\n",
            "Epoch: 2, Step: 17/71, Loss: 4.054683208465576, in time: 0.1616504192352295\n",
            "Epoch: 2, Step: 18/71, Loss: 0.036994483321905136, in time: 0.20224428176879883\n",
            "Epoch: 2, Step: 19/71, Loss: 0.008891710080206394, in time: 0.21063232421875\n",
            "Epoch: 2, Step: 20/71, Loss: 4.054683208465576, in time: 0.15067005157470703\n",
            "Epoch: 2, Step: 21/71, Loss: 0.036994483321905136, in time: 0.19461774826049805\n",
            "Epoch: 2, Step: 22/71, Loss: 0.018482310697436333, in time: 0.25164294242858887\n",
            "Epoch: 2, Step: 23/71, Loss: 0.02188102900981903, in time: 0.17551922798156738\n",
            "Epoch: 2, Step: 24/71, Loss: 0.007869316264986992, in time: 0.23633337020874023\n",
            "Epoch: 2, Step: 25/71, Loss: 0.007561896927654743, in time: 0.09780049324035645\n",
            "Epoch: 2, Step: 26/71, Loss: 4.054683208465576, in time: 0.14354538917541504\n",
            "Epoch: 2, Step: 27/71, Loss: 0.017223279923200607, in time: 0.14478564262390137\n",
            "Epoch: 2, Step: 28/71, Loss: 0.01679745502769947, in time: 0.19645190238952637\n",
            "Epoch: 2, Step: 29/71, Loss: 0.015525469556450844, in time: 0.19141840934753418\n",
            "Epoch: 2, Step: 30/71, Loss: 0.02227410487830639, in time: 0.13403797149658203\n",
            "Epoch: 2, Step: 31/71, Loss: 0.007772529497742653, in time: 0.17550873756408691\n",
            "Epoch: 2, Step: 32/71, Loss: 4.054683208465576, in time: 0.14653301239013672\n",
            "Epoch: 2, Step: 33/71, Loss: 0.02227410487830639, in time: 0.136138916015625\n",
            "Epoch: 2, Step: 34/71, Loss: 0.018482310697436333, in time: 0.23292183876037598\n",
            "Epoch: 2, Step: 35/71, Loss: 0.015591004863381386, in time: 0.20521140098571777\n",
            "Epoch: 2, Step: 36/71, Loss: 0.11672236025333405, in time: 0.25138401985168457\n",
            "Epoch: 2, Step: 37/71, Loss: 0.015591004863381386, in time: 0.24375557899475098\n",
            "Epoch: 2, Step: 38/71, Loss: 0.036994483321905136, in time: 0.2915668487548828\n",
            "Epoch: 2, Step: 39/71, Loss: 0.007869316264986992, in time: 0.423450231552124\n",
            "Epoch: 2, Step: 40/71, Loss: 0.036994483321905136, in time: 0.3404538631439209\n",
            "Epoch: 2, Step: 41/71, Loss: 0.007502657826989889, in time: 0.2754058837890625\n",
            "Epoch: 2, Step: 42/71, Loss: 0.021103668957948685, in time: 0.3804795742034912\n",
            "Epoch: 2, Step: 43/71, Loss: 0.018482310697436333, in time: 0.317840576171875\n",
            "Epoch: 2, Step: 44/71, Loss: 0.015591004863381386, in time: 0.2781844139099121\n",
            "Epoch: 2, Step: 45/71, Loss: 0.036994483321905136, in time: 0.3090071678161621\n",
            "Epoch: 2, Step: 46/71, Loss: 0.015525469556450844, in time: 0.2032630443572998\n",
            "Epoch: 2, Step: 47/71, Loss: 0.02188102900981903, in time: 0.15032553672790527\n",
            "Epoch: 2, Step: 48/71, Loss: 0.015591004863381386, in time: 0.20617461204528809\n",
            "Epoch: 2, Step: 49/71, Loss: 0.11672236025333405, in time: 0.1937260627746582\n",
            "Epoch: 2, Step: 50/71, Loss: 0.015591004863381386, in time: 0.20681476593017578\n",
            "Epoch: 2, Step: 51/71, Loss: 0.036994483321905136, in time: 0.20868444442749023\n",
            "Epoch: 2, Step: 52/71, Loss: 0.007772529497742653, in time: 0.16959309577941895\n",
            "Epoch: 2, Step: 53/71, Loss: 0.015591004863381386, in time: 0.2074294090270996\n",
            "Epoch: 2, Step: 54/71, Loss: 0.036994483321905136, in time: 0.2121272087097168\n",
            "Epoch: 2, Step: 55/71, Loss: 0.036994483321905136, in time: 0.2222895622253418\n",
            "Epoch: 2, Step: 56/71, Loss: 0.02188102900981903, in time: 0.16098642349243164\n",
            "Epoch: 2, Step: 57/71, Loss: 4.054683208465576, in time: 0.1534278392791748\n",
            "Epoch: 2, Step: 58/71, Loss: 0.02227410487830639, in time: 0.1470353603363037\n",
            "Epoch: 2, Step: 59/71, Loss: 0.023765498772263527, in time: 0.1989755630493164\n",
            "Epoch: 2, Step: 60/71, Loss: 0.030097341164946556, in time: 0.2340233325958252\n",
            "Epoch: 2, Step: 61/71, Loss: 0.023765498772263527, in time: 0.1970076560974121\n",
            "Epoch: 2, Step: 62/71, Loss: 4.054683208465576, in time: 0.1400763988494873\n",
            "Epoch: 2, Step: 63/71, Loss: 0.02188102900981903, in time: 0.19002151489257812\n",
            "Epoch: 2, Step: 64/71, Loss: 0.007869316264986992, in time: 0.2412717342376709\n",
            "Epoch: 2, Step: 65/71, Loss: 0.02227410487830639, in time: 0.12984180450439453\n",
            "Epoch: 2, Step: 66/71, Loss: 0.01679745502769947, in time: 0.1907196044921875\n",
            "Epoch: 2, Step: 67/71, Loss: 0.018482310697436333, in time: 0.2500736713409424\n",
            "Epoch: 2, Step: 68/71, Loss: 0.05925355851650238, in time: 0.14070415496826172\n",
            "Epoch: 2, Step: 69/71, Loss: 0.036994483321905136, in time: 0.2139115333557129\n",
            "Epoch: 2, Step: 70/71, Loss: 0.015591004863381386, in time: 0.21227121353149414\n",
            "Time to complete one epoch: 30.352035760879517\n",
            "train_loss:0.019751464226283134 and val loss:0.4782724976539612\n",
            "Epoch 3, Step 0/40, Loss: 0.0120, Time: 3.02s\n",
            "Epoch 3, Step 1/40, Loss: 0.0165, Time: 1.89s\n",
            "Epoch 3, Step 2/40, Loss: 0.0167, Time: 1.78s\n",
            "Epoch 3, Step 3/40, Loss: 0.0175, Time: 2.26s\n",
            "Epoch 3, Step 4/40, Loss: 0.0136, Time: 2.16s\n",
            "Epoch 3, Step 5/40, Loss: 0.0195, Time: 3.06s\n",
            "Epoch 3, Step 6/40, Loss: 0.0114, Time: 2.13s\n",
            "Epoch 3, Step 7/40, Loss: 0.0223, Time: 1.85s\n",
            "Epoch 3, Step 8/40, Loss: 0.0126, Time: 1.93s\n",
            "Epoch 3, Step 9/40, Loss: 0.0179, Time: 1.69s\n",
            "Epoch 3, Step 10/40, Loss: 0.0154, Time: 2.35s\n",
            "Epoch 3, Step 11/40, Loss: 0.0154, Time: 2.46s\n",
            "Epoch 3, Step 12/40, Loss: 0.0163, Time: 2.33s\n",
            "Epoch 3, Step 13/40, Loss: 0.0157, Time: 2.18s\n",
            "Epoch 3, Step 14/40, Loss: 0.0135, Time: 1.85s\n",
            "Epoch 3, Step 15/40, Loss: 0.0206, Time: 1.93s\n",
            "Epoch 3, Step 16/40, Loss: 0.0163, Time: 2.63s\n",
            "Epoch 3, Step 17/40, Loss: 0.0199, Time: 1.99s\n",
            "Epoch 3, Step 18/40, Loss: 0.0155, Time: 2.02s\n",
            "Epoch 3, Step 19/40, Loss: 0.0181, Time: 2.38s\n",
            "Epoch 3, Step 20/40, Loss: 0.0139, Time: 2.10s\n",
            "Epoch 3, Step 21/40, Loss: 0.0187, Time: 2.99s\n",
            "Epoch 3, Step 22/40, Loss: 0.0145, Time: 2.60s\n",
            "Epoch 3, Step 23/40, Loss: 0.0121, Time: 2.15s\n",
            "Epoch 3, Step 24/40, Loss: 0.0155, Time: 2.44s\n",
            "Epoch 3, Step 25/40, Loss: 0.0138, Time: 2.94s\n",
            "Epoch 3, Step 26/40, Loss: 0.0156, Time: 3.18s\n",
            "Epoch 3, Step 27/40, Loss: 0.0170, Time: 2.61s\n",
            "Epoch 3, Step 28/40, Loss: 0.0153, Time: 1.81s\n",
            "Epoch 3, Step 29/40, Loss: 0.0142, Time: 2.38s\n",
            "Epoch 3, Step 30/40, Loss: 0.0187, Time: 2.22s\n",
            "Epoch 3, Step 31/40, Loss: 0.0200, Time: 2.99s\n",
            "Epoch 3, Step 32/40, Loss: 0.0126, Time: 2.14s\n",
            "Epoch 3, Step 33/40, Loss: 0.0145, Time: 2.68s\n",
            "Epoch 3, Step 34/40, Loss: 0.0171, Time: 2.35s\n",
            "Epoch 3, Step 35/40, Loss: 0.0148, Time: 2.44s\n",
            "Epoch 3, Step 36/40, Loss: 0.0166, Time: 2.67s\n",
            "Epoch 3, Step 37/40, Loss: 0.0144, Time: 2.16s\n",
            "Epoch 3, Step 38/40, Loss: 0.0133, Time: 1.92s\n",
            "Epoch 3, Step 39/40, Loss: 0.0151, Time: 2.29s\n",
            "Epoch 3 completed in 101.43s\n",
            "Epoch: 3, Step: 0/71, Loss: 0.04719415307044983, in time: 0.2040407657623291\n",
            "Epoch: 3, Step: 1/71, Loss: 0.013345764949917793, in time: 0.21271514892578125\n",
            "Epoch: 3, Step: 2/71, Loss: 0.006841995753347874, in time: 0.24917054176330566\n",
            "Epoch: 3, Step: 3/71, Loss: 0.005983073730021715, in time: 0.09270596504211426\n",
            "Epoch: 3, Step: 4/71, Loss: 0.024085164070129395, in time: 0.22559285163879395\n",
            "Epoch: 3, Step: 5/71, Loss: 0.02166203409433365, in time: 0.17823123931884766\n",
            "Epoch: 3, Step: 6/71, Loss: 0.014880090020596981, in time: 0.22196674346923828\n",
            "Epoch: 3, Step: 7/71, Loss: 0.27870771288871765, in time: 0.1816108226776123\n",
            "Epoch: 3, Step: 8/71, Loss: 0.01851993426680565, in time: 0.3345043659210205\n",
            "Epoch: 3, Step: 9/71, Loss: 0.014247739687561989, in time: 0.33002376556396484\n",
            "Epoch: 3, Step: 10/71, Loss: 0.028146227821707726, in time: 0.27759408950805664\n",
            "Epoch: 3, Step: 11/71, Loss: 0.02166203409433365, in time: 0.24631190299987793\n",
            "Epoch: 3, Step: 12/71, Loss: 0.028146227821707726, in time: 0.21619892120361328\n",
            "Epoch: 3, Step: 13/71, Loss: 0.014247739687561989, in time: 0.2429215908050537\n",
            "Epoch: 3, Step: 14/71, Loss: 0.011540708132088184, in time: 0.2005751132965088\n",
            "Epoch: 3, Step: 15/71, Loss: 0.014880090020596981, in time: 0.20149683952331543\n",
            "Epoch: 3, Step: 16/71, Loss: 0.0038540500681847334, in time: 0.12562990188598633\n",
            "Epoch: 3, Step: 17/71, Loss: 0.01851993426680565, in time: 0.27339816093444824\n",
            "Epoch: 3, Step: 18/71, Loss: 0.011540708132088184, in time: 0.2041022777557373\n",
            "Epoch: 3, Step: 19/71, Loss: 0.005023256875574589, in time: 0.15348052978515625\n",
            "Epoch: 3, Step: 20/71, Loss: 0.02166203409433365, in time: 0.15638232231140137\n",
            "Epoch: 3, Step: 21/71, Loss: 0.27870771288871765, in time: 0.1296236515045166\n",
            "Epoch: 3, Step: 22/71, Loss: 0.03796689212322235, in time: 0.2039201259613037\n",
            "Epoch: 3, Step: 23/71, Loss: 0.02166203409433365, in time: 0.1863846778869629\n",
            "Epoch: 3, Step: 24/71, Loss: 0.006841995753347874, in time: 0.24816560745239258\n",
            "Epoch: 3, Step: 25/71, Loss: 0.013345764949917793, in time: 0.21541786193847656\n",
            "Epoch: 3, Step: 26/71, Loss: 0.01851993426680565, in time: 0.2847862243652344\n",
            "Epoch: 3, Step: 27/71, Loss: 5.957492351531982, in time: 0.12984323501586914\n",
            "Epoch: 3, Step: 28/71, Loss: 0.013345764949917793, in time: 0.2008216381072998\n",
            "Epoch: 3, Step: 29/71, Loss: 0.028146227821707726, in time: 0.20009732246398926\n",
            "Epoch: 3, Step: 30/71, Loss: 0.006841995753347874, in time: 0.2523782253265381\n",
            "Epoch: 3, Step: 31/71, Loss: 0.006841995753347874, in time: 0.2633674144744873\n",
            "Epoch: 3, Step: 32/71, Loss: 0.01851993426680565, in time: 0.26998138427734375\n",
            "Epoch: 3, Step: 33/71, Loss: 0.016508052125573158, in time: 0.1298379898071289\n",
            "Epoch: 3, Step: 34/71, Loss: 0.014880090020596981, in time: 0.19351983070373535\n",
            "Epoch: 3, Step: 35/71, Loss: 0.27870771288871765, in time: 0.13307452201843262\n",
            "Epoch: 3, Step: 36/71, Loss: 0.04719415307044983, in time: 0.21710658073425293\n",
            "Epoch: 3, Step: 37/71, Loss: 0.02166203409433365, in time: 0.22580623626708984\n",
            "Epoch: 3, Step: 38/71, Loss: 0.006841995753347874, in time: 0.32438039779663086\n",
            "Epoch: 3, Step: 39/71, Loss: 0.02166203409433365, in time: 0.20954394340515137\n",
            "Epoch: 3, Step: 40/71, Loss: 5.957492351531982, in time: 0.1952371597290039\n",
            "Epoch: 3, Step: 41/71, Loss: 0.0038540500681847334, in time: 0.17547178268432617\n",
            "Epoch: 3, Step: 42/71, Loss: 0.005983073730021715, in time: 0.12844181060791016\n",
            "Epoch: 3, Step: 43/71, Loss: 0.014247739687561989, in time: 0.23956894874572754\n",
            "Epoch: 3, Step: 44/71, Loss: 0.04719415307044983, in time: 0.2124638557434082\n",
            "Epoch: 3, Step: 45/71, Loss: 5.957492351531982, in time: 0.17317652702331543\n",
            "Epoch: 3, Step: 46/71, Loss: 0.014880090020596981, in time: 0.21343660354614258\n",
            "Epoch: 3, Step: 47/71, Loss: 0.028146227821707726, in time: 0.21638917922973633\n",
            "Epoch: 3, Step: 48/71, Loss: 0.01851993426680565, in time: 0.2875995635986328\n",
            "Epoch: 3, Step: 49/71, Loss: 0.016508052125573158, in time: 0.13033533096313477\n",
            "Epoch: 3, Step: 50/71, Loss: 5.957492351531982, in time: 0.17549800872802734\n",
            "Epoch: 3, Step: 51/71, Loss: 0.014247739687561989, in time: 0.24359726905822754\n",
            "Epoch: 3, Step: 52/71, Loss: 0.03796689212322235, in time: 0.1981520652770996\n",
            "Epoch: 3, Step: 53/71, Loss: 0.014880090020596981, in time: 0.19234395027160645\n",
            "Epoch: 3, Step: 54/71, Loss: 0.013345764949917793, in time: 0.21127843856811523\n",
            "Epoch: 3, Step: 55/71, Loss: 0.02166203409433365, in time: 0.18209505081176758\n",
            "Epoch: 3, Step: 56/71, Loss: 5.957492351531982, in time: 0.15096306800842285\n",
            "Epoch: 3, Step: 57/71, Loss: 0.014247739687561989, in time: 0.2431180477142334\n",
            "Epoch: 3, Step: 58/71, Loss: 0.006841995753347874, in time: 0.2615935802459717\n",
            "Epoch: 3, Step: 59/71, Loss: 0.01851993426680565, in time: 0.27098941802978516\n",
            "Epoch: 3, Step: 60/71, Loss: 0.014247739687561989, in time: 0.2313520908355713\n",
            "Epoch: 3, Step: 61/71, Loss: 0.04719415307044983, in time: 0.2172682285308838\n",
            "Epoch: 3, Step: 62/71, Loss: 0.014247739687561989, in time: 0.25977134704589844\n",
            "Epoch: 3, Step: 63/71, Loss: 0.022925524041056633, in time: 0.1951906681060791\n",
            "Epoch: 3, Step: 64/71, Loss: 0.006841995753347874, in time: 0.2775859832763672\n",
            "Epoch: 3, Step: 65/71, Loss: 0.014247739687561989, in time: 0.22886919975280762\n",
            "Epoch: 3, Step: 66/71, Loss: 0.005983073730021715, in time: 0.1369788646697998\n",
            "Epoch: 3, Step: 67/71, Loss: 0.0038540500681847334, in time: 0.15081501007080078\n",
            "Epoch: 3, Step: 68/71, Loss: 0.011540708132088184, in time: 0.23488998413085938\n",
            "Epoch: 3, Step: 69/71, Loss: 0.013345764949917793, in time: 0.26729416847229004\n",
            "Epoch: 3, Step: 70/71, Loss: 0.024085164070129395, in time: 0.27649664878845215\n",
            "Time to complete one epoch: 30.979691743850708\n",
            "train_loss:0.01586404212284833 and val loss:0.44704103469848633\n",
            "Epoch 4, Step 0/40, Loss: 0.0239, Time: 2.65s\n",
            "Epoch 4, Step 1/40, Loss: 0.0157, Time: 2.14s\n",
            "Epoch 4, Step 2/40, Loss: 0.0124, Time: 2.41s\n",
            "Epoch 4, Step 3/40, Loss: 0.0144, Time: 2.09s\n",
            "Epoch 4, Step 4/40, Loss: 0.0184, Time: 2.37s\n",
            "Epoch 4, Step 5/40, Loss: 0.0150, Time: 2.67s\n",
            "Epoch 4, Step 6/40, Loss: 0.0142, Time: 1.85s\n",
            "Epoch 4, Step 7/40, Loss: 0.0147, Time: 2.21s\n",
            "Epoch 4, Step 8/40, Loss: 0.0162, Time: 2.46s\n",
            "Epoch 4, Step 9/40, Loss: 0.0134, Time: 2.66s\n",
            "Epoch 4, Step 10/40, Loss: 0.0133, Time: 2.28s\n",
            "Epoch 4, Step 11/40, Loss: 0.0152, Time: 2.60s\n",
            "Epoch 4, Step 12/40, Loss: 0.0113, Time: 1.94s\n",
            "Epoch 4, Step 13/40, Loss: 0.0153, Time: 2.31s\n",
            "Epoch 4, Step 14/40, Loss: 0.0157, Time: 2.73s\n",
            "Epoch 4, Step 15/40, Loss: 0.0152, Time: 2.22s\n",
            "Epoch 4, Step 16/40, Loss: 0.0130, Time: 2.46s\n",
            "Epoch 4, Step 17/40, Loss: 0.0163, Time: 2.53s\n",
            "Epoch 4, Step 18/40, Loss: 0.0144, Time: 2.07s\n",
            "Epoch 4, Step 19/40, Loss: 0.0157, Time: 2.70s\n",
            "Epoch 4, Step 20/40, Loss: 0.0156, Time: 2.46s\n",
            "Epoch 4, Step 21/40, Loss: 0.0192, Time: 2.45s\n",
            "Epoch 4, Step 22/40, Loss: 0.0149, Time: 2.28s\n",
            "Epoch 4, Step 23/40, Loss: 0.0134, Time: 2.07s\n",
            "Epoch 4, Step 24/40, Loss: 0.0141, Time: 2.40s\n",
            "Epoch 4, Step 25/40, Loss: 0.0160, Time: 2.51s\n",
            "Epoch 4, Step 26/40, Loss: 0.0114, Time: 2.59s\n",
            "Epoch 4, Step 27/40, Loss: 0.0142, Time: 1.99s\n",
            "Epoch 4, Step 28/40, Loss: 0.0163, Time: 2.12s\n",
            "Epoch 4, Step 29/40, Loss: 0.0132, Time: 2.28s\n",
            "Epoch 4, Step 30/40, Loss: 0.0148, Time: 2.63s\n",
            "Epoch 4, Step 31/40, Loss: 0.0178, Time: 2.49s\n",
            "Epoch 4, Step 32/40, Loss: 0.0142, Time: 2.40s\n",
            "Epoch 4, Step 33/40, Loss: 0.0123, Time: 2.16s\n",
            "Epoch 4, Step 34/40, Loss: 0.0115, Time: 2.62s\n",
            "Epoch 4, Step 35/40, Loss: 0.0128, Time: 3.10s\n",
            "Epoch 4, Step 36/40, Loss: 0.0192, Time: 2.07s\n",
            "Epoch 4, Step 37/40, Loss: 0.0120, Time: 1.92s\n",
            "Epoch 4, Step 38/40, Loss: 0.0142, Time: 2.22s\n",
            "Epoch 4, Step 39/40, Loss: 0.0165, Time: 2.35s\n",
            "Epoch 4 completed in 103.03s\n",
            "Epoch: 4, Step: 0/71, Loss: 3.4300808906555176, in time: 0.1564345359802246\n",
            "Epoch: 4, Step: 1/71, Loss: 3.4300808906555176, in time: 0.1920006275177002\n",
            "Epoch: 4, Step: 2/71, Loss: 0.08305993676185608, in time: 0.17357516288757324\n",
            "Epoch: 4, Step: 3/71, Loss: 0.013280559331178665, in time: 0.3326301574707031\n",
            "Epoch: 4, Step: 4/71, Loss: 0.02332131564617157, in time: 0.16094565391540527\n",
            "Epoch: 4, Step: 5/71, Loss: 0.02332131564617157, in time: 0.17049741744995117\n",
            "Epoch: 4, Step: 6/71, Loss: 0.019810887053608894, in time: 0.2762892246246338\n",
            "Epoch: 4, Step: 7/71, Loss: 0.016447853296995163, in time: 0.19406914710998535\n",
            "Epoch: 4, Step: 8/71, Loss: 0.01647578924894333, in time: 0.14267539978027344\n",
            "Epoch: 4, Step: 9/71, Loss: 0.013280559331178665, in time: 0.24660897254943848\n",
            "Epoch: 4, Step: 10/71, Loss: 0.027589693665504456, in time: 0.21554899215698242\n",
            "Epoch: 4, Step: 11/71, Loss: 0.013280559331178665, in time: 0.24095630645751953\n",
            "Epoch: 4, Step: 12/71, Loss: 0.007417505607008934, in time: 0.15797924995422363\n",
            "Epoch: 4, Step: 13/71, Loss: 0.015812549740076065, in time: 0.21290802955627441\n",
            "Epoch: 4, Step: 14/71, Loss: 0.008338150568306446, in time: 0.23599934577941895\n",
            "Epoch: 4, Step: 15/71, Loss: 0.006309975404292345, in time: 0.09942483901977539\n",
            "Epoch: 4, Step: 16/71, Loss: 0.008338150568306446, in time: 0.2591979503631592\n",
            "Epoch: 4, Step: 17/71, Loss: 3.4300808906555176, in time: 0.1486954689025879\n",
            "Epoch: 4, Step: 18/71, Loss: 0.005377410911023617, in time: 0.16069269180297852\n",
            "Epoch: 4, Step: 19/71, Loss: 0.08305993676185608, in time: 0.1292870044708252\n",
            "Epoch: 4, Step: 20/71, Loss: 0.015812549740076065, in time: 0.1927051544189453\n",
            "Epoch: 4, Step: 21/71, Loss: 0.08305993676185608, in time: 0.14960002899169922\n",
            "Epoch: 4, Step: 22/71, Loss: 0.027589693665504456, in time: 0.2143089771270752\n",
            "Epoch: 4, Step: 23/71, Loss: 0.004255005158483982, in time: 0.11402702331542969\n",
            "Epoch: 4, Step: 24/71, Loss: 0.007417505607008934, in time: 0.1739029884338379\n",
            "Epoch: 4, Step: 25/71, Loss: 0.019810887053608894, in time: 0.272352933883667\n",
            "Epoch: 4, Step: 26/71, Loss: 0.019810887053608894, in time: 0.27367115020751953\n",
            "Epoch: 4, Step: 27/71, Loss: 0.013280559331178665, in time: 0.2289447784423828\n",
            "Epoch: 4, Step: 28/71, Loss: 0.08305993676185608, in time: 0.1343367099761963\n",
            "Epoch: 4, Step: 29/71, Loss: 0.016447853296995163, in time: 0.21579909324645996\n",
            "Epoch: 4, Step: 30/71, Loss: 0.02332131564617157, in time: 0.23602080345153809\n",
            "Epoch: 4, Step: 31/71, Loss: 0.011999281123280525, in time: 0.2589755058288574\n",
            "Epoch: 4, Step: 32/71, Loss: 0.02332131564617157, in time: 0.21866416931152344\n",
            "Epoch: 4, Step: 33/71, Loss: 3.4300808906555176, in time: 0.16242146492004395\n",
            "Epoch: 4, Step: 34/71, Loss: 0.02332131564617157, in time: 0.23552989959716797\n",
            "Epoch: 4, Step: 35/71, Loss: 0.03791678696870804, in time: 0.2803022861480713\n",
            "Epoch: 4, Step: 36/71, Loss: 3.4300808906555176, in time: 0.15276503562927246\n",
            "Epoch: 4, Step: 37/71, Loss: 3.4300808906555176, in time: 0.14183354377746582\n",
            "Epoch: 4, Step: 38/71, Loss: 0.006309975404292345, in time: 0.09840798377990723\n",
            "Epoch: 4, Step: 39/71, Loss: 0.008087380789220333, in time: 0.19066357612609863\n",
            "Epoch: 4, Step: 40/71, Loss: 0.013280559331178665, in time: 0.23731088638305664\n",
            "Epoch: 4, Step: 41/71, Loss: 0.020706677809357643, in time: 0.13124585151672363\n",
            "Epoch: 4, Step: 42/71, Loss: 0.008338150568306446, in time: 0.25874924659729004\n",
            "Epoch: 4, Step: 43/71, Loss: 0.027589693665504456, in time: 0.20802879333496094\n",
            "Epoch: 4, Step: 44/71, Loss: 0.019810887053608894, in time: 0.27527809143066406\n",
            "Epoch: 4, Step: 45/71, Loss: 0.03791678696870804, in time: 0.20627641677856445\n",
            "Epoch: 4, Step: 46/71, Loss: 0.008087380789220333, in time: 0.20970606803894043\n",
            "Epoch: 4, Step: 47/71, Loss: 0.023918205872178078, in time: 0.20436549186706543\n",
            "Epoch: 4, Step: 48/71, Loss: 0.020706677809357643, in time: 0.12415480613708496\n",
            "Epoch: 4, Step: 49/71, Loss: 3.4300808906555176, in time: 0.1536095142364502\n",
            "Epoch: 4, Step: 50/71, Loss: 0.027589693665504456, in time: 0.21499109268188477\n",
            "Epoch: 4, Step: 51/71, Loss: 0.027589693665504456, in time: 0.2079005241394043\n",
            "Epoch: 4, Step: 52/71, Loss: 0.005377410911023617, in time: 0.15580964088439941\n",
            "Epoch: 4, Step: 53/71, Loss: 0.015812549740076065, in time: 0.1895139217376709\n",
            "Epoch: 4, Step: 54/71, Loss: 0.02332131564617157, in time: 0.1651461124420166\n",
            "Epoch: 4, Step: 55/71, Loss: 0.21092642843723297, in time: 0.19848036766052246\n",
            "Epoch: 4, Step: 56/71, Loss: 0.008338150568306446, in time: 0.2466297149658203\n",
            "Epoch: 4, Step: 57/71, Loss: 0.008087380789220333, in time: 0.19903874397277832\n",
            "Epoch: 4, Step: 58/71, Loss: 0.016447853296995163, in time: 0.1943669319152832\n",
            "Epoch: 4, Step: 59/71, Loss: 0.02332131564617157, in time: 0.17466998100280762\n",
            "Epoch: 4, Step: 60/71, Loss: 0.027589693665504456, in time: 0.22060608863830566\n",
            "Epoch: 4, Step: 61/71, Loss: 0.011999281123280525, in time: 0.27378129959106445\n",
            "Epoch: 4, Step: 62/71, Loss: 0.01647578924894333, in time: 0.16411733627319336\n",
            "Epoch: 4, Step: 63/71, Loss: 0.011999281123280525, in time: 0.2425553798675537\n",
            "Epoch: 4, Step: 64/71, Loss: 3.4300808906555176, in time: 0.17305278778076172\n",
            "Epoch: 4, Step: 65/71, Loss: 0.008338150568306446, in time: 0.32999229431152344\n",
            "Epoch: 4, Step: 66/71, Loss: 0.011999281123280525, in time: 0.2810697555541992\n",
            "Epoch: 4, Step: 67/71, Loss: 0.016447853296995163, in time: 0.18197941780090332\n",
            "Epoch: 4, Step: 68/71, Loss: 0.013280559331178665, in time: 0.26332831382751465\n",
            "Epoch: 4, Step: 69/71, Loss: 0.02332131564617157, in time: 0.1868879795074463\n",
            "Epoch: 4, Step: 70/71, Loss: 0.020706677809357643, in time: 0.12456917762756348\n",
            "Time to complete one epoch: 29.48477077484131\n",
            "train_loss:0.014927483093924821 and val loss:0.4078776240348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_dataset, batch_size, criterion, device, config_dict):\n",
        "\n",
        "  model.eval()\n",
        "  epoch_start = time.time()\n",
        "  test_loss = 0\n",
        "  num_steps_per_epoch = (test_dataset.data_len()//batch_size) + 1 *(test_dataset.data_len()%batch_size != 0)\n",
        "  recon_coords_list = []\n",
        "  recon_feats_list = []\n",
        "  target_coords_list = []\n",
        "  target_feats_list = []\n",
        "\n",
        "  for step in range(num_steps_per_epoch):\n",
        "    step_start = time.time()\n",
        "    data_dict = test_dataset.get_batch_single_pc(batch_size)\n",
        "    data_dict = {k: v.to(device) for k, v in data_dict.items()}\n",
        "\n",
        "    # === Forward pass ===\n",
        "    _, output = model(data_dict)\n",
        "    recon_coords = output[0]\n",
        "    recon_feats = output[1]\n",
        "    coords, feats, _, batch = inpdict_to_point(data_dict)\n",
        "    target_coords = coords.float()\n",
        "    target_feats = feats.float()\n",
        "\n",
        "    #--Loss--\n",
        "    coord_loss = criterion(recon_coords, target_coords)\n",
        "    feat_loss = criterion(recon_feats, target_feats)\n",
        "    alpha = config_dict.get(\"feature_loss_weight\", 1.0)\n",
        "    loss = coord_loss + alpha * feat_loss\n",
        "    step_end = time.time()\n",
        "\n",
        "\n",
        "    recon_coords_list.append(recon_coords)\n",
        "    recon_feats_list.append(recon_feats)\n",
        "    target_coords_list.append(target_coords)\n",
        "    target_feats_list.append(target_feats)\n",
        "\n",
        "    step_end = time.time()\n",
        "    print(f\"Step: {step}/{num_steps_per_epoch}, Loss: {loss.item()}, in time: {step_end-step_start}\")\n",
        "    test_loss += loss.item()\n",
        "    #torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    del data_dict\n",
        "\n",
        "  print(f'Time to complete one epoch: {time.time()-epoch_start}')\n",
        "  return test_loss/num_steps_per_epoch, recon_coords_list, recon_feats_list, target_coords_list, target_feats_list"
      ],
      "metadata": {
        "id": "VU0aWdRuDcru"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(os.path.join(save_dir, 'best_cosine_model_further.pth')))\n",
        "model.eval()\n",
        "\n",
        "criterion = nn.HuberLoss()\n",
        "with torch.no_grad():\n",
        "  test_loss, recon_coords_list, recon_feats_list, target_coords_list, target_feats_list = test(model, val_dataset, batch_size= 1, criterion = criterion, device = device, config_dict = config_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qIEfGyaKQr-d",
        "outputId": "6b832a89-cea8-4fa7-cd38-ad6845392ef7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0/70, Loss: 0.02283553220331669, in time: 0.19862890243530273\n",
            "Step: 1/70, Loss: 0.03924164921045303, in time: 0.19416522979736328\n",
            "Step: 2/70, Loss: 0.02848813869059086, in time: 0.15033793449401855\n",
            "Step: 3/70, Loss: 0.03510502353310585, in time: 0.18929457664489746\n",
            "Step: 4/70, Loss: 0.034798797219991684, in time: 0.2069239616394043\n",
            "Step: 5/70, Loss: 0.060813721269369125, in time: 0.212050199508667\n",
            "Step: 6/70, Loss: 0.02848813869059086, in time: 0.14800238609313965\n",
            "Step: 7/70, Loss: 0.034798797219991684, in time: 0.1980435848236084\n",
            "Step: 8/70, Loss: 0.029891494661569595, in time: 0.17203927040100098\n",
            "Step: 9/70, Loss: 0.01953672617673874, in time: 0.15073895454406738\n",
            "Step: 10/70, Loss: 0.018315687775611877, in time: 0.1418297290802002\n",
            "Step: 11/70, Loss: 0.02848813869059086, in time: 0.20050835609436035\n",
            "Step: 12/70, Loss: 0.010375464335083961, in time: 0.3077707290649414\n",
            "Step: 13/70, Loss: 0.034798797219991684, in time: 0.25523948669433594\n",
            "Step: 14/70, Loss: 0.034798797219991684, in time: 0.2687807083129883\n",
            "Step: 15/70, Loss: 0.0414053276181221, in time: 0.18261098861694336\n",
            "Step: 16/70, Loss: 0.061628058552742004, in time: 0.2115488052368164\n",
            "Step: 17/70, Loss: 0.02283553220331669, in time: 0.22194957733154297\n",
            "Step: 18/70, Loss: 0.034798797219991684, in time: 0.19828033447265625\n",
            "Step: 19/70, Loss: 0.0414053276181221, in time: 0.1380915641784668\n",
            "Step: 20/70, Loss: 0.060813721269369125, in time: 0.20737195014953613\n",
            "Step: 21/70, Loss: 0.019550088793039322, in time: 0.14352893829345703\n",
            "Step: 22/70, Loss: 0.02848813869059086, in time: 0.14370298385620117\n",
            "Step: 23/70, Loss: 0.03510502353310585, in time: 0.1951613426208496\n",
            "Step: 24/70, Loss: 0.02283553220331669, in time: 0.21885132789611816\n",
            "Step: 25/70, Loss: 0.029891494661569595, in time: 0.17538142204284668\n",
            "Step: 26/70, Loss: 0.061628058552742004, in time: 0.1984708309173584\n",
            "Step: 27/70, Loss: 0.02283553220331669, in time: 0.19494962692260742\n",
            "Step: 28/70, Loss: 0.03076147474348545, in time: 0.24323725700378418\n",
            "Step: 29/70, Loss: 0.060813721269369125, in time: 0.21948647499084473\n",
            "Step: 30/70, Loss: 0.034798797219991684, in time: 0.21181249618530273\n",
            "Step: 31/70, Loss: 0.019550088793039322, in time: 0.14098715782165527\n",
            "Step: 32/70, Loss: 0.060813721269369125, in time: 0.21025562286376953\n",
            "Step: 33/70, Loss: 0.03510502353310585, in time: 0.18416261672973633\n",
            "Step: 34/70, Loss: 0.03198032081127167, in time: 0.26346564292907715\n",
            "Step: 35/70, Loss: 0.02283553220331669, in time: 0.19092011451721191\n",
            "Step: 36/70, Loss: 0.034798797219991684, in time: 0.2216815948486328\n",
            "Step: 37/70, Loss: 0.02207096293568611, in time: 0.20101046562194824\n",
            "Step: 38/70, Loss: 0.061628058552742004, in time: 0.23092103004455566\n",
            "Step: 39/70, Loss: 0.0414053276181221, in time: 0.13680243492126465\n",
            "Step: 40/70, Loss: 0.02848813869059086, in time: 0.14726543426513672\n",
            "Step: 41/70, Loss: 0.029891494661569595, in time: 0.164963960647583\n",
            "Step: 42/70, Loss: 0.019975334405899048, in time: 0.17777132987976074\n",
            "Step: 43/70, Loss: 0.061628058552742004, in time: 0.2654430866241455\n",
            "Step: 44/70, Loss: 0.019975334405899048, in time: 0.16571617126464844\n",
            "Step: 45/70, Loss: 0.03076147474348545, in time: 0.2858436107635498\n",
            "Step: 46/70, Loss: 0.010375464335083961, in time: 0.3740849494934082\n",
            "Step: 47/70, Loss: 0.03198032081127167, in time: 0.3830573558807373\n",
            "Step: 48/70, Loss: 0.02283553220331669, in time: 0.214827299118042\n",
            "Step: 49/70, Loss: 0.03076147474348545, in time: 0.25242090225219727\n",
            "Step: 50/70, Loss: 0.02207096293568611, in time: 0.19438982009887695\n",
            "Step: 51/70, Loss: 0.02538514882326126, in time: 0.12389087677001953\n",
            "Step: 52/70, Loss: 0.034798797219991684, in time: 0.2370164394378662\n",
            "Step: 53/70, Loss: 0.03198032081127167, in time: 0.2650165557861328\n",
            "Step: 54/70, Loss: 0.019975334405899048, in time: 0.1387186050415039\n",
            "Step: 55/70, Loss: 0.060813721269369125, in time: 0.21608948707580566\n",
            "Step: 56/70, Loss: 0.029891494661569595, in time: 0.18284034729003906\n",
            "Step: 57/70, Loss: 0.019975334405899048, in time: 0.13684654235839844\n",
            "Step: 58/70, Loss: 0.02283553220331669, in time: 0.19076013565063477\n",
            "Step: 59/70, Loss: 0.029891494661569595, in time: 0.1815357208251953\n",
            "Step: 60/70, Loss: 0.0414053276181221, in time: 0.14475321769714355\n",
            "Step: 61/70, Loss: 0.010375464335083961, in time: 0.2354722023010254\n",
            "Step: 62/70, Loss: 0.03198032081127167, in time: 0.2795696258544922\n",
            "Step: 63/70, Loss: 0.0414053276181221, in time: 0.21054935455322266\n",
            "Step: 64/70, Loss: 0.02283553220331669, in time: 0.21330547332763672\n",
            "Step: 65/70, Loss: 0.019975334405899048, in time: 0.164076566696167\n",
            "Step: 66/70, Loss: 0.03510502353310585, in time: 0.18867158889770508\n",
            "Step: 67/70, Loss: 0.02538514882326126, in time: 0.13597726821899414\n",
            "Step: 68/70, Loss: 0.060813721269369125, in time: 0.2076711654663086\n",
            "Step: 69/70, Loss: 0.019975334405899048, in time: 0.12808823585510254\n",
            "Time to complete one epoch: 28.805707454681396\n"
          ]
        }
      ]
    }
  ]
}